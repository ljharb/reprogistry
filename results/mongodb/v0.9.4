[
	{
		"reproduceVersion": "0.0.0-local",
		"timestamp": "2025-12-31T07:58:36.289Z",
		"os": "linux",
		"arch": "x64",
		"strategy": "npm:11.7.0",
		"reproduced": false,
		"attested": false,
		"package": {
			"spec": "mongodb@0.9.4",
			"name": "mongodb",
			"version": "0.9.4",
			"location": "https://registry.npmjs.org/mongodb/-/mongodb-0.9.4.tgz",
			"integrity": "sha512-L1r7R7xd1M9gv+zj3xobYsNxXCpgfgfKn6jF+iamYF/Oi+cY+AWKsvbMgxz+9Y1lCj1lYGV9Ig1yIpYmMS55UQ==",
			"publishedAt": "2011-06-21T15:31:45.212Z",
			"publishedWith": {
				"node": "v0.4.8",
				"npm": "1.0.13"
			}
		},
		"source": {
			"integrity": null,
			"location": "git://github.com/christkv/node-mongodb-native.git",
			"spec": "github:christkv/node-mongodb-native#HEAD"
		},
		"comparisonHash": "8a2a8dcbb729bf3ee9216e490c955701f0ddebe1",
		"diff": {
			"files": {
				".gitignore": {
					"match": false,
					"packageHash": "aff0b74da20436976b338dba19ce82b2996ef9c6b90097862250e5c39fa59cab",
					"size": 41,
					"status": "missing-in-source"
				},
				"HISTORY": {
					"match": false,
					"packageHash": "94154cb67195e163dac9232fcfd2c6154ac043fb34332a0d2049093525e3c478",
					"size": 239,
					"status": "missing-in-source"
				},
				"Makefile": {
					"diff": "--- published/Makefile\n+++ rebuilt/Makefile\n@@ -1,26 +1,71 @@\n-\n NODE = node\n+NPM = npm\n+NODEUNIT = node_modules/nodeunit/bin/nodeunit\n+DOX = node_modules/dox/bin/dox\n name = all\n \n total: build_native\n \n build_native:\n-\t$(MAKE) -C ./external-libs/bson\n+\t# $(MAKE) -C ./external-libs/bson all\n \n-clean_native:\n-\t$(MAKE) -C ./external-libs/bson clean\n+build_native_debug:\n+\t$(MAKE) -C ./external-libs/bson all_debug\n \n-test: build_native test_integration_pure test_integration_native\n-\t@$(NODE) spec/spec.node.js\n+build_native_clang:\n+\t$(MAKE) -C ./external-libs/bson clang\n \n-test_integration_pure:\n-\t@$(NODE) integration/integration_tests.js pure $(name)\n+build_native_clang_debug:\n+\t$(MAKE) -C ./external-libs/bson clang_debug\n \n-test_integration_native:\n-\t@$(NODE) integration/integration_tests.js native $(name)\n+clean_native:\n+\t$(MAKE) -C ./external-libs/bson clean\n+\n+test: build_native\n+\t@echo \"\\n == Run All tests minus replicaset tests==\"\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot\n+\n+test_pure: build_native\n+\t@echo \"\\n == Run All tests minus replicaset tests==\"\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot --noactive\n+\n+test_junit: build_native\n+\t@echo \"\\n == Run All tests minus replicaset tests==\"\n+\t$(NODE) dev/tools/test_all.js --junit --noreplicaset\n+\n+test_nodeunit_pure:\n+\t@echo \"\\n == Execute Test Suite using Pure JS BSON Parser == \"\n+\t@$(NODEUNIT) test/ test/gridstore test/bson\n+\n+test_js:\n+\t@$(NODEUNIT) $(TESTS)\n+\n+test_nodeunit_replicaset_pure:\n+\t@echo \"\\n == Execute Test Suite using Pure JS BSON Parser == \"\n+\t@$(NODEUNIT) test/replicaset\n+\n+test_nodeunit_native:\n+\t@echo \"\\n == Execute Test Suite using Native BSON Parser == \"\n+\t@TEST_NATIVE=TRUE $(NODEUNIT) test/ test/gridstore test/bson\t\n+\n+test_nodeunit_replicaset_native:\n+\t@echo \"\\n == Execute Test Suite using Native BSON Parser == \"\n+\t@TEST_NATIVE=TRUE $(NODEUNIT) test/replicaset\n+\n+test_all: build_native\n+\t@echo \"\\n == Run All tests ==\"\n+\t$(NODE) dev/tools/test_all.js --boot\n+\n+test_all_junit: build_native\n+\t@echo \"\\n == Run All tests ==\"\n+\t$(NODE) dev/tools/test_all.js --junit --boot\n \n clean:\n \trm ./external-libs/bson/bson.node\n \trm -r ./external-libs/bson/build\n \n-.PHONY: total\n\\ No newline at end of file\n+generate_docs:\n+\t$(NODE) dev/tools/build-docs.js\n+\tmake --directory=./docs/sphinx-docs --file=Makefile html\n+\n+.PHONY: total\n",
					"match": false,
					"packageHash": "57d35c65b095522d697e61830eff581d69737b1731f327350a1f5d4810d27e15",
					"size": 496,
					"sourceHash": "9a40c06659b42802352fedb651c7e3ac1993e441b74345c4f4015c7835dbd375",
					"status": "content"
				},
				"Readme.md": {
					"diff": "--- published/Readme.md\n+++ rebuilt/Readme.md\n@@ -1,18 +1,29 @@\n+Main Documentation site\n+=======================\n+\n+[Documentation](http://christkv.github.com/node-mongodb-native/)\n+\n Install\n ========\n \n-Run:\n+To install the most recent release from npm, run:\n+\n+    npm install mongodb\n+    \n+That may give you a warning telling you that bugs['web'] should be bugs['url'], it would be safe to ignore it (this has been fixed in the development version) \n \n-    make\n+To install the latest from the repository, run::\n+\n+    npm install path/to/node-mongodb-native\n \n Community\n ========\n-Check out the google group http://groups.google.com/group/node-mongodb-native for questions/answers from users of the driver.\n+Check out the google group [node-mongodb-native](http://groups.google.com/group/node-mongodb-native) for questions/answers from users of the driver.\n \n Introduction\n ========\n \n-This is a node.js driver for MongoDB. It's a port (or close to a port) of the libary for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n+This is a node.js driver for MongoDB. It's a port (or close to a port) of the library for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n \n A simple example of inserting a document.\n \n@@ -27,7 +38,7 @@\n             // Locate all the entries using find\n             collection.find().toArray(function(err, results) {\n               test.assertEquals(1, results.length);\n-              test.assertTrue(results.a === 2);\n+              test.assertTrue(results[0].a === 2);\n \n               // Let's close the db\n               client.close();\n@@ -39,35 +50,59 @@\n       client.collection('test_insert', test);\n     });\n \n-Important\n+Data types\n ========\n \n-To enable the driver to use the C/C++ bson parser pass it the option native_parser:true like below\n+To store and retrieve the non-JSON MongoDb primitives ([ObjectID](http://www.mongodb.org/display/DOCS/Object+IDs), Long, Binary, [Timestamp](http://www.mongodb.org/display/DOCS/Timestamp+data+type), [DBRef](http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef), Code).\n+\n+In particular, every document has a unique `_id` which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example: \n+\n+    // Get the objectID type\n+    var ObjectID = require('mongodb').ObjectID;\n+    \n+    var idString = '4e4e1638c85e808431000003';\n+    collection.findOne({_id: new ObjectID(idString)}, console.log)  // ok\n+    collection.findOne({_id: idString}, console.log)  // wrong! callback gets undefined\n+\n+Here are the constructors the non-Javascript BSON primitive types:\n+\n+    // Fetch the library\n+    var mongo = require('mongodb');\n+    // Create new instances of BSON types\n+    new mongo.Long(numberString)\n+    new mongo.ObjectID(hexString)\n+    new mongo.Timestamp()  // the actual unique number is generated on insert.\n+    new mongo.DBRef(collectionName, id, dbName)\n+    new mongo.Binary(buffer)  // takes a string or Buffer\n+    new mongo.Code(code, [context])\n+    new mongo.Symbol(string)\n+    new mongo.MinKey()\n+    new mongo.MaxKey()\n+    new mongo.Double(number)\t// Force double storage\n \n+The C/C++ bson parser/serializer\n+--------\n+\n+From V0.8.0 to V0.9.6.9, the Javascript bson parser was slower than an optional C/C++ bson parser. As of V0.9.6.9+, due to performance improvements in the Javascript parser, the C/C++ parser is deprecated and is not installed by default anymore.\n+\n+If you are running a version of this library has the C/C++ parser compiled, to enable the driver to use the C/C++ bson parser pass it the option native_parser:true like below\n+\n+    // using Deprecated native_parser:\n     var client = new Db('integration_tests_20',\n                         new Server(\"127.0.0.1\", 27017),\n                         {native_parser:true});\n \n-The version V0.8.0 > contains a C/C++ native BSON parser, this leads to some small changes in the way you need to access the BSON classes as you need to use the right versions of the classes with the right driver.\n-\n-To access the correct version of BSON objects for your instance do the following\n-\n-    client.bson_serializer.Long\n-    client.bson_serializer.ObjectID\n-    client.bson_serializer.Timestamp\n",
					"match": false,
					"packageHash": "b6a1d4a5ffdfb80bd399f975d341a6c6a4ab384a8c0ecdd4f0d51f4c226d1303",
					"size": 13804,
					"sourceHash": "eec96097c0795dae69c2266acd746c7b1ee7a3bec8afe76b1dc19d24418f5075",
					"status": "content"
				},
				"benchmark/grid_fs_write_benchmark.js": {
					"match": false,
					"packageHash": "24102b757fdfab1762d8528e4fb1cbce7ba668117f251edc0ee0f96c8048201b",
					"size": 1365,
					"status": "missing-in-source"
				},
				"benchmark/streaming_benchmark.js": {
					"match": false,
					"packageHash": "4f9fb84f4f3606ab8ff272256853bbb4570694bfd1001888611d41ebd5b82bd6",
					"size": 1657,
					"status": "missing-in-source"
				},
				"data/data-27017/admin.0": {
					"match": false,
					"packageHash": "c47ab7b606a2b14a22ff6492c4fdb6019ac07162809e7186bc83ec4575978fd2",
					"size": 67108864,
					"status": "missing-in-source"
				},
				"data/data-27017/admin.1": {
					"match": false,
					"packageHash": "254bcc3fc4f27172636df4bf32de9f107f620d559b20d760197e452b97453917",
					"size": 134217728,
					"status": "missing-in-source"
				},
				"data/data-27017/admin.ns": {
					"match": false,
					"packageHash": "04503afdad50743484b130d6d89baeb9bcb02a6dc833aba2904fb5c29a02f7a8",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/data-27017/integration_tests.0": {
					"match": false,
					"packageHash": "ca1e1c9e1b3834073776f7c8a17f2047491a0a7217b5f1d07b0a8fd472ca03bf",
					"size": 67108864,
					"status": "missing-in-source"
				},
				"data/data-27017/integration_tests.1": {
					"match": false,
					"packageHash": "254bcc3fc4f27172636df4bf32de9f107f620d559b20d760197e452b97453917",
					"size": 134217728,
					"status": "missing-in-source"
				},
				"data/data-27017/integration_tests.ns": {
					"match": false,
					"packageHash": "22bd79debdbcaab63c8337a968a25803aab5c34ba4752e01611755c9329719f1",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/data-27017/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"data/log-27017": {
					"match": false,
					"packageHash": "bb1c6d590f21e4dec1eac345057bd3b05d37f3cacb928ee9dcbc6bc1aaeb1bfe",
					"size": 328301,
					"status": "missing-in-source"
				},
				"data/log-30000": {
					"match": false,
					"packageHash": "55833b4542192340156794d85ad5495d1ea8b06cc42f9b3f27b8b1683581a039",
					"size": 3995,
					"status": "missing-in-source"
				},
				"data/log-30001": {
					"match": false,
					"packageHash": "bf06a18d0576dfc97cd3ef97f79968e797434741eb530644fa84988cacc33b87",
					"size": 17172,
					"status": "missing-in-source"
				},
				"data/log-30002": {
					"match": false,
					"packageHash": "efac664b48519ccfccd8f9e1645e0d1d31e1e162c44cd2288eb31eec032d982b",
					"size": 9564,
					"status": "missing-in-source"
				},
				"data/log-30003": {
					"match": false,
					"packageHash": "0779bd7f7aadd4da6a08059b5b2898ba3ab9ae76f3446690c2f81ee58a348e56",
					"size": 6560,
					"status": "missing-in-source"
				},
				"data/log-30004": {
					"match": false,
					"packageHash": "69dc31d4aeba95cb0167cb249ac2d6b15b8664fb48defef2d3fb82f2a7802717",
					"size": 6474,
					"status": "missing-in-source"
				},
				"data/rs-30000/local.0": {
					"match": false,
					"packageHash": "7cfd8bdbd6fc87a8ab361c941b9523116e7287ca3f50c8326df7ee8477fc86c3",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30000/local.1": {
					"match": false,
					"packageHash": "8d979111873ffababa398135b9283b290fe1e0e92d35abd56e61b88fc50ae7e6",
					"size": 268435456,
					"status": "missing-in-source"
				},
				"data/rs-30000/local.ns": {
					"match": false,
					"packageHash": "4d09d7cba7911cd524ade8102c64094a4946a71ae779af48cf246d1957753b1e",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30000/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"data/rs-30000/ruby-test-db.0": {
					"match": false,
					"packageHash": "49d2570895b6de4f19528a76a4804bcbadc8e662aa227ce972405c83ba8d4797",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30000/ruby-test-db.ns": {
					"match": false,
					"packageHash": "802309feb1ba193ac841d4af43a96c4170805343d177b3adadc002919b7d790c",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30001/local.0": {
					"match": false,
					"packageHash": "228818b7c7a95178ebff4bcabba6a7d7e8ffa64acb1597392380a5720dfc35a8",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30001/local.1": {
					"match": false,
					"packageHash": "ea1bb9dfe68c6b6d55bd56ac14bbce4474291c9c7c17e66dcb3b83053c68f0b6",
					"size": 268435456,
					"status": "missing-in-source"
				},
				"data/rs-30001/local.ns": {
					"match": false,
					"packageHash": "e2c26711b9d6694186480324adf0adc58ee9793a8acc14dd4efc30a50601451b",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30001/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"data/rs-30001/ruby-test-db.0": {
					"match": false,
					"packageHash": "49d2570895b6de4f19528a76a4804bcbadc8e662aa227ce972405c83ba8d4797",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30001/ruby-test-db.ns": {
					"match": false,
					"packageHash": "802309feb1ba193ac841d4af43a96c4170805343d177b3adadc002919b7d790c",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30002/local.0": {
					"match": false,
					"packageHash": "0f61b3ee57212c1cf068541f378dc476aabbcdb6376362298e6cf6036e2e6bd5",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30002/local.1": {
					"match": false,
					"packageHash": "a4475d6de101d60874b5f8149f013c04ef7c62500a9d60a9851991f93e836f03",
					"size": 268435456,
					"status": "missing-in-source"
				},
				"data/rs-30002/local.ns": {
					"match": false,
					"packageHash": "313cd3087c707220e3eab83eeee441fb4d84d6f6e4c1ac3404a2b1ac772bf5c3",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30002/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"data/rs-30002/ruby-test-db.0": {
					"match": false,
					"packageHash": "49d2570895b6de4f19528a76a4804bcbadc8e662aa227ce972405c83ba8d4797",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30002/ruby-test-db.ns": {
					"match": false,
					"packageHash": "802309feb1ba193ac841d4af43a96c4170805343d177b3adadc002919b7d790c",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30003/local.0": {
					"match": false,
					"packageHash": "f01b45bf7e245dfbe06430d46d48d28df1f6fb32c20f9f047f78899d651eaa1a",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30003/local.ns": {
					"match": false,
					"packageHash": "d974acafc63135162d9232b7665c66557ba39c216cea912c8252a648fb525460",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30003/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"data/rs-30004/local.0": {
					"match": false,
					"packageHash": "f01b45bf7e245dfbe06430d46d48d28df1f6fb32c20f9f047f78899d651eaa1a",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30004/local.ns": {
					"match": false,
					"packageHash": "d974acafc63135162d9232b7665c66557ba39c216cea912c8252a648fb525460",
					"size": 16777216,
					"status": "missing-in-source"
				},
				"data/rs-30004/mongod.lock": {
					"match": false,
					"packageHash": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
					"size": 0,
					"status": "missing-in-source"
				},
				"examples/admin.js": {
					"match": false,
					"packageHash": "f22f00e77ef12bfe6ea6495a377bceda1e85fcfb3c8d79bf17a5a374c3173622",
					"size": 2315,
					"status": "missing-in-source"
				},
				"examples/blog.js": {
					"match": false,
					"packageHash": "2128cd7d6e6e90684c0c45fded19a69cea2443c0d31aa160f8a2e049ebc5d999",
					"size": 6018,
					"status": "missing-in-source"
				},
				"examples/capped.js": {
					"match": false,
					"packageHash": "b211ffeea1ac82da70eed332f27714b5f7fe9f05f6f3b2f3eb6e453e179e3505",
					"size": 1382,
					"status": "missing-in-source"
				},
				"examples/cursor.js": {
					"match": false,
					"packageHash": "0aa044634a02963db933e80f7f2547f747073d57db9c4c315000d50412c06d55",
					"size": 2721,
					"status": "missing-in-source"
				},
				"examples/gridfs.js": {
					"match": false,
					"packageHash": "d2a1e91cd9fb5a48db54167e38f80c422e2277e1660f9d70144961d5538a4b2c",
					"size": 5966,
					"status": "missing-in-source"
				},
				"examples/index_test.js": {
					"match": false,
					"packageHash": "cdb6045c4c177e356f0ca89ea38c4c426f79b364f375125bb74076f1e4c04ffc",
					"size": 2530,
					"status": "missing-in-source"
				},
				"examples/info.js": {
					"match": false,
					"packageHash": "31ba6b94616e873374fdc91d144ffbf035fe07a50748438c3bcaa07a0a7e3c69",
					"size": 1813,
					"status": "missing-in-source"
				},
				"examples/oplog.js": {
					"match": false,
					"packageHash": "f87622670f2560ee3dcf249c429881a9d64d8da9c49d21a2f7f83a8a510f0cd9",
					"size": 3277,
					"status": "missing-in-source"
				},
				"examples/queries.js": {
					"match": false,
					"packageHash": "e0938c115f4687d150c0e1de0280831355dad08ab0918e59ffba3c670195575b",
					"size": 5369,
					"status": "missing-in-source"
				},
				"examples/replSetServersQueries.js": {
					"match": false,
					"packageHash": "130750b1e553310fdd69f9f59f441d8049c8568c3ac4584a6ad167cfa0861bef",
					"size": 5818,
					"status": "missing-in-source"
				},
				"examples/replSetServersSimple.js": {
					"match": false,
					"packageHash": "82a5dff935ed38f7a4f968222bc429e5324fcb58d9d8e8de96c14e563454e450",
					"size": 2431,
					"status": "missing-in-source"
				},
				"examples/simple.js": {
					"match": false,
					"packageHash": "c5f41c240c2a0c3f016586f6d6403bf546ba0926625e4bcdcc211276b04746e0",
					"size": 1798,
					"status": "missing-in-source"
				},
				"examples/strict.js": {
					"match": false,
					"packageHash": "bb563d97813b9414815dd9fb32ae89a3e3da444effa5988bb8a771987b925917",
					"size": 1553,
					"status": "missing-in-source"
				},
				"examples/types.js": {
					"match": false,
					"packageHash": "595034328f2fc876f005844457bd4efaf877e1e377141b39bae41fa6358b9538",
					"size": 1825,
					"status": "missing-in-source"
				},
				"examples/url.js": {
					"match": false,
					"packageHash": "f1a6af085cc620e22e31b76ae6da2d8f13f231c40834e4a0591dcfd331a7253a",
					"size": 413,
					"status": "missing-in-source"
				},
				"examples/v8.log": {
					"match": false,
					"packageHash": "525f38addbdaaaa3e140723c9c2dd1078f1b8d9ab7ee262f3d3676aa3d4620a5",
					"size": 460300,
					"status": "missing-in-source"
				},
				"external-libs/bson/.gitignore": {
					"match": false,
					"packageHash": "9c1cf3205322777097ff8e8d431a18d2cae134686837ee4906f93779ddce2f47",
					"size": 30,
					"status": "missing-in-source"
				},
				"external-libs/bson/Makefile": {
					"diff": "--- published/external-libs/bson/Makefile\n+++ rebuilt/external-libs/bson/Makefile\n@@ -1,18 +1,45 @@\n-\n NODE = node\n name = all\n+JOBS = 1\n \n all:\n \trm -rf build .lock-wscript bson.node\n \tnode-waf configure build\n-\t@$(NODE) test_bson.js\n-\t@$(NODE) test_full_bson.js\n+\tcp -R ./build/Release/bson.node . || true\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n+\n+all_debug:\n+\trm -rf build .lock-wscript bson.node\n+\tnode-waf --debug configure build\n+\tcp -R ./build/Release/bson.node . || true\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n test:\n-\t@$(NODE) test_bson.js\n-\t@$(NODE) test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n+\n+clang:\n+\trm -rf build .lock-wscript bson.node\n+\tCXX=clang node-waf configure build\n+\tcp -R ./build/Release/bson.node . || true\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n+\n+clang_debug:\n+\trm -rf build .lock-wscript bson.node\n+\tCXX=clang node-waf --debug configure build\n+\tcp -R ./build/Release/bson.node . || true\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n clean:\n \trm -rf build .lock-wscript bson.node\n-\t\n+\n .PHONY: all\n\\ No newline at end of file\n",
					"match": false,
					"packageHash": "9e78d0b91294e2f961a2936ee7c0f992d14f1ec6208070c69691da832cf7e285",
					"size": 262,
					"sourceHash": "739cf490555ff92e959059d59f3ab3116a6159879968a0b0b57c5ac838e62163",
					"status": "content"
				},
				"external-libs/bson/binary.cc": {
					"match": false,
					"packageHash": "69034af53496f621a8ef4fcbb2bc4490c9ac3990766811a4c9f10d209c1e2477",
					"size": 9954,
					"status": "missing-in-source"
				},
				"external-libs/bson/binary.h": {
					"match": false,
					"packageHash": "a9d8d0ed8f6c50629bf34a28388631c99862e48b7a371dee2ff940c8251e4154",
					"size": 1553,
					"status": "missing-in-source"
				},
				"external-libs/bson/bson.cc": {
					"diff": "--- published/external-libs/bson/bson.cc\n+++ rebuilt/external-libs/bson/bson.cc\n@@ -1,27 +1,33 @@\n #include <assert.h>\n #include <string.h>\n #include <stdlib.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic push\n+#pragma clang diagnostic ignored \"-Wunused-parameter\"\n+#endif\n+\n #include <v8.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic pop\n+#endif\n+\n #include <node.h>\n #include <node_version.h>\n-#include <node_events.h>\n #include <node_buffer.h>\n #include <cstring>\n #include <cmath>\n #include <cstdlib>\n #include <iostream>\n #include <limits>\n+#include <vector>\n \n #include \"bson.h\"\n-#include \"long.h\"\n-#include \"timestamp.h\"\n-#include \"objectid.h\"\n-#include \"binary.h\"\n-#include \"code.h\"\n-#include \"dbref.h\"\n \n using namespace v8;\n using namespace node;\n+using namespace std;\n \n // BSON DATA TYPES\n const uint32_t BSON_DATA_NUMBER = 1;\n@@ -34,20 +40,23 @@\n const uint32_t BSON_DATA_DATE = 9;\n const uint32_t BSON_DATA_NULL = 10;\n const uint32_t BSON_DATA_REGEXP = 11;\n+const uint32_t BSON_DATA_CODE = 13;\n+const uint32_t BSON_DATA_SYMBOL = 14;\n const uint32_t BSON_DATA_CODE_W_SCOPE = 15;\n const uint32_t BSON_DATA_INT = 16;\n const uint32_t BSON_DATA_TIMESTAMP = 17;\n const uint32_t BSON_DATA_LONG = 18;\n+const uint32_t BSON_DATA_MIN_KEY = 0xff;\n+const uint32_t BSON_DATA_MAX_KEY = 0x7f;\n \n const int32_t BSON_INT32_MAX = (int32_t)2147483647L;\n const int32_t BSON_INT32_MIN = (int32_t)(-1) * 2147483648L;\n \n-// BSON BINARY DATA SUBTYPES\n-const uint32_t BSON_BINARY_SUBTYPE_FUNCTION = 1;\n-const uint32_t BSON_BINARY_SUBTYPE_BYTE_ARRAY = 2;\n-const uint32_t BSON_BINARY_SUBTYPE_UUID = 3;\n-const uint32_t BSON_BINARY_SUBTYPE_MD5 = 4;\n-const uint32_t BSON_BINARY_SUBTYPE_USER_DEFINED = 128;\n+const int64_t BSON_INT64_MAX = ((int64_t)1 << 63) - 1;\n+const int64_t BSON_INT64_MIN = (int64_t)-1 << 63;\n+\n+const int64_t JS_INT_MAX = (int64_t)1 << 53;\n+const int64_t JS_INT_MIN = (int64_t)-1 << 53;\n \n static Handle<Value> VException(const char *msg) {\n     HandleScope scope;\n@@ -56,38 +65,6 @@\n \n Persistent<FunctionTemplate> BSON::constructor_template;\n \n-class MyExternal : public String::ExternalAsciiStringResource {\n- public:\n-  MyExternal (char *d, size_t length) : ExternalAsciiStringResource() {\n-    data_ = static_cast<char*>(malloc(length));\n-    memcpy(data_, d, length);\n-    // data_ = d;\n-    length_ = length;\n-    \n-    // Adjust of external allocated memory\n-    V8::AdjustAmountOfExternalAllocatedMemory(sizeof(MyExternal));      \n-  }\n-\n-  virtual ~MyExternal () {\n-    // Adjust the memory allocated\n-    V8::AdjustAmountOfExternalAllocatedMemory(-length_);  \n-    // Free the string\n-    free(data_);\n-  }\n-\n-  virtual const char * data () const {\n-    return data_;\n-  }\n-\n",
					"match": false,
					"packageHash": "ebff2c7ea22a80c5d0bbd87b49a532317617a382ea970f93bf643f8483add5cb",
					"size": 64035,
					"sourceHash": "3bbf33024abd0bbd372cf8d602e8323759fdd165640ea5b8822dd862932a142e",
					"status": "content"
				},
				"external-libs/bson/bson.h": {
					"diff": "--- published/external-libs/bson/bson.h\n+++ rebuilt/external-libs/bson/bson.h\n@@ -8,50 +8,98 @@\n using namespace v8;\n using namespace node;\n \n-class BSON : public EventEmitter {\n+class BSON : public ObjectWrap {\n   public:    \n-    BSON() : EventEmitter() {}\n+    BSON() : ObjectWrap() {}\n     ~BSON() {}\n     \n     static void Initialize(Handle<Object> target);\n+    static Handle<Value> BSONDeserializeStream(const Arguments &args);\n+\n+    // JS based objects\n     static Handle<Value> BSONSerialize(const Arguments &args);\n     static Handle<Value> BSONDeserialize(const Arguments &args);\n \n-    // Encode functions\n-    static Handle<Value> EncodeLong(const Arguments &args);\n-    static Handle<Value> ToLong(const Arguments &args);\n-    static Handle<Value> ToInt(const Arguments &args);\n-  \n+    // Calculate size of function\n+    static Handle<Value> CalculateObjectSize(const Arguments &args);\n+    static Handle<Value> SerializeWithBufferAndIndex(const Arguments &args);\n+\n+  \t// Experimental\n+    static Handle<Value> CalculateObjectSize2(const Arguments &args);\n+    static Handle<Value> BSONSerialize2(const Arguments &args);\n+\n     // Constructor used for creating new BSON objects from C++\n     static Persistent<FunctionTemplate> constructor_template;\n \n   private:\n     static Handle<Value> New(const Arguments &args);\n-    static Handle<Value> deserialize(char *data, bool is_array_item);\n-    static uint32_t serialize(char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, bool check_key);\n+    static Handle<Value> deserialize(BSON *bson, char *data, uint32_t dataLength, uint32_t startIndex, bool is_array_item);\n+    static uint32_t serialize(BSON *bson, char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, bool check_key, bool serializeFunctions);\n \n     static char* extract_string(char *data, uint32_t offset);\n     static const char* ToCString(const v8::String::Utf8Value& value);\n-    static uint32_t calculate_object_size(Handle<Value> object);\n+    static uint32_t calculate_object_size(BSON *bson, Handle<Value> object, bool serializeFunctions);\n \n     static void write_int32(char *data, uint32_t value);\n     static void write_int64(char *data, int64_t value);\n     static void write_double(char *data, double value);\n-    static int deserialize_sint8(char *data, uint32_t offset);\n-    static int deserialize_sint16(char *data, uint32_t offset);\n-    static long deserialize_sint32(char *data, uint32_t offset);\n     static uint16_t deserialize_int8(char *data, uint32_t offset);\n     static uint32_t deserialize_int32(char* data, uint32_t offset);\n     static char *check_key(Local<String> key);\n-    static char *decode_utf8(char * string, uint32_t length);\n+     \n+    // BSON type instantiate functions\n+    Persistent<Function> longConstructor;\n+    Persistent<Function> objectIDConstructor;\n+    Persistent<Function> binaryConstructor;\n+    Persistent<Function> codeConstructor;\n+    Persistent<Function> dbrefConstructor;\n+    Persistent<Function> symbolConstructor;\n+    Persistent<Function> doubleConstructor;\n+    Persistent<Function> timestampConstructor;\n+    Persistent<Function> minKeyConstructor;\n+    Persistent<Function> maxKeyConstructor;\n+    \n+    // Equality Objects\n+    Persistent<String> longString;\n+    Persistent<String> objectIDString;\n+    Persistent<String> binaryString;\n+    Persistent<String> codeString;\n+    Persistent<String> dbrefString;\n+    Persistent<String> symbolString;\n+    Persistent<String> doubleString;\n+    Persistent<String> timestampString;\n+    Persistent<String> minKeyString;\n+    Persistent<String> maxKeyString;\n+    \n+    // Equality speed up comparision objects\n+    Persistent<String> _bsontypeString;\n+    Persistent<String> _longLowString;\n+    Persistent<String> _longHighString;\n+    Persistent<String> _objectIDidString;\n+    Persistent<String> _binaryPositionString;\n+    Persistent<String> _binarySubTypeString;\n+    Persistent<String> _binaryBufferString;\n+    Persistent<String> _doubleValueString;\n+    Persistent<String> _symbolValueString;\n+\n+    Persistent<String> _dbRefRefString;\n+    Persistent<String> _dbRefIdRefString;\n+    Persistent<String> _dbRefDbRefString;\n+    Persistent<String> _dbRefNamespaceString;\n+    Persistent<String> _dbRefDbString;\n+    Persistent<String> _dbRefOidString;\n",
					"match": false,
					"packageHash": "d0dd5e0c53567ede4e063edc41a62b7a239b26c0616dedc26fd75e6680d1d7f3",
					"size": 2280,
					"sourceHash": "3ac7c29a803a949feb401a361cff6d5b953f45756933d3d0815dc4833beed2ae",
					"status": "content"
				},
				"external-libs/bson/code.cc": {
					"match": false,
					"packageHash": "2247213aa8d88f7128cc77cc34e61ab737a5d71c02e26085ea168f55a91430ce",
					"size": 5651,
					"status": "missing-in-source"
				},
				"external-libs/bson/code.h": {
					"match": false,
					"packageHash": "56853f4dd68cd1dcc5e8612eb74e827b5b66d17ece160c1a5076a4c0f30a53a3",
					"size": 1402,
					"status": "missing-in-source"
				},
				"external-libs/bson/dbref.cc": {
					"match": false,
					"packageHash": "2735812c0634a029537b92d4ecb869626b1ad68d26107e451da3f04c518688bd",
					"size": 6004,
					"status": "missing-in-source"
				},
				"external-libs/bson/dbref.h": {
					"match": false,
					"packageHash": "9b78991199e9ce741f9b8ea94e900be912c2bd49686464f6d88abd3a57712384",
					"size": 1784,
					"status": "missing-in-source"
				},
				"external-libs/bson/local.cc": {
					"match": false,
					"packageHash": "05c0c129e97ca07fc2e829ddb2cf416463d533cd4d3ab195c6d6c69afa31eab0",
					"size": 504,
					"status": "missing-in-source"
				},
				"external-libs/bson/local.h": {
					"match": false,
					"packageHash": "8be6e556982f4cd8cc62d9ae0357cd9f3db763fafab4fdc12e9507dbeb986350",
					"size": 276,
					"status": "missing-in-source"
				},
				"external-libs/bson/long.cc": {
					"match": false,
					"packageHash": "94df3aea1892135a90f7b7ae50bf27d73f4bd79720602edda469e7d0d12afb29",
					"size": 23509,
					"status": "missing-in-source"
				},
				"external-libs/bson/long.h": {
					"match": false,
					"packageHash": "702466e672945407700baacd42aa7c1944303894e73a016d7464cdd46e218126",
					"size": 2636,
					"status": "missing-in-source"
				},
				"external-libs/bson/objectid.cc": {
					"match": false,
					"packageHash": "4c6a43c275ec964478cb0492713edaed175729e2bf60e879c9a83edfa0472cc2",
					"size": 10777,
					"status": "missing-in-source"
				},
				"external-libs/bson/objectid.h": {
					"match": false,
					"packageHash": "a00c1c4ee8de5be0d27a9d599a7da79d8d5e66e99699ffd0dc5fade446ae3fa9",
					"size": 1629,
					"status": "missing-in-source"
				},
				"external-libs/bson/test_bson.js": {
					"match": false,
					"packageHash": "2710b2f1975f4027367e14153d0e98b28c6c4f11a98f4bad01c01e86a6aa5d2b",
					"size": 11219,
					"status": "missing-in-source"
				},
				"external-libs/bson/test_full_bson.js": {
					"match": false,
					"packageHash": "cd8847defbb57b146bb5baf9961c8007eb1f4285dff1cfde01267c3b5e191460",
					"size": 7357,
					"status": "missing-in-source"
				},
				"external-libs/bson/timestamp.cc": {
					"match": false,
					"packageHash": "547b2936881042f691df521d71ed9c940b9f83cfb0e490e2012c5f2d2e360ca3",
					"size": 24476,
					"status": "missing-in-source"
				},
				"external-libs/bson/timestamp.h": {
					"match": false,
					"packageHash": "c71a9cb52baae2ebaa5e076f2d5f2cf1356703db1d654cdfdbcf9c3cbdc98b0e",
					"size": 2855,
					"status": "missing-in-source"
				},
				"external-libs/bson/wscript": {
					"diff": "--- published/external-libs/bson/wscript\n+++ rebuilt/external-libs/bson/wscript\n@@ -18,14 +18,15 @@\n def configure(conf):\n   conf.check_tool(\"compiler_cxx\")\n   conf.check_tool(\"node_addon\")\n-  # conf.env.append_value('CXXFLAGS', ['-DDEBUG', '-g', '-O0', '-Wall', '-Wextra'])\n+  conf.env.append_value('CXXFLAGS', ['-O3', '-funroll-loops'])\n \n+  # conf.env.append_value('CXXFLAGS', ['-DDEBUG', '-g', '-O0', '-Wall', '-Wextra'])\n   # conf.check(lib='node', libpath=['/usr/lib', '/usr/local/lib'], uselib_store='NODE')\n \n def build(bld):\n   obj = bld.new_task_gen(\"cxx\", \"shlib\", \"node_addon\")\n   obj.target = \"bson\"\n-  obj.source = [\"bson.cc\", \"long.cc\", \"objectid.cc\", \"binary.cc\", \"code.cc\", \"dbref.cc\", \"timestamp.cc\", \"local.cc\"]\n+  obj.source = [\"bson.cc\"]\n   # obj.uselib = \"NODE\"\n \n def shutdown():\n",
					"match": false,
					"packageHash": "8e03c424744fc9ced4ff9b92060f1e97deaaae24cc544ecf33f0a0294628d233",
					"size": 1205,
					"sourceHash": "561b73e306e26f9f160255232a6fd86da2141e466b1d5ebff8e0cbc6423972ea",
					"status": "content"
				},
				"install.sh": {
					"match": false,
					"packageHash": "9c5932cce21d5a1be3c8f8ca52fcc573dd2269372e3d50082f4c408cf4f0ccd7",
					"size": 270,
					"status": "missing-in-source"
				},
				"integration/integration_tests.js": {
					"match": false,
					"packageHash": "5ff5eb4d8039e3180d466959e00df9c565c46edfd4df4f86df39b0364dce166f",
					"size": 185844,
					"status": "missing-in-source"
				},
				"integration/replicaTest.js": {
					"match": false,
					"packageHash": "be6844cc7ab08753d206cb20755852360007b312b5c04441f4cf54be02faf675",
					"size": 1001,
					"status": "missing-in-source"
				},
				"integration/test_gs_weird_bug.png": {
					"match": false,
					"packageHash": "357ae8923961baeaf8eb1d434f5a3f8d9ac51ae0dbb2ed412e2687615f757ebc",
					"size": 52184,
					"status": "missing-in-source"
				},
				"integration/test_gs_working_field_read.pdf": {
					"match": false,
					"packageHash": "c866f44c43c7554e4b25394fa52901ee8c8c176649d4cd7f4dd4e92e8d4bd4f8",
					"size": 130253,
					"status": "missing-in-source"
				},
				"lib/mongodb/admin.js": {
					"diff": "--- published/lib/mongodb/admin.js\n+++ rebuilt/lib/mongodb/admin.js\n@@ -1,29 +1,90 @@\n+/*!\n+ * Module dependencies.\n+ */\n var Collection = require('./collection').Collection,\n     Cursor = require('./cursor').Cursor,\n     DbCommand = require('./commands/db_command').DbCommand;\n \n-var Admin = exports.Admin = function(db) {  \n+/**\n+ * Allows the user to access the admin functionality of MongoDB\n+ *\n+ * @class Represents the Admin methods of MongoDB.\n+ * @param {Object} db Current db instance we wish to perform Admin operations on.\n+ * @return {Function} Constructor for Admin type.\n+ */\n+function Admin(db) {  \n+  if(!(this instanceof Admin)) return new Admin(db);\n+  \n   this.db = db;\n };\n \n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n+Admin.prototype.buildInfo = function(callback) {\n+  this.serverInfo(callback);\n+}\n+\n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api private\n+ */\n Admin.prototype.serverInfo = function(callback) {\n   var self = this;\n   var command = {buildinfo:1};\n-  var databaseName = self.db.databaseName;\n-  self.db.databaseName = 'admin';\n-  this.db.executeDbCommand(command, function(err, doc) {\n+  this.command(command, function(err, doc) {\n     if(err != null) return callback(err, null);\n     return callback(null, doc.documents[0]);\n   });\n-  // Ensure change before event loop executes\n-  self.db.databaseName = databaseName;\n }\n \n+/**\n+ * Retrieve this db's server status.\n+ *\n+ * @param {Function} callback returns the server status.\n+ * @return {null}\n+ * @api public\n+ */\n+Admin.prototype.serverStatus = function(callback) {\n+  var self = this;\n+\n+  this.command({serverStatus: 1}, function(err, result) {\n+    if (err == null && result.documents[0].ok == 1) {\n+      callback(null, result.documents[0]);\n+    } else {\n+      if (err) {\n+        callback(err, false);\n+      } else {\n+        callback(self.wrap(result.documents[0]), false);\n+      }\n+    }\n+  });\n+};\n+\n+/**\n+ * Retrieve the current profiling Level for MongoDB\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n Admin.prototype.profilingLevel = function(callback) {\n+  var self = this;\n   var command = {profile:-1};\n-  this.db.executeDbCommand(command, function(err, doc) {\n+\n+  this.command(command, function(err, doc) {\n     doc = doc.documents[0];\n-    if(err == null && (doc.ok == 1 || doc.was.constructor == Numeric)) {\n+    \n+    if(err == null && (doc.ok == 1 || typeof doc.was === 'number')) {\n       var was = doc.was;\n       if(was == 0) {\n",
					"match": false,
					"packageHash": "9323f5545c77655815ec25a19d0e830de41b3e1b713b7a819402662242d708df",
					"size": 2879,
					"sourceHash": "a037f0b049bc29d04bfc3ee04e7483bbe7b8750c68b95e6c0b45b94b67897af1",
					"status": "content"
				},
				"lib/mongodb/bson/binary_parser.js": {
					"match": false,
					"packageHash": "50b9670fd67131ec884d3d086cc4b4fbaf67cadd8c91c1a32dc179d71d436e43",
					"size": 10772,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/bson.js": {
					"match": false,
					"packageHash": "faa86aebef93624d7d0c2d60fd56596026f551dd84889b6fad649f86c5ca61c2",
					"size": 27378,
					"status": "missing-in-source"
				},
				"lib/mongodb/collection.js": {
					"diff": "--- published/lib/mongodb/collection.js\n+++ rebuilt/lib/mongodb/collection.js\n@@ -1,632 +1,1508 @@\n-var InsertCommand = require('./commands/insert_command').InsertCommand,\n-  QueryCommand = require('./commands/query_command').QueryCommand,\n-  DeleteCommand = require('./commands/delete_command').DeleteCommand,\n-  UpdateCommand = require('./commands/update_command').UpdateCommand,\n-  DbCommand = require('./commands/db_command').DbCommand,\n-  BinaryParser = require('./bson/binary_parser').BinaryParser,\n-  Cursor = require('./cursor').Cursor;\n-\n+/**\n+ * Module dependencies.\n+ * @ignore\n+ */\n+var InsertCommand = require('./commands/insert_command').InsertCommand\n+  , QueryCommand = require('./commands/query_command').QueryCommand\n+  , DeleteCommand = require('./commands/delete_command').DeleteCommand\n+  , UpdateCommand = require('./commands/update_command').UpdateCommand\n+  , DbCommand = require('./commands/db_command').DbCommand\n+  , ObjectID = require('bson').ObjectID\n+  , Code = require('bson').Code\n+  , Cursor = require('./cursor').Cursor\n+  , utils = require('./utils');\n \n /**\n-  Sort functions, Normalize and prepare sort parameters\n+ * Precompiled regexes\n+ * @ignore\n **/\n-var formatSortValue = function(sortDirection) {\n-  var value = (\"\" + sortDirection).toLowerCase();\n-  if(value == 'ascending' || value == 'asc' || value == 1) return 1;\n-  if(value == 'descending' || value == 'desc' || value == -1 ) return -1;\n-  throw new Error(\"Illegal sort clause, must be of the form \" +\n-    \"[['field1', '(ascending|descending)'], ['field2', '(ascending|descending)']]\");\n-};\n+const eErrorMessages = /No matching object found/;\n \n-var formattedOrderClause = function(sortValue) {\n-  var orderBy = {};\n-  var self = this;\n-\n-  if(Array.isArray(sortValue)) {\n-    sortValue.forEach(function(sortElement) {\n-      if(sortElement.constructor == String) {\n-        orderBy[sortElement] = 1;\n-      } else {\n-        orderBy[sortElement[0]] = formatSortValue(sortElement[1]);\n-      }\n-    });\n-  } else if(sortValue.constructor == String) {\n-    orderBy[sortValue] = 1;\n-  } else {\n-    throw new Error(\"Illegal sort clause, must be of the form \" +\n-      \"[['field1', '(ascending|descending)'], ['field2', '(ascending|descending)']]\");\n-  }\n-  return orderBy;\n-};\n+/**\n+ * toString helper.\n+ * @ignore\n+ */\n+var toString = Object.prototype.toString;\n \n /**\n-  Handles all the operations on objects in collections\n-**/\n-var Collection = exports.Collection = function(db, collectionName, pkFactory) {\n+ * Create a new Collection instance\n+ *\n+ * Options\n+ *  - **slaveOk** {Boolean, default:false}, Allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions on the document.\n+ *  - **raw** {Boolean, default:false}, perform all operations using raw bson objects.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *\n+ * @class Represents a Collection\n+ * @param {Object} db db instance.\n+ * @param {String} collectionName collection name.\n+ * @param {Object} [pkFactory] alternative primary key factory.\n+ * @param {Object} [options] additional options for the collection.\n+ * @return {Object} a collection instance.\n+ */\n+function Collection (db, collectionName, pkFactory, options) {\n+  if(!(this instanceof Collection)) return new Collection(db, collectionName, pkFactory, options);\n+  \n+  checkCollectionName(collectionName);\n+\n   this.db = db;\n   this.collectionName = collectionName;\n-  this.internalHint;\n-  this.pkFactory = pkFactory == null ? db.bson_serializer.ObjectID : pkFactory;\n-  // Add getter and setters\n-  this.__defineGetter__(\"hint\", function() { return this.internalHint; });\n-  this.__defineSetter__(\"hint\", function(value) { this.internalHint = this.normalizeHintField(value); });\n-  // Ensure the collection name is not illegal\n-  this.checkCollectionName(collectionName);\n-};\n+  this.internalHint = null;\n",
					"match": false,
					"packageHash": "580a64aeb5c1b36905378e05847c938768ffa0e68cb05f8c2938414818ce6c7a",
					"size": 23204,
					"sourceHash": "a3563efa6644f7f9f404b47e0386f1ca647574fd634b04b427ee5ef9c05c8b56",
					"status": "content"
				},
				"lib/mongodb/commands/base_command.js": {
					"diff": "--- published/lib/mongodb/commands/base_command.js\n+++ rebuilt/lib/mongodb/commands/base_command.js\n@@ -1,28 +1,20 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser;\n-\n /**\n   Base object used for common functionality\n **/\n var BaseCommand = exports.BaseCommand = function() {\n };\n \n-BaseCommand.prototype.toBinary = function() {\n-  // Get the command op code\n-  var op_code = this.getOpCode();\n-  // Get the command data structure\n-  var command = this.getCommand();\n-  // Total Size of command\n-  var totalSize = 4*4 + command.length;\n-  // Create the command with the standard header file\n-  return BinaryParser.fromInt(totalSize) + BinaryParser.fromInt(this.requestId) + BinaryParser.fromInt(0) + BinaryParser.fromInt(op_code) + command;\n-};\n-\n var id = 1;\n BaseCommand.prototype.getRequestId = function() {\n   if (!this.requestId) this.requestId = id++;\n   return this.requestId;\n };\n \n+BaseCommand.prototype.updateRequestId = function() {\n+  this.requestId = id++;\n+  return this.requestId;\n+};\n+\n // OpCodes\n BaseCommand.OP_REPLY = 1;\n BaseCommand.OP_MSG = 1000;\n@@ -32,4 +24,4 @@\n BaseCommand.OP_QUERY = 2004;\n BaseCommand.OP_GET_MORE = 2005;\n BaseCommand.OP_DELETE =\t2006;\n-BaseCommand.OP_KILL_CURSORS =\t2007;\n+BaseCommand.OP_KILL_CURSORS =\t2007;\n\\ No newline at end of file\n",
					"match": false,
					"packageHash": "4d899faab07d1f313aad22479001f9c819ae5bc298c64261b1609d6854d4b497",
					"size": 1052,
					"sourceHash": "3b9d5bf8d0033df72262cdd5c60feb91a4052597696bb1123bae0beeda3f9aa9",
					"status": "content"
				},
				"lib/mongodb/commands/db_command.js": {
					"diff": "--- published/lib/mongodb/commands/db_command.js\n+++ rebuilt/lib/mongodb/commands/db_command.js\n@@ -1,21 +1,27 @@\n var QueryCommand = require('./query_command').QueryCommand,\n   InsertCommand = require('./insert_command').InsertCommand,\n-  MD5 = require('../crypto/md5').MD5,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits,\n+  crypto = require('crypto');\n \n /**\n   Db Command\n **/\n-var DbCommand = exports.DbCommand = function(db, collectionName, queryOptions, numberToSkip, numberToReturn, query, returnFieldSelector) {\n-  QueryCommand.call(db, this);\n-\n+var DbCommand = exports.DbCommand = function(dbInstance, collectionName, queryOptions, numberToSkip, numberToReturn, query, returnFieldSelector, options) {\n+  QueryCommand.call(this);\n   this.collectionName = collectionName;\n   this.queryOptions = queryOptions;\n   this.numberToSkip = numberToSkip;\n   this.numberToReturn = numberToReturn;\n   this.query = query;\n   this.returnFieldSelector = returnFieldSelector;\n-  this.db = db;\n+  this.db = dbInstance;\n+\n+  // Make sure we don't get a null exception\n+  options = options == null ? {} : options;\n+  // Let us defined on a command basis if we want functions to be serialized or not\n+  if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n+    this.serializeFunctions = true;\n+  }\n };\n \n inherits(DbCommand, QueryCommand);\n@@ -27,6 +33,11 @@\n DbCommand.SYSTEM_USER_COLLECTION = \"system.users\";\n DbCommand.SYSTEM_COMMAND_COLLECTION = \"$cmd\";\n \n+// New commands\n+DbCommand.NcreateIsMasterCommand = function(db, databaseName) {\n+  return new DbCommand(db, databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'ismaster':1}, null);\n+};\n+\n // Provide constructors for different db commands\n DbCommand.createIsMasterCommand = function(db) {\n   return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'ismaster':1}, null);\n@@ -41,12 +52,19 @@\n };\n \n DbCommand.createAuthenticationCommand = function(db, username, password, nonce) {\n+  // Use node md5 generator\n+  var md5 = crypto.createHash('md5');\n   // Generate keys used for authentication\n-  var hash_password = MD5.hex_md5(username + \":mongo:\" + password);\n-  var key = MD5.hex_md5(nonce + username + hash_password);\n+  md5.update(username + \":mongo:\" + password);\n+  var hash_password = md5.digest('hex');\n+  // Final key\n+  md5 = crypto.createHash('md5');\n+  md5.update(nonce + username + hash_password);\n+  var key = md5.digest('hex');  \n+  // Creat selector\n   var selector = {'authenticate':1, 'user':username, 'nonce':nonce, 'key':key};\n   // Create db command\n-  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, selector, null);\n+  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NONE, 0, -1, selector, null);\n };\n \n DbCommand.createLogoutCommand = function(db) {\n@@ -73,14 +91,27 @@\n   return new DbCommand(db, \"admin.\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'renameCollection':renameCollection, 'to':toCollection}, null);\n };\n \n-DbCommand.createGetLastErrorCommand = function(db) {\n-  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'getlasterror':1}, null);\n-};\n+DbCommand.createGetLastErrorCommand = function(options, db) {\n \n-DbCommand.createGetLastStatusCommand = function(db) {\n-  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'getlasterror':1}, null);\n+  if (typeof db === 'undefined') {\n+    db =  options;\n+    options = {};\n+  }\n+  // Final command \n+  var command = {'getlasterror':1};\n+  // If we have an options Object let's merge in the fields (fsync/wtimeout/w)\n+  if('object' === typeof options) {\n+    for(var name in options) {\n+      command[name] = options[name]\n+    }\n+  }\n+  \n+  // Execute command\n+  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, command, null);\n };\n \n+DbCommand.createGetLastStatusCommand = DbCommand.createGetLastErrorCommand;\n",
					"match": false,
					"packageHash": "4e9b4a1f6af0fb990607151b676e905896710227f0c49ef90e337f78dfb56b41",
					"size": 6852,
					"sourceHash": "6f41be547a863b0b0169eba3b2ecc8d56a3ca07357817dae0fd22c2063b2d8ed",
					"status": "content"
				},
				"lib/mongodb/commands/delete_command.js": {
					"diff": "--- published/lib/mongodb/commands/delete_command.js\n+++ rebuilt/lib/mongodb/commands/delete_command.js\n@@ -1,6 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -8,6 +7,17 @@\n var DeleteCommand = exports.DeleteCommand = function(db, collectionName, selector) {\n   BaseCommand.call(this);\n \n+  // Validate correctness off the selector\n+  var object = selector;\n+  if(Buffer.isBuffer(object)) {\n+    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;        \n+    if(object_size != object.length)  {\n+      var error = new Error(\"delete raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n+  \n   this.collectionName = collectionName;\n   this.selector = selector;\n   this.db = db;\n@@ -15,9 +25,7 @@\n \n inherits(DeleteCommand, BaseCommand);\n \n-DeleteCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_DELETE;\n-};\n+DeleteCommand.OP_DELETE =\t2006;\n \n /*\n struct {\n@@ -28,7 +36,76 @@\n     mongo.BSON      selector;               // query object.  See below for details.\n }\n */\n-DeleteCommand.prototype.getCommand = function() {\n-  // Generate the command string\n-  return BinaryParser.fromInt(0) + BinaryParser.encode_cstring(this.collectionName) + BinaryParser.fromInt(0) + this.db.bson_serializer.BSON.serialize(this.selector);\n+DeleteCommand.prototype.toBinary = function() {\n+  // Calculate total length of the document\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.selector, false, true) + (4 * 4);\n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n+  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index] = totalLengthOfCommand & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write the request ID\n+  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n+  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n+  _command[_index + 1] = (this.requestId >> 8) & 0xff;\n+  _command[_index] = this.requestId & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  // Write the op_code for the command\n+  _command[_index + 3] = (DeleteCommand.OP_DELETE >> 24) & 0xff;     \n+  _command[_index + 2] = (DeleteCommand.OP_DELETE >> 16) & 0xff;\n+  _command[_index + 1] = (DeleteCommand.OP_DELETE >> 8) & 0xff;\n+  _command[_index] = DeleteCommand.OP_DELETE & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+\n+  // Write the collection name to the command\n+  _index = _index + _command.write(this.collectionName, _index, 'utf8') + 1;\n+  _command[_index - 1] = 0;    \n+\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+\n+  // Document binary length\n+  var documentLength = 0\n+\n+  // Serialize the selector\n+  // If we are passing a raw buffer, do minimal validation\n",
					"match": false,
					"packageHash": "bb4e73589d1ef5f5c89d2be4730aefef13bdfc32879d389e7dd2620e246d8392",
					"size": 1140,
					"sourceHash": "4e4d0f12685765c7a6eb62947f5f73432a07453a99925a1a0e1a63d53d4d218a",
					"status": "content"
				},
				"lib/mongodb/commands/get_more_command.js": {
					"diff": "--- published/lib/mongodb/commands/get_more_command.js\n+++ rebuilt/lib/mongodb/commands/get_more_command.js\n@@ -1,6 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits,\n+  binaryutils = require('../utils');\n \n /**\n   Get More Document Command\n@@ -16,11 +16,68 @@\n \n inherits(GetMoreCommand, BaseCommand);\n \n-GetMoreCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_GET_MORE;\n-};\n+GetMoreCommand.OP_GET_MORE = 2005;\n+\n+GetMoreCommand.prototype.toBinary = function() {\n+  // Calculate total length of the document\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 8 + (4 * 4);\n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index++] = totalLengthOfCommand & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 24) & 0xff;     \n+\n+  // Write the request ID\n+  _command[_index++] = this.requestId & 0xff;\n+  _command[_index++] = (this.requestId >> 8) & 0xff;\n+  _command[_index++] = (this.requestId >> 16) & 0xff;\n+  _command[_index++] = (this.requestId >> 24) & 0xff;     \n+\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+\n+  // Write the op_code for the command\n+  _command[_index++] = GetMoreCommand.OP_GET_MORE & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 8) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 16) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 24) & 0xff;     \n+\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+\n+  // Write the collection name to the command\n+  _index = _index + _command.write(this.collectionName, _index, 'utf8') + 1;\n+  _command[_index - 1] = 0;    \n \n-GetMoreCommand.prototype.getCommand = function() {\n-  // Generate the command string\n-  return BinaryParser.fromInt(0) + BinaryParser.encode_cstring(this.collectionName) + BinaryParser.fromInt(this.numberToReturn) + this.db.bson_serializer.BSON.encodeLong(this.cursorId);\n+  // Number of documents to return\n+  _command[_index++] = this.numberToReturn & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 8) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 16) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 24) & 0xff;     \n+  \n+  // Encode the cursor id\n+  var low_bits = this.cursorId.getLowBits();\n+  // Encode low bits\n+  _command[_index++] = low_bits & 0xff;\n+  _command[_index++] = (low_bits >> 8) & 0xff;\n+  _command[_index++] = (low_bits >> 16) & 0xff;\n+  _command[_index++] = (low_bits >> 24) & 0xff;     \n+  \n+  var high_bits = this.cursorId.getHighBits();\n+  // Encode high bits\n+  _command[_index++] = high_bits & 0xff;\n+  _command[_index++] = (high_bits >> 8) & 0xff;\n+  _command[_index++] = (high_bits >> 16) & 0xff;\n+  _command[_index++] = (high_bits >> 24) & 0xff;     \n+  // Return command\n+  return _command;\n };\n\\ No newline at end of file\n",
					"match": false,
					"packageHash": "743c8fc634a4b9d7d0e0572ca69a15097b73f219f1718578d8c228c6200dc076",
					"size": 854,
					"sourceHash": "9c9ec06d4ad6306c6823afb357aa2f5a41fffc9ac401894f0ba240e13b72a592",
					"status": "content"
				},
				"lib/mongodb/commands/insert_command.js": {
					"diff": "--- published/lib/mongodb/commands/insert_command.js\n+++ rebuilt/lib/mongodb/commands/insert_command.js\n@@ -1,30 +1,53 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n **/\n-var InsertCommand = exports.InsertCommand = function(db, collectionName, checkKeys) {\n+var InsertCommand = exports.InsertCommand = function(db, collectionName, checkKeys, options) {\n   BaseCommand.call(this);\n \n   this.collectionName = collectionName;\n   this.documents = [];\n   this.checkKeys = checkKeys == null ? true : checkKeys;\n   this.db = db;\n+  this.flags = 0;\n+  this.serializeFunctions = false;\n+  \n+  // Ensure valid options hash\n+  options = options == null ? {} : options;\n+\n+  // Check if we have keepGoing set -> set flag if it's the case\n+  if(options['keepGoing'] != null && options['keepGoing']) {\n+    // This will finish inserting all non-index violating documents even if it returns an error\n+    this.flags = 1;\n+  }\n+  \n+  // Let us defined on a command basis if we want functions to be serialized or not\n+  if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n+    this.serializeFunctions = true;\n+  }\n };\n \n inherits(InsertCommand, BaseCommand);\n \n+// OpCodes\n+InsertCommand.OP_INSERT =\t2002;\n+\n InsertCommand.prototype.add = function(document) {\n+  if(Buffer.isBuffer(document)) {\n+    var object_size = document[0] | document[1] << 8 | document[2] << 16 | document[3] << 24;    \n+    if(object_size != document.length)  {\n+      var error = new Error(\"insert raw message size does not match message header size [\" + document.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n+  \n   this.documents.push(document);\n   return this;\n };\n \n-InsertCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_INSERT;\n-};\n-\n /*\n struct {\n     MsgHeader header;             // standard message header\n@@ -33,11 +56,86 @@\n     BSON[]    documents;          // one or more documents to insert into the collection\n }\n */\n-InsertCommand.prototype.getCommand = function() {\n-  var command_string = '';\n+InsertCommand.prototype.toBinary = function() {\n+  // Calculate total length of the document\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + (4 * 4);\n+  // var docLength = 0\n   for(var i = 0; i < this.documents.length; i++) {\n-    command_string = command_string + this.db.bson_serializer.BSON.serialize(this.documents[i], this.checkKeys);\n+    if(Buffer.isBuffer(this.documents[i])) {\n+      totalLengthOfCommand += this.documents[i].length;\n+    } else {\n+      // Calculate size of document\n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.documents[i], this.serializeFunctions, true);\n+    }\n   }\n-  // Build the command string\n-  return BinaryParser.fromInt(0) + BinaryParser.encode_cstring(this.collectionName) + command_string;\n-};\n\\ No newline at end of file\n+  \n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n+  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index] = totalLengthOfCommand & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write the request ID\n+  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n+  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n",
					"match": false,
					"packageHash": "597420b7cbb2aa9cb083f3a78f0c19e62719a28788e9805a903884e15e76e342",
					"size": 1346,
					"sourceHash": "90970dad921cc155b641d84092b4cee8d1b9f7486e4199685a856dc4fb4ed19d",
					"status": "content"
				},
				"lib/mongodb/commands/kill_cursor_command.js": {
					"diff": "--- published/lib/mongodb/commands/kill_cursor_command.js\n+++ rebuilt/lib/mongodb/commands/kill_cursor_command.js\n@@ -1,6 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits,\n+  binaryutils = require('../utils');\n \n /**\n   Insert Document Command\n@@ -14,9 +14,7 @@\n \n inherits(KillCursorCommand, BaseCommand);\n \n-KillCursorCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_KILL_CURSORS;\n-};\n+KillCursorCommand.OP_KILL_CURSORS = 2007;\n \n /*\n struct {\n@@ -26,12 +24,75 @@\n     int64[]   cursorIDs;                // array of cursorIDs to close\n }\n */\n-KillCursorCommand.prototype.getCommand = function() {\n-  var self = this;\n-  // Generate the command string\n-  var command_string = BinaryParser.fromInt(0) + BinaryParser.fromInt(this.cursorIds.length);\n-  this.cursorIds.forEach(function(cursorId) {\n-    command_string = command_string + self.db.bson_serializer.BSON.encodeLong(cursorId);\n-  });\n-  return command_string;\n+KillCursorCommand.prototype.toBinary = function() {\n+  // Calculate total length of the document\n+  var totalLengthOfCommand = 4 + 4 + (4 * 4) + (this.cursorIds.length * 8);\n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n+  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index] = totalLengthOfCommand & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write the request ID\n+  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n+  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n+  _command[_index + 1] = (this.requestId >> 8) & 0xff;\n+  _command[_index] = this.requestId & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  // Write the op_code for the command\n+  _command[_index + 3] = (KillCursorCommand.OP_KILL_CURSORS >> 24) & 0xff;     \n+  _command[_index + 2] = (KillCursorCommand.OP_KILL_CURSORS >> 16) & 0xff;\n+  _command[_index + 1] = (KillCursorCommand.OP_KILL_CURSORS >> 8) & 0xff;\n+  _command[_index] = KillCursorCommand.OP_KILL_CURSORS & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+\n+  // Number of cursors to kill\n+  var numberOfCursors = this.cursorIds.length;\n+  _command[_index + 3] = (numberOfCursors >> 24) & 0xff;     \n+  _command[_index + 2] = (numberOfCursors >> 16) & 0xff;\n+  _command[_index + 1] = (numberOfCursors >> 8) & 0xff;\n+  _command[_index] = numberOfCursors & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+\n+  // Encode all the cursors\n+  for(var i = 0; i < this.cursorIds.length; i++) {\n+    // Encode the cursor id\n+    var low_bits = this.cursorIds[i].getLowBits();\n+    // Encode low bits\n+    _command[_index + 3] = (low_bits >> 24) & 0xff;     \n+    _command[_index + 2] = (low_bits >> 16) & 0xff;\n+    _command[_index + 1] = (low_bits >> 8) & 0xff;\n+    _command[_index] = low_bits & 0xff;\n+    // Adjust index\n+    _index = _index + 4;\n+      \n+    var high_bits = this.cursorIds[i].getHighBits();\n+    // Encode high bits\n+    _command[_index + 3] = (high_bits >> 24) & 0xff;     \n+    _command[_index + 2] = (high_bits >> 16) & 0xff;\n+    _command[_index + 1] = (high_bits >> 8) & 0xff;\n+    _command[_index] = high_bits & 0xff;\n",
					"match": false,
					"packageHash": "0ad08c529bcfcbabd020fe86bd9d54260b079a031dac460af6fe024f6f360726",
					"size": 1149,
					"sourceHash": "d01af72568b490f4e88bb07da854ed5b050494e55e90b60b8b3d11561f8ffa5e",
					"status": "content"
				},
				"lib/mongodb/commands/query_command.js": {
					"diff": "--- published/lib/mongodb/commands/query_command.js\n+++ rebuilt/lib/mongodb/commands/query_command.js\n@@ -1,13 +1,37 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n **/\n-var QueryCommand = exports.QueryCommand = function(db, collectionName, queryOptions, numberToSkip, numberToReturn, query, returnFieldSelector) {\n+var QueryCommand = exports.QueryCommand = function(db, collectionName, queryOptions, numberToSkip, numberToReturn, query, returnFieldSelector, options) {\n   BaseCommand.call(this);\n+\n+  // Validate correctness off the selector\n+  var object = query,\n+    object_size;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n+      var error = new Error(\"query selector raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n+\n+  object = returnFieldSelector;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n+      var error = new Error(\"query fields raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n   \n+  // Make sure we don't get a null exception\n+  options = options == null ? {} : options;\n+  // Set up options\n   this.collectionName = collectionName;\n   this.queryOptions = queryOptions;\n   this.numberToSkip = numberToSkip;\n@@ -15,13 +39,16 @@\n   this.query = query;\n   this.returnFieldSelector = returnFieldSelector;\n   this.db = db;\n+  \n+  // Let us defined on a command basis if we want functions to be serialized or not\n+  if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n+    this.serializeFunctions = true;\n+  }\n };\n \n inherits(QueryCommand, BaseCommand);\n \n-QueryCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_QUERY;\n-};\n+QueryCommand.OP_QUERY = 2004;\n \n /*\n struct {\n@@ -34,16 +61,143 @@\n   [ BSON      returnFieldSelector; ]  // OPTIONAL : selector indicating the fields to return.  See below for details.\n }\n */\n-QueryCommand.prototype.getCommand = function() {\n-  // Generate the command string\n-  var command_string = BinaryParser.fromInt(this.queryOptions) + BinaryParser.encode_cstring(this.collectionName);\n-  command_string = command_string + BinaryParser.fromInt(this.numberToSkip) + BinaryParser.fromInt(this.numberToReturn);\n-  command_string = command_string + this.db.bson_serializer.BSON.serialize(this.query);\n-  if(this.returnFieldSelector != null)  {\n-    var count = 0; for(var name in this.returnFieldSelector) { count += 1; }\n-    if(count > 0) command_string = command_string + this.db.bson_serializer.BSON.serialize(this.returnFieldSelector);\n+QueryCommand.prototype.toBinary = function() {\n+  var totalLengthOfCommand = 0;\n+  // Calculate total length of the document\n+  if(Buffer.isBuffer(this.query)) {\n+    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.query.length + (4 * 4);    \n+  } else {\n+    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.db.bson.calculateObjectSize(this.query, this.serializeFunctions, true) + (4 * 4);    \n+  }\n+  \n+  // Calculate extra fields size\n+  if(this.returnFieldSelector != null && !(Buffer.isBuffer(this.returnFieldSelector)))  {\n+    if(Object.keys(this.returnFieldSelector).length > 0) {\n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.returnFieldSelector, this.serializeFunctions, true);\n+    }\n+  } else if(Buffer.isBuffer(this.returnFieldSelector)) {\n+    totalLengthOfCommand += this.returnFieldSelector.length;\n   }\n-  return command_string;\n+\n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n+  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n",
					"match": false,
					"packageHash": "d0bb00e2103b70b9ad7fe08fe8adb8bb75dffc1be701937ee1d45c29caf9c1a2",
					"size": 2203,
					"sourceHash": "42e8d9b5c0118fe3e93e96f00a9998faafbc79b058168904ec3341353c71240b",
					"status": "content"
				},
				"lib/mongodb/commands/update_command.js": {
					"diff": "--- published/lib/mongodb/commands/update_command.js\n+++ rebuilt/lib/mongodb/commands/update_command.js\n@@ -1,17 +1,37 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('sys').inherits;\n+  inherits = require('util').inherits;\n \n /**\n   Update Document Command\n **/\n var UpdateCommand = exports.UpdateCommand = function(db, collectionName, spec, document, options) {\n-  BaseCommand.apply(this);\n+  BaseCommand.call(this);\n+\n+  var object = spec;\n+  if(Buffer.isBuffer(object)) {\n+    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n+    if(object_size != object.length)  {\n+      var error = new Error(\"update spec raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n+\n+  var object = document;\n+  if(Buffer.isBuffer(object)) {\n+    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n+    if(object_size != object.length)  {\n+      var error = new Error(\"update document raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n+      error.name = 'MongoError';\n+      throw error;\n+    }\n+  }\n \n   this.collectionName = collectionName;\n   this.spec = spec;\n   this.document = document;\n   this.db = db;\n+  this.serializeFunctions = false;\n \n   // Generate correct flags\n   var db_upsert = 0;\n@@ -21,14 +41,15 @@\n \n   // Flags\n   this.flags = parseInt(db_multi_update.toString() + db_upsert.toString(), 2);\n+  // Let us defined on a command basis if we want functions to be serialized or not\n+  if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n+    this.serializeFunctions = true;\n+  }\n };\n \n inherits(UpdateCommand, BaseCommand);\n \n-\n-UpdateCommand.prototype.getOpCode = function() {\n-  return BaseCommand.OP_UPDATE;\n-};\n+UpdateCommand.OP_UPDATE = 2001;\n \n /*\n struct {\n@@ -40,10 +61,112 @@\n     BSON      document;           // the document data to update with or insert\n }\n */\n-UpdateCommand.prototype.getCommand = function() {\n-  // Generate the command string\n-  var command_string = BinaryParser.fromInt(0) + BinaryParser.encode_cstring(this.collectionName);\n-  return command_string + BinaryParser.fromInt(this.flags) + this.db.bson_serializer.BSON.serialize(this.spec) + this.db.bson_serializer.BSON.serialize(this.document, false);\n+UpdateCommand.prototype.toBinary = function() {\n+  // Calculate total length of the document\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.spec, false, true) +\n+      this.db.bson.calculateObjectSize(this.document, this.serializeFunctions, true) + (4 * 4);\n+\n+  // Let's build the single pass buffer command\n+  var _index = 0;\n+  var _command = new Buffer(totalLengthOfCommand);\n+  // Write the header information to the buffer\n+  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n+  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index] = totalLengthOfCommand & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write the request ID\n+  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n+  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n+  _command[_index + 1] = (this.requestId >> 8) & 0xff;\n+  _command[_index] = this.requestId & 0xff;\n+  // Adjust index\n+  _index = _index + 4;\n+  // Write zero\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  _command[_index++] = 0;\n+  // Write the op_code for the command\n+  _command[_index + 3] = (UpdateCommand.OP_UPDATE >> 24) & 0xff;     \n",
					"match": false,
					"packageHash": "0e80e269e59b01283fe740d688c46f531fa80f0222a6dae5b931869f541acdee",
					"size": 1803,
					"sourceHash": "1e9efcade1290911d79395a36a74e0f4bdfc7608c739482547738ac544d537d3",
					"status": "content"
				},
				"lib/mongodb/connection.js": {
					"match": false,
					"packageHash": "d86a1ba5229a7332b58b5928e2d8ffa9dd01b68835140714b141e2a5171c05d9",
					"size": 11207,
					"status": "missing-in-source"
				},
				"lib/mongodb/crypto/md5.js": {
					"match": false,
					"packageHash": "f1077dc944ce6ea4d93259b41eb52df6cff776a1ba4ce1fc590f3e5fb3b485ee",
					"size": 12334,
					"status": "missing-in-source"
				},
				"lib/mongodb/cursor.js": {
					"diff": "--- published/lib/mongodb/cursor.js\n+++ rebuilt/lib/mongodb/cursor.js\n@@ -1,45 +1,41 @@\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   GetMoreCommand = require('./commands/get_more_command').GetMoreCommand,\n   KillCursorCommand = require('./commands/kill_cursor_command').KillCursorCommand,\n-  Integer = require('./goog/math/integer').Integer,\n-  Long = require('./goog/math/long').Long;\n+  Long = require('bson').Long,\n+  CursorStream = require('./cursorstream'),\n+  utils = require('./utils');\n \n /**\n  * Constructor for a cursor object that handles all the operations on query result\n- * using find. This cursor object is unidirectional and cannot traverse backwards.\n- * As an alternative, {@link Cursor#toArray} can be used to obtain all the results.\n- * Clients should not be creating a cursor directly, but use {@link Collection#find}\n- * to acquire a cursor.\n- *\n- * @constructor\n- *\n- * @param db {Db} The database object to work with\n- * @param collection {Colleciton} The collection to query\n- * @param selector\n- * @param fields\n- * @param skip {number}\n- * @param limit {number} The number of results to return. -1 has a special meaning and\n- *     is used by {@link Db#eval}. A value of 1 will also be treated as if it were -1.\n- * @param sort {string|Array<Array<string|object> >} Please refer to {@link Cursor#sort}\n- * @param hint\n- * @param explain\n- * @param snapshot\n- * @param timeout\n- * @param tailable {?boolean}\n- * @param batchSize {?number} The number of the subset of results to request the database\n- *     to return for every request. This should initially be greater than 1 otherwise\n- *     the database will automatically close the cursor. The batch size can be set to 1\n- *     with {@link Cursor#batchSize} after performing the initial query to the database.\n- *\n- * @see Cursor#toArray\n- * @see Cursor#skip\n- * @see Cursor#sort\n- * @see Cursor#limit\n- * @see Cursor#batchSize\n- * @see Collection#find\n- * @see Db#eval\n- */\n-var Cursor = exports.Cursor = function(db, collection, selector, fields, skip, limit, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk) {\n+ * using find. This cursor object is unidirectional and cannot traverse backwards. Clients should not be creating a cursor directly, \n+ * but use find to acquire a cursor.\n+ *\n+ * @class Represents a Cursor.\n+ * @param {Db} db the database object to work with.\n+ * @param {Collection} collection the collection to query.\n+ * @param {Object} selector the query selector.\n+ * @param {Object} fields an object containing what fields to include or exclude from objects returned.\n+ * @param {Number} skip number of documents to skip.\n+ * @param {Number} limit the number of results to return. -1 has a special meaning and is used by Db.eval. A value of 1 will also be treated as if it were -1.\n+ * @param {String|Array|Object} sort the required sorting for the query.\n+ * @param {Object} hint force the query to use a specific index.\n+ * @param {Boolean} explain return the explaination of the query.\n+ * @param {Boolean} snapshot Snapshot mode assures no duplicates are returned.\n+ * @param {Boolean} timeout allow the query to timeout.\n+ * @param {Boolean} tailable allow the cursor to be tailable.\n+ * @param {Number} batchSize the number of the subset of results to request the database to return for every request. This should initially be greater than 1 otherwise the database will automatically close the cursor. The batch size can be set to 1 with cursorInstance.batchSize after performing the initial query to the database.\n+ * @param {Boolean} raw return all query documents as raw buffers (default false).\n+ * @param {Boolean} read specify override of read from source (primary/secondary).\n+ * @param {Boolean} returnKey only return the index key.\n+ * @param {Number} maxScan limit the number of items to scan.\n+ * @param {Number} min set index bounds.\n+ * @param {Number} max set index bounds.\n+ * @param {Boolean} showDiskLoc show disk location of results.\n+ * @param {String} comment you can put a $comment field on a query to make looking in the profiler logs simpler.\n+ */\n+function Cursor(db, collection, selector, fields, skip, limit\n+\t, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk, raw, read\n+\t, returnKey, maxScan, min, max, showDiskLoc, comment) {\n   this.db = db;\n   this.collection = collection;\n   this.selector = selector;\n@@ -53,11 +49,19 @@\n   this.timeout = timeout == null ? true : timeout;\n   this.tailable = tailable;\n   this.batchSizeValue = batchSize == null ? 0 : batchSize;\n-  this.slaveOk = slaveOk == null ? false : slaveOk;\n-\n+  this.slaveOk = slaveOk == null ? collection.slaveOk : slaveOk;\n+  this.raw = raw == null ? false : raw;\n+  this.read = read == null ? true : read;\n+  this.returnKey = returnKey;\n+  this.maxScan = maxScan;\n+  this.min = min;\n+  this.max = max;\n+  this.showDiskLoc = showDiskLoc;\n+  this.comment = comment;\n+  \n   this.totalNumberOfRecords = 0;\n   this.items = [];\n-  this.cursorId = this.db.bson_serializer.Long.fromInt(0);\n+  this.cursorId = Long.fromInt(0);\n",
					"match": false,
					"packageHash": "dcb840c60f972443c6228eb6ff164dd4f1eea9f1067f07e17ad9823819e325e2",
					"size": 22690,
					"sourceHash": "c4a6fb5b596bb88a2e03a3d00859e3cb8ae2ac573e12fa196743abfbf01737ea",
					"status": "content"
				},
				"lib/mongodb/db.js": {
					"diff": "--- published/lib/mongodb/db.js\n+++ rebuilt/lib/mongodb/db.js\n@@ -1,346 +1,391 @@\n+/**\n+ * Module dependencies.\n+ * @ignore\n+ */\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   DbCommand = require('./commands/db_command').DbCommand,\n-  BinaryParser = require('./bson/binary_parser').BinaryParser,\n   MongoReply = require('./responses/mongo_reply').MongoReply,\n   Admin = require('./admin').Admin,\n-  Connection = require('./connection').Connection,\n   Collection = require('./collection').Collection,\n-  Server = require('./connection').Server,\n-  ServerPair = require('./connection').ServerPair,\n-  ServerCluster = require('./connection').ServerCluster,\n-  ReplSetServers = require('./connection').ReplSetServers,\n+  Server = require('./connection/server').Server,\n+  ReplSet = require('./connection/repl_set').ReplSet,\n   Cursor = require('./cursor').Cursor,\n-  MD5 = require('./crypto/md5').MD5,\n   EventEmitter = require('events').EventEmitter,\n-  inherits = require('sys').inherits,\n-  sys = require('sys');\n+  inherits = require('util').inherits,\n+  crypto = require('crypto');\n+\n+/**\n+ * Internal class for callback storage \n+ * @ignore\n+ */\n+var CallbackStore = function() {\n+  // Make class an event emitter\n+  EventEmitter.call(this);\n+  // Add a info about call variable\n+  this._notReplied = {};\n+}\n+\n+/**\n+ * @ignore\n+ */\n+inherits(CallbackStore, EventEmitter);\n+\n+/**\n+ * Create a new Db instance.\n+ *\n+ * Options\n+ *  - **strict** {true | {w:n, wtimeout:n} | {fsync:true}, default:false}, execute insert with a getLastError command returning the result of the insert command.\n+ *  - **native_parser** {Boolean, default:false}, use c++ bson parser.\n+ *  - **forceServerObjectId** {Boolean, default:false}, force server to create _id fields instead of client.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *  - **slaveOk** {Boolean, default:false}, allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions.\n+ *  - **raw** {Boolean, default:false}, peform operations using raw bson buffers.\n+ *  - **recordQueryStats** {Boolean, default:false}, record query statistics during execution.\n+ *  - **reaper** {Boolean, default:false}, enables the reaper, timing out calls that never return.\n+ *  - **reaperInterval** {Number, default:10000}, number of miliseconds between reaper wakups.\n+ *  - **reaperTimeout** {Number, default:30000}, the amount of time before a callback times out.\n+ *  - **retryMiliSeconds** {Number, default:5000}, number of miliseconds between retries.\n+ *  - **numberOfRetries** {Number, default:5}, number of retries off connection.\n+ *\n+ * @class Represents a Collection\n+ * @param {String} databaseName name of the database.\n+ * @param {Object} serverConfig server config object.\n+ * @param {Object} [options] additional options for the collection.\n+ */\n+function Db(databaseName, serverConfig, options) {\n \n-var Db = exports.Db = function(databaseName, serverConfig, options) {\n+  if(!(this instanceof Db)) return new Db(databaseName, serverConfig, options);\n+  \n   EventEmitter.call(this);\n   this.databaseName = databaseName;\n-  this.serverConfig = serverConfig;\n-  this.options = options == null ? {} : options;\n+  this.serverConfig = serverConfig;  \n+  this.options = options == null ? {} : options;  \n+  // State to check against if the user force closed db\n+  this._applicationClosed = false;\n+  // Fetch the override flag if any\n+  var overrideUsedFlag = this.options['override_used_flag'] == null ? false : this.options['override_used_flag'];  \n+  // Verify that nobody is using this config\n+  if(!overrideUsedFlag && typeof this.serverConfig == 'object' && this.serverConfig._isUsed()) {\n+    throw new Error(\"A Server or ReplSet instance cannot be shared across multiple Db instances\");\n+  } else if(!overrideUsedFlag && typeof this.serverConfig == 'object'){\n+    // Set being used\n+    this.serverConfig._used = true;    \n+  }\n+  \n+  // Ensure we have a valid db name\n+  validateDatabaseName(databaseName);\n   \n   // Contains all the connections for the db\n   try {\n-    this.bson_serializer = this.options.native_parser ? require('../../external-libs/bson/bson') : require('./bson/bson');\n-    this.bson_deserializer = this.options.native_parser ? require('../../external-libs/bson/bson') : require('./bson/bson');      \n+    this.native_parser = this.options.native_parser;\n+    // The bson lib\n+    var bsonLib = this.bsonLib = this.options.native_parser ? require('bson').BSONNative : new require('bson').BSONPure;\n",
					"match": false,
					"packageHash": "c11bebf09008477cbd331665208e54eebb113ec1fe9fca94392629edc9684f83",
					"size": 34247,
					"sourceHash": "8975ea6eee925ca6f0b33ba4cb074d8de4c712ad9540ccd346aeb97e626a7e93",
					"status": "content"
				},
				"lib/mongodb/goog/math/integer.js": {
					"match": false,
					"packageHash": "86c20b74016703cf6d4e609bcb2aa4eeb824a533419ef82cdf5530fe241e76cc",
					"size": 22646,
					"status": "missing-in-source"
				},
				"lib/mongodb/goog/math/long.js": {
					"match": false,
					"packageHash": "fb03f43098c1f6f90e7e614f3e282071e2ab27f6f40e087fa2cb6aab8a805301",
					"size": 23354,
					"status": "missing-in-source"
				},
				"lib/mongodb/gridfs/chunk.js": {
					"diff": "--- published/lib/mongodb/gridfs/chunk.js\n+++ rebuilt/lib/mongodb/gridfs/chunk.js\n@@ -1,8 +1,8 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  sys = require('sys');\n+var Binary = require('bson').Binary,\n+  ObjectID = require('bson').ObjectID;\n \n /**\n- * Class for representing a signle chunk in GridFS.\n+ * Class for representing a single chunk in GridFS.\n  *\n  * @class\n  *\n@@ -17,36 +17,48 @@\n  * @see Chunk#buildMongoObject\n  */\n var Chunk = exports.Chunk = function(file, mongoObject) {\n+  if(!(this instanceof Chunk)) return new Chunk(file, mongoObject);\n+  \n   this.file = file;\n+  var self = this;\n   var mongoObjectFinal = mongoObject == null ? {} : mongoObject;\n-  this.objectId = mongoObjectFinal._id == null ? new file.db.bson_serializer.ObjectID() : mongoObjectFinal._id;\n+\n+  this.objectId = mongoObjectFinal._id == null ? new ObjectID() : mongoObjectFinal._id;\n   this.chunkNumber = mongoObjectFinal.n == null ? 0 : mongoObjectFinal.n;\n-  this.data = new file.db.bson_serializer.Binary();\n+  this.data = new Binary();\n \n   if(mongoObjectFinal.data == null) {\n-  } else if(mongoObjectFinal.data.constructor == String) {\n+  } else if(typeof mongoObjectFinal.data == \"string\") {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data, 'binary', 0);\n-    this.data = new file.db.bson_serializer.Binary(buffer);\n-  } else if(mongoObjectFinal.data.constructor == Array) {\n+    this.data = new Binary(buffer);\n+  } else if(Array.isArray(mongoObjectFinal.data)) {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data.join(''), 'binary', 0);\n-    this.data = new file.db.bson_serializer.Binary(buffer);\n-  } else if(mongoObjectFinal.data instanceof file.db.bson_serializer.Binary || Object.prototype.toString.call(mongoObjectFinal.data) == \"[object Binary]\") {    \n+    this.data = new Binary(buffer);\n+  } else if(mongoObjectFinal.data instanceof Binary || Object.prototype.toString.call(mongoObjectFinal.data) == \"[object Binary]\") {    \n     this.data = mongoObjectFinal.data;\n+  } else if(Buffer.isBuffer(mongoObjectFinal.data)) {\n   } else {\n     throw Error(\"Illegal chunk format\");\n   }\n   // Update position\n   this.internalPosition = 0;\n-\t/**\n-\t * The position of the read/write head\n-\t * @name position\n-\t * @lends Chunk#\n-\t * @field\n-\t */\n-  this.__defineGetter__(\"position\", function() { return this.internalPosition; });\n-  this.__defineSetter__(\"position\", function(value) { this.internalPosition = value; });\n+\n+  /**\n+   * The position of the read/write head\n+   * @name position\n+   * @lends Chunk#\n+   * @field\n+   */\n+  Object.defineProperty(this, \"position\", { enumerable: true\n+    , get: function () {\n+        return this.internalPosition;\n+      }\n+    , set: function(value) {\n+        this.internalPosition = value;\n+      }\n+  });  \n };\n \n /**\n@@ -58,9 +70,10 @@\n  *     will contain a reference to this object.\n  */\n Chunk.prototype.write = function(data, callback) {\n-  this.data.write(data.toString('binary'), this.internalPosition);\n+  this.data.write(data, this.internalPosition);\n   this.internalPosition = this.data.length();\n-  callback(null, this);\n+  if(callback != null) return callback(null, this);\n+  return this;\n };\n \n /**\n@@ -72,6 +85,9 @@\n  *     the chunk. Returns an empty String otherwise.\n  */\n Chunk.prototype.read = function(length) {  \n+  // Default to full read if no index defined\n+  length = length == null || length == 0 ? this.length() : length;\n+  \n   if(this.length() - this.internalPosition + 1 >= length) {\n     var data = this.data.read(this.internalPosition, length);\n",
					"match": false,
					"packageHash": "73a4e9e1f9279b7baed087eb9bda4fd4b86eb06d61ae4877c9bcaf0808796c3c",
					"size": 6575,
					"sourceHash": "ebb1d349b3e79a84bc1a2f0a2d8bb5679a6f63325d9d48d0665c1296119ff3a5",
					"status": "content"
				},
				"lib/mongodb/gridfs/gridstore.js": {
					"diff": "--- published/lib/mongodb/gridfs/gridstore.js\n+++ rebuilt/lib/mongodb/gridfs/gridstore.js\n@@ -6,104 +6,165 @@\n  * chunks of split files behind the scenes. More information about GridFS can be\n  * found <a href=\"http://www.mongodb.org/display/DOCS/GridFS\">here</a>.\n  */\n-\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  Chunk = require('./chunk').Chunk,\n+var Chunk = require('./chunk').Chunk,\n   DbCommand = require('../commands/db_command').DbCommand,\n-  Integer = require('../goog/math/integer').Integer,\n-  // ObjectID = require('../bson/bson').ObjectID,\n+  ObjectID = require('bson').ObjectID,\n   Buffer = require('buffer').Buffer,\n   fs = require('fs'),\n   util = require('util'),\n-  Stream = require('stream').Stream;\n+  ReadStream = require('./readstream').ReadStream;\n \n+var REFERENCE_BY_FILENAME = 0,\n+  REFERENCE_BY_ID = 1;\n \n /**\n  * A class representation of a file stored in GridFS.\n  *\n- * @class\n- *\n- * @param db {Db} A database instance to interact with.\n- * @param filename {string} The name for the file.\n- * @param mode {?string} Set the mode for this file. Available modes:\n- *     <ul>\n- *       <li>\"r\" - read only. This is the default mode.</li>\n- *       <li>\"w\" - write in truncate mode. Existing data will be overwriten</li>\n- *       <li>\"w+\" - write in edit mode.</li>\n- *     </ul>\n-\n- * @param options {?object} Optional properties to specify. Recognized keys:\n- *\n- *     <pre><code>\n- *     {\n- *       'root' : , // {string} root collection to use. Defaults to GridStore#DEFAULT_ROOT_COLLECTION\n- *       'chunk_type' : , // {string} mime type of the file. Defaults to GridStore#DEFAULT_CONTENT_TYPE\n- *       'chunk_size' : , // {number} size for the chunk. Defaults to Chunk#DEFAULT_CHUNK_SIZE.\n- *       'metadata' : , // {object} arbitrary data the user wants to store\n- *     }\n- *     </code></pre>\n- *\n- * @see <a href=\"http://www.mongodb.org/display/DOCS/GridFS+Specification\">MongoDB GridFS Specification</a>\n- */\n-var GridStore = exports.GridStore = function(db, filename, mode, options) {\n-  this.db = db;\n-  this.filename = filename;\n+ * Modes\n+ *  - **\"r\"** - read only. This is the default mode.\n+ *  - **\"w\"** - write in truncate mode. Existing data will be overwriten.\n+ *  - **w+\"** - write in edit mode.\n+ *\n+ * Options\n+ *  - **root** {String}, root collection to use. Defaults to **{GridStore.DEFAULT_ROOT_COLLECTION}**.\n+ *  - **chunk_type** {String}, mime type of the file. Defaults to **{GridStore.DEFAULT_CONTENT_TYPE}**.\n+ *  - **chunk_size** {Number}, size for the chunk. Defaults to **{Chunk.DEFAULT_CHUNK_SIZE}**.\n+ *  - **metadata** {Object}, arbitrary data the user wants to store.\n+ *\n+ * @class Represents the GridStore.\n+ * @param {Db} db A database instance to interact with.\n+ * @param {ObjectID} id an unique ObjectID for this file\n+ * @param {String} [filename] optional a filename for this file, no unique constrain on the field\n+ * @param {String} mode set the mode for this file.\n+ * @param {Object} options optional properties to specify. Recognized keys:\n+ * @return {GridStore}\n+ */\n+function GridStore(db, id, filename, mode, options) {\n+  if(!(this instanceof GridStore)) return new GridStore(db, id, filename, mode, options);\n+\n+  var self = this;\n+  this.db = db;  \n+  var _filename = filename;\n+\n+  if(typeof filename == 'string' && typeof mode == 'string') {\n+    _filename = filename;  \n+  } else if(typeof filename == 'string' && typeof mode == 'object' && mode != null) {\n+    var _mode = mode;\n+    mode = filename;\n+    options = _mode;    \n+    _filename = id;\n+  } else if(typeof filename == 'string' && mode == null) {\n+    mode = filename;\n+    _filename = id;\n+  }\n+  \n+  // set grid referencetype\n+  this.referenceBy = typeof id == 'string' ? 0 : 1;\n+  this.filename = _filename;\n+  this.fileId = typeof id == 'string' ? new ObjectID() : id;\n+  \n+  // Set up the rest\n   this.mode = mode == null ? \"r\" : mode;\n   this.options = options == null ? {} : options;\n   this.root = this.options['root'] == null ? exports.GridStore.DEFAULT_ROOT_COLLECTION : this.options['root'];\n",
					"match": false,
					"packageHash": "22754f33bfbefd7ef52e3c62101f4d4d3c023bb37fab29c9c4c849ef6423eb21",
					"size": 39004,
					"sourceHash": "3431d98442b60a3a8125b38249d63dfe6a81338d3bf41d9b86b2e8e71c0c3aac",
					"status": "content"
				},
				"lib/mongodb/index.js": {
					"diff": "--- published/lib/mongodb/index.js\n+++ rebuilt/lib/mongodb/index.js\n@@ -1,27 +1,151 @@\n-var sys = require('sys')\n-// // Add both the BSON Pure classes and the native ones\n-var BSONPure = exports.BSONPure = require('./bson/bson');\n-var BSONNative = null\n try {\n-  BSONNative = exports.BSONNative = require('../../external-libs/bson/bson');\n+  exports.BSONPure = require('bson').BSONPure;\n+  exports.BSONNative = require('bson').BSONNative;\n } catch(err) {\n+  // do nothing\n }\n \n-[\n-\t'bson/binary_parser',\n-\t\n-\t'commands/base_command', 'commands/db_command', 'commands/delete_command',\n-\t'commands/get_more_command', 'commands/insert_command', 'commands/kill_cursor_command',\n-\t'commands/query_command', 'commands/update_command',\n-\t\n-\t'responses/mongo_reply',\n-\t\n-\t'admin', 'collection', 'connection', 'cursor', 'db',\n-\t\n-\t'goog/math/integer', 'goog/math/long', 'crypto/md5',\n-\t'gridfs/chunk', 'gridfs/gridstore'\n-].forEach(function(path){\n-\tvar module = require('./' + path);\n-\tfor (var i in module)\n-\t\texports[i] = module[i];\n+[ 'commands/base_command'\n+  , 'commands/db_command'\n+  , 'commands/delete_command'\n+  , 'commands/get_more_command'\n+  , 'commands/insert_command'\n+  , 'commands/kill_cursor_command'\n+  , 'commands/query_command'\n+  , 'commands/update_command'\n+  , 'responses/mongo_reply'\n+  , 'admin'\n+  , 'collection'\n+  , 'connection/connection'\n+  , 'connection/server'\n+  , 'connection/repl_set'\n+  , 'cursor'\n+  , 'db'\n+  , 'gridfs/grid'\n+  ,\t'gridfs/chunk'\n+  , 'gridfs/gridstore'].forEach(function (path) {\n+  \tvar module = require('./' + path);\n+  \tfor (var i in module) {\n+  \t\texports[i] = module[i];\n+    }\n+\n+    // backwards compat\n+    exports.ReplSetServers = exports.ReplSet;\n+    \n+    // Add BSON Classes\n+    exports.Binary = require('bson').Binary;\n+    exports.Code = require('bson').Code;\n+    exports.DBRef = require('bson').DBRef;\n+    exports.Double = require('bson').Double;\n+    exports.Long = require('bson').Long;\n+    exports.MinKey = require('bson').MinKey;\n+    exports.MaxKey = require('bson').MaxKey;\n+    exports.ObjectID = require('bson').ObjectID;\n+    exports.Symbol = require('bson').Symbol;\n+    exports.Timestamp = require('bson').Timestamp;  \n+    \n+    // Add BSON Parser\n+    exports.BSON = require('bson').BSONPure.BSON;\n });\n+\n+// Exports all the classes for the PURE JS BSON Parser\n+exports.pure = function() {\n+  var classes = {};\n+  // Map all the classes\n+  [ 'commands/base_command'\n+    , 'commands/db_command'\n+    , 'commands/delete_command'\n+    , 'commands/get_more_command'\n+    , 'commands/insert_command'\n+    , 'commands/kill_cursor_command'\n+    , 'commands/query_command'\n+    , 'commands/update_command'\n+    , 'responses/mongo_reply'\n+    , 'admin'\n+    , 'collection'\n+    , 'connection/connection'\n+    , 'connection/server'\n+    , 'connection/repl_set'\n+    , 'cursor'\n+    , 'db'\n+    , 'gridfs/grid'\n+    ,\t'gridfs/chunk'\n+    , 'gridfs/gridstore'].forEach(function (path) {\n+    \tvar module = require('./' + path);\n+    \tfor (var i in module) {\n",
					"match": false,
					"packageHash": "f01695854213310b143a1f5c674e6fc44b87a2d821c090426124c14fa11e5caa",
					"size": 801,
					"sourceHash": "f177a8277bbee6a3ec1c3e002bda2026d47cad3da7c164edb7f615a0b9c7b360",
					"status": "content"
				},
				"lib/mongodb/responses/mongo_reply.js": {
					"diff": "--- published/lib/mongodb/responses/mongo_reply.js\n+++ rebuilt/lib/mongodb/responses/mongo_reply.js\n@@ -1,49 +1,123 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  Integer = require('../goog/math/integer').Integer,\n-  Long = require('../goog/math/long').Long;\n+var Long = require('bson').Long;\n \n /**\n   Reply message from mongo db\n **/\n-var MongoReply = exports.MongoReply = function(db, binary_reply) {\n+var MongoReply = exports.MongoReply = function() {\n   this.documents = [];\n-  var index = 0;\n+  this.index = 0;\n+};\n+\n+MongoReply.prototype.parseHeader = function(binary_reply, bson) {\n   // Unpack the standard header first\n-  var messageLength = BinaryParser.toInt(binary_reply.substr(index, 4));\n-  index = index + 4;\n+  this.messageLength = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n   // Fetch the request id for this reply\n-  this.requestId = BinaryParser.toInt(binary_reply.substr(index, 4));\n-  index = index + 4;\n+  this.requestId = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n   // Fetch the id of the request that triggered the response\n-  this.responseTo = BinaryParser.toInt(binary_reply.substr(index, 4));\n+  this.responseTo = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n   // Skip op-code field\n-  index = index + 4 + 4;\n+  this.index = this.index + 4 + 4;\n   // Unpack the reply message\n-  this.responseFlag = BinaryParser.toInt(binary_reply.substr(index, 4));\n-  index = index + 4;  \n+  this.responseFlag = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n   // Unpack the cursor id (a 64 bit long integer)\n-  this.cursorId = new db.bson_serializer.BSON.toLong(BinaryParser.toInt(binary_reply.substr(index, 4)), BinaryParser.toInt(binary_reply.substr(index + 4, 4)));\n-  index = index + 8;\n+  var low_bits = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n+  var high_bits = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n+  this.cursorId = new Long(low_bits, high_bits);\n   // Unpack the starting from\n-  this.startingFrom = BinaryParser.toInt(binary_reply.substr(index, 4));\n-  index = index + 4;\n+  this.startingFrom = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;\n   // Unpack the number of objects returned\n-  this.numberReturned = BinaryParser.toInt(binary_reply.substr(index, 4));\n-  index = index + 4;\n-  // Let's unpack all the bson document, deserialize them and store them\n-  for(var object_index = 0; object_index < this.numberReturned; object_index++) {\n-    // Read the size of the bson object\n-    var bsonObjectSize = BinaryParser.toInt(binary_reply.substr(index, 4));\n-    \n-    // sys.debug(\"--------------------------------------------------- incoming\")\n-    // BinaryParser.hprint(binary_reply.substr(index, bsonObjectSize))\n+  this.numberReturned = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n+  this.index = this.index + 4;  \n+}\n+\n+MongoReply.prototype.parseBody = function(binary_reply, bson, raw, callback) {\n+  raw = raw == null ? false : raw;\n+  // Just set a doc limit for deserializing\n+  var docLimitSize = 1024*20;  \n+  \n+  // If our message length is very long, let's switch to process.nextTick for messages\n+  if(this.messageLength > docLimitSize) {\n+    var batchSize = this.numberReturned;\n+    this.documents = new Array(this.numberReturned);\n     \n-    // Read the entire object and deserialize it\n-    this.documents.push(db.bson_deserializer.BSON.deserialize(binary_reply.substr(index, bsonObjectSize)));\n-    // Adjust for next object\n-    index = index + bsonObjectSize;\n-  }\n-};\n+    // Just walk down until we get a positive number >= 1\n+    for(var i = 50; i > 0; i--) {\n+      if((this.numberReturned/i) >= 1) {\n+        batchSize = i;\n+        break;\n+      }      \n+    }  \n+\n+    // Actual main creator of the processFunction setting internal state to control the flow\n+    var parseFunction = function(_self, _binary_reply, _batchSize, _numberReturned) {\n+      var object_index = 0;      \n+      // Internal loop process that will use nextTick to ensure we yield some time\n+      var processFunction = function() {\n+        // Adjust batchSize if we have less results left than batchsize\n+        if((_numberReturned - object_index) < _batchSize) {\n+          _batchSize = _numberReturned - object_index;\n+        }\n",
					"match": false,
					"packageHash": "e4684a42743a167509501d4803fba2695658f4e33fee6108442f1b60fe987877",
					"size": 2344,
					"sourceHash": "2a7cc4f286e5792d1ac34e4e92fc1586b483f6f974fb93afbf63d6d80d64e842",
					"status": "content"
				},
				"package.json": {
					"diff": "--- published/package.json\n+++ rebuilt/package.json\n@@ -1,26 +1,80 @@\n { \"name\" :            \"mongodb\"\n , \"description\" :     \"A node.js driver for MongoDB\"\n-, \"version\" :         \"0.9.4\"\n+, \"keywords\" :        [\"mongodb\", \"mongo\", \"driver\", \"db\"]\n+, \"version\" :         \"0.9.9-8\"\n , \"author\" :          \"Christian Amor Kvalheim <christkv@gmail.com>\"\n-, \"contributors\" :  [ \"Nathan White <nw@nwhite.net>\",\n-                      \"Adam Wiggins <adam@heroku.com>\",\n-                      \"Ian Millington <idmillington@googlemail.com>\",\n-                      \"Aaron Heckmann <aaron.heckmann+github@gmail.com>\",\n-                      \"Ciaran Jessup <ciaranj@gmail.com>\",\n+, \"contributors\" :  [ \"Aaron Heckmann\",\n                       \"Christoph Pojer\",\n+                      \"Pau Ramon Revilla\",\n+                      \"Nathan White\",\n+                      \"Emmerman\",\n+                      \"Seth LaForge\",\n+                      \"Boris Filipov\",\n+                      \"Stefan Schrmeli\",\n+                      \"Tedde Lundgren\",\n+                      \"renctan\",\n+                      \"Sergey Ukustov\",\n+                      \"Ciaran Jessup\",\n+                      \"kuno\",\n+                      \"srimonti\",\n                       \"Erik Abele\",\n-                      \"Henrik Johansson <dahankzter@gmail.com>\" ]\n+                      \"Pratik Daga\",\n+                      \"Slobodan Utvic\",\n+                      \"Kristina Chodorow\",\n+                      \"Yonathan Randolph\",\n+                      \"Brian Noguchi\",\n+                      \"Sam Epstein\",\n+                      \"James Harrison Fisher\",\n+                      \"Vladimir Dronnikov\",\n+                      \"Ben Hockey\",\n+                      \"Henrik Johansson\",\n+                      \"Simon Weare\",\n+                      \"Alex Gorbatchev\",\n+                      \"Shimon Doodkin\",\n+                      \"Kyle Mueller\",\n+                      \"Eran Hammer-Lahav\",\n+                      \"Marcin Ciszak\",\n+                      \"Franois de Metz\",\n+                      \"Vinay Pulim\",\n+                      \"nstielau\",\n+                      \"Adam Wiggins\",\n+                      \"entrinzikyl\",\n+                      \"Jeremy Selier\",\n+                      \"Ian Millington\",\n+                      \"Public Keating\",\n+                      \"andrewjstone\",\n+                      \"Christopher Stott\",\n+                      \"Corey Jewett\",\n+                      \"brettkiefer\",\n+                      \"Rob Holland\",\n+                      \"Senmiao Liu\",\n+                      \"heroic\",\n+                      \"gitfy\",\n+                      \"Andrew Stone\",\n+\t\t\t\t\t\t\t\t\t\t\t\"John Le Drew\"]\n+\n , \"repository\" :    { \"type\" :  \"git\"\n                     , \"url\" :   \"http://github.com/christkv/node-mongodb-native.git\" }\n , \"bugs\" :          { \"mail\" :  \"node-mongodb-native@googlegroups.com\"\n-                    , \"web\" :   \"http://github.com/christkv/node-mongodb-native/issues\" }\n-, \"os\" :            [ \"linux\"\n-                    , \"darwin\"\n-                    , \"freebsd\" ]\n+                    , \"url\" :   \"http://github.com/christkv/node-mongodb-native/issues\" }\n+, \"dependencies\" : {\n+  \"bson\": \"0.0.4\"\n+}                    \n+, \"devDependencies\": {\n+      \"dox\": \"0.2.0\"\n+    , \"uglify-js\": \"1.2.5\"\n+    , \"ejs\": \"0.6.1\"\n+    , \"nodeunit\": \"0.7.3\"\n+    , \"github3\": \">=0.3.0\"\n+\t  , \"markdown\": \"0.3.1\"\n+\t  , \"gleak\": \"0.2.3\"\n+\t  , \"step\": \"0.0.5\"\n+  }\n+, \"config\":         { \"native\" : false }                    \n , \"main\":             \"./lib/mongodb/index\"\n , \"directories\" :   { \"lib\" : \"./lib/mongodb\" }\n , \"engines\" :       { \"node\" : \">=0.4.0\" }\n-, \"scripts\": { \"install\" : \"bash ./install.sh\" }\n+, \"scripts\": { \"test\" : \"make test_pure\" }\n , \"licenses\" :    [ { \"type\" :  \"Apache License, Version 2.0\"\n                     , \"url\" :   \"http://www.apache.org/licenses/LICENSE-2.0\" } ]\n }\n",
					"match": false,
					"packageHash": "09c6c0562274d1837b7eb56615724646b664a148e2ec83febbe8b1f301ba7c5b",
					"size": 1350,
					"sourceHash": "3e819dd4d5c4f71977047355855ffb75eb246f9cfed7b07d12c2abb32fdb8bc5",
					"status": "content"
				},
				"server.js": {
					"match": false,
					"packageHash": "8902e0fbd87ec48ff53615424eb684d9c9cb687b0e26e98b1c5b3938888816ba",
					"size": 3226,
					"status": "missing-in-source"
				},
				"spec/lib/images/bg.png": {
					"match": false,
					"packageHash": "3d7d558f7ba5d5970065e85c9579c058742dcc35e94c1feb2d3b4e1c7756cc06",
					"size": 154,
					"status": "missing-in-source"
				},
				"spec/lib/images/hr.png": {
					"match": false,
					"packageHash": "b407c7d51e5000cffbe97529ed9bdc722c626094167a34229ea0b91dd671ae53",
					"size": 321,
					"status": "missing-in-source"
				},
				"spec/lib/images/loading.gif": {
					"match": false,
					"packageHash": "eb7cfd3d959b2e09c170f532e29f8b825f9bc770b2279fde58e595617753e244",
					"size": 2608,
					"status": "missing-in-source"
				},
				"spec/lib/images/sprites.bg.png": {
					"match": false,
					"packageHash": "3f404a1b483a5aeec9b99b657856b4151d40f50b94d30ea4405a2c6a7b592d25",
					"size": 4876,
					"status": "missing-in-source"
				},
				"spec/lib/images/sprites.png": {
					"match": false,
					"packageHash": "b65311848fa4a3c232360681b62b440477b065363b59ef9ea4357390c27c7725",
					"size": 3629,
					"status": "missing-in-source"
				},
				"spec/lib/images/vr.png": {
					"match": false,
					"packageHash": "11fdc4b649d93d8c077155e61001ccbd7ddc0919dcaaec929083c5de6919bc3d",
					"size": 145,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.css": {
					"match": false,
					"packageHash": "96c074a4e58e79b1dafbf0f2c852835e173f4cd0b5e8d6c7b6c205293c6a0995",
					"size": 3019,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.growl.js": {
					"match": false,
					"packageHash": "5af3532d49cd2c73301e2e043f3bd030fc0de96046136883fb59204c4b23bfbb",
					"size": 3148,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.jquery.js": {
					"match": false,
					"packageHash": "5f558f31121aa0ccc7cfbba83c4fd4fde806834f7555fbb3c0203a0c56fededb",
					"size": 2001,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.js": {
					"match": false,
					"packageHash": "8b102afbd4fdcf144515c4da499e842cec45521952ec316ce7144b995097335f",
					"size": 51846,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.shell.js": {
					"match": false,
					"packageHash": "565558c0b4c5130c873450c3b2d86e2925e54cbb3cc01121d14ac5335e311441",
					"size": 820,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.timers.js": {
					"match": false,
					"packageHash": "f0e2bfaee430fcbeb16341e10e1af40328690b55551bfb7e2dbeb375a271e145",
					"size": 1819,
					"status": "missing-in-source"
				},
				"spec/lib/jspec.xhr.js": {
					"match": false,
					"packageHash": "d665e0fad1faea2bd61a409aa995b42ff59d66d4bbf4a85462a29b4f5e0c49cb",
					"size": 4516,
					"status": "missing-in-source"
				},
				"spec/spec.bson.js": {
					"match": false,
					"packageHash": "963515f82996951bda6a8131ea41a9d6d651591b713f483c5aa4436746ce1d19",
					"size": 11341,
					"status": "missing-in-source"
				},
				"spec/spec.commands.js": {
					"match": false,
					"packageHash": "17cb7f9cbf6f8f95ec193e94249db0b51b7c4c7fe588d5d176cf53d982f14025",
					"size": 4256,
					"status": "missing-in-source"
				},
				"spec/spec.node.js": {
					"match": false,
					"packageHash": "d2ef3b0bd2b83145b99fc581631690adf34d5b275c8dbd4a4d4c3e8729d78ad3",
					"size": 585,
					"status": "missing-in-source"
				},
				"spec/spec.objectid.js": {
					"match": false,
					"packageHash": "55aa08f3f612de92c4b11fa1d7603a947d8acf8af70bd0e39a3085846234356b",
					"size": 316,
					"status": "missing-in-source"
				},
				".travis.yml": {
					"match": false,
					"status": "missing-in-package"
				},
				"external-libs/bson/index.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"install.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/connection.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/connection_pool.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/connection_utils.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/repl_set.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/server.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/strategies/ping_strategy.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/strategies/statistics_strategy.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/cursorstream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/gridfs/grid.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/gridfs/readstream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/utils.js": {
					"match": false,
					"status": "missing-in-package"
				}
			},
			"summary": {
				"differentFiles": 23,
				"matchingFiles": 1,
				"missingInPackage": 14,
				"missingInSource": 103,
				"score": 0.0070921985815602835,
				"totalFiles": 141
			}
		}
	}
]
