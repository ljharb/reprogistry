[
	{
		"reproduceVersion": "0.0.0-local",
		"timestamp": "2025-12-31T08:01:19.486Z",
		"os": "linux",
		"arch": "x64",
		"strategy": "npm:11.7.0",
		"reproduced": false,
		"attested": false,
		"package": {
			"spec": "mongodb@0.9.7-2-2",
			"name": "mongodb",
			"version": "0.9.7-2-2",
			"location": "https://registry.npmjs.org/mongodb/-/mongodb-0.9.7-2-2.tgz",
			"integrity": "sha512-DDe0qhlssbrrjINusQJr5/M4vr+/us1DOGmcQqV1eC5m/nuo0C7lXfvhw9Mp8q/zZzffzPBjlZXU4LHxWE9Nxw==",
			"publishedAt": "2011-12-16T18:41:34.956Z",
			"publishedWith": {
				"node": "v0.6.5",
				"npm": "1.1.0-alpha-6"
			}
		},
		"source": {
			"integrity": null,
			"location": "git://github.com/christkv/node-mongodb-native.git",
			"spec": "github:christkv/node-mongodb-native#HEAD"
		},
		"comparisonHash": "8a2a8dcbb729bf3ee9216e490c955701f0ddebe1",
		"diff": {
			"files": {
				".npmignore": {
					"match": false,
					"packageHash": "2ac216020eec6dc966e708bde381cb1f645096ee302a5a385b05ffe87f477fec",
					"size": 22,
					"status": "missing-in-source"
				},
				"HISTORY": {
					"match": false,
					"packageHash": "62e2767a179241248a1c399f394afaece56ad139c60fe61de2fff3bbfb88a60b",
					"size": 12921,
					"status": "missing-in-source"
				},
				"Makefile": {
					"diff": "--- published/Makefile\n+++ rebuilt/Makefile\n@@ -1,12 +1,13 @@\n-\n NODE = node\n-NODEUNIT = deps/nodeunit/bin/nodeunit\n+NPM = npm\n+NODEUNIT = node_modules/nodeunit/bin/nodeunit\n+DOX = node_modules/dox/bin/dox\n name = all\n \n total: build_native\n \n build_native:\n-\t$(MAKE) -C ./external-libs/bson all\n+\t# $(MAKE) -C ./external-libs/bson all\n \n build_native_debug:\n \t$(MAKE) -C ./external-libs/bson all_debug\n@@ -22,16 +23,23 @@\n \n test: build_native\n \t@echo \"\\n == Run All tests minus replicaset tests==\"\n-\t$(NODE) tools/test_all.js --noreplicaset\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot\n+\n+test_pure: build_native\n+\t@echo \"\\n == Run All tests minus replicaset tests==\"\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot --noactive\n \n test_junit: build_native\n \t@echo \"\\n == Run All tests minus replicaset tests==\"\n-\t$(NODE) tools/test_all.js --junit --noreplicaset\n+\t$(NODE) dev/tools/test_all.js --junit --noreplicaset\n \n test_nodeunit_pure:\n \t@echo \"\\n == Execute Test Suite using Pure JS BSON Parser == \"\n \t@$(NODEUNIT) test/ test/gridstore test/bson\n \n+test_js:\n+\t@$(NODEUNIT) $(TESTS)\n+\n test_nodeunit_replicaset_pure:\n \t@echo \"\\n == Execute Test Suite using Pure JS BSON Parser == \"\n \t@$(NODEUNIT) test/replicaset\n@@ -46,14 +54,18 @@\n \n test_all: build_native\n \t@echo \"\\n == Run All tests ==\"\n-\t$(NODE) tools/test_all.js\n+\t$(NODE) dev/tools/test_all.js --boot\n \n test_all_junit: build_native\n \t@echo \"\\n == Run All tests ==\"\n-\t$(NODE) tools/test_all.js --junit\n+\t$(NODE) dev/tools/test_all.js --junit --boot\n \n clean:\n \trm ./external-libs/bson/bson.node\n \trm -r ./external-libs/bson/build\n \n-.PHONY: total\n\\ No newline at end of file\n+generate_docs:\n+\t$(NODE) dev/tools/build-docs.js\n+\tmake --directory=./docs/sphinx-docs --file=Makefile html\n+\n+.PHONY: total\n",
					"match": false,
					"packageHash": "d92a0fdda1f48a9d48f2f9515ad76a57c64d837a6c0755ed8ed5119bc91a6180",
					"size": 1451,
					"sourceHash": "9a40c06659b42802352fedb651c7e3ac1993e441b74345c4f4015c7835dbd375",
					"status": "content"
				},
				"Readme.md": {
					"diff": "--- published/Readme.md\n+++ rebuilt/Readme.md\n@@ -1,11 +1,18 @@\n+Main Documentation site\n+=======================\n+\n+[Documentation](http://christkv.github.com/node-mongodb-native/)\n+\n Install\n ========\n \n To install the most recent release from npm, run:\n \n     npm install mongodb\n+    \n+That may give you a warning telling you that bugs['web'] should be bugs['url'], it would be safe to ignore it (this has been fixed in the development version) \n \n-To install from the latest from the repository, run::\n+To install the latest from the repository, run::\n \n     npm install path/to/node-mongodb-native\n \n@@ -16,7 +23,7 @@\n Introduction\n ========\n \n-This is a node.js driver for MongoDB. It's a port (or close to a port) of the libary for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n+This is a node.js driver for MongoDB. It's a port (or close to a port) of the library for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n \n A simple example of inserting a document.\n \n@@ -31,7 +38,7 @@\n             // Locate all the entries using find\n             collection.find().toArray(function(err, results) {\n               test.assertEquals(1, results.length);\n-              test.assertTrue(results.a === 2);\n+              test.assertTrue(results[0].a === 2);\n \n               // Let's close the db\n               client.close();\n@@ -46,23 +53,32 @@\n Data types\n ========\n \n-To store and retrieve the non-JSON MongoDb primitives ([ObjectID](http://www.mongodb.org/display/DOCS/Object+IDs), Long, Binary, [Timestamp](http://www.mongodb.org/display/DOCS/Timestamp+data+type), [DBRef](http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef), Code), you have to use one of the types from the bson_serializer.\n+To store and retrieve the non-JSON MongoDb primitives ([ObjectID](http://www.mongodb.org/display/DOCS/Object+IDs), Long, Binary, [Timestamp](http://www.mongodb.org/display/DOCS/Timestamp+data+type), [DBRef](http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef), Code).\n \n-In particular, every document has a unique `_id` which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example:\n+In particular, every document has a unique `_id` which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example: \n \n+    // Get the objectID type\n+    var ObjectID = require('mongodb').ObjectID;\n+    \n     var idString = '4e4e1638c85e808431000003';\n-    collection.findOne({_id: new client.bson_serializer.ObjectID(idString)}, console.log)  // ok\n+    collection.findOne({_id: new ObjectID(idString)}, console.log)  // ok\n     collection.findOne({_id: idString}, console.log)  // wrong! callback gets undefined\n \n Here are the constructors the non-Javascript BSON primitive types:\n \n-    var client = new Db(...);\n-    new client.bson_serializer.Long(numberString)\n-    new client.bson_serializer.ObjectID(hexString)\n-    new client.bson_serializer.Timestamp()  // the actual unique number is generated on insert.\n-    new client.bson_serializer.DBRef(collectionName, id, dbName)\n-    new client.bson_serializer.Binary(buffer)  // takes a string or Buffer\n-    new client.bson_serializer.Code(code, [context])\n+    // Fetch the library\n+    var mongo = require('mongodb');\n+    // Create new instances of BSON types\n+    new mongo.Long(numberString)\n+    new mongo.ObjectID(hexString)\n+    new mongo.Timestamp()  // the actual unique number is generated on insert.\n+    new mongo.DBRef(collectionName, id, dbName)\n+    new mongo.Binary(buffer)  // takes a string or Buffer\n+    new mongo.Code(code, [context])\n+    new mongo.Symbol(string)\n+    new mongo.MinKey()\n+    new mongo.MaxKey()\n+    new mongo.Double(number)\t// Force double storage\n \n The C/C++ bson parser/serializer\n --------\n@@ -76,7 +92,7 @@\n                         new Server(\"127.0.0.1\", 27017),\n                         {native_parser:true});\n \n-Since objects created using the C/C++ bson parser are incompatible with a client configured to use the Javascript bson parser and vice versa, you should call constructors using `client.bson_serializer` as described above (don't use `mongodb.BSONNative` and `mongodb.BSONPure` directly).\n+The C++ parser uses the js objects both for serialization and deserialization.\n \n GitHub information\n ========\n@@ -357,7 +373,7 @@\n ========\n Just as Felix Geisend√∂rfer I'm also working on the driver for my own startup and this driver is a big project that also benefits other companies who are using MongoDB.\n \n-If your company could benefit from a even better-engineered node.js mongodb driver I would appreciate any type of sponsorship you may be able to provide. All the sponsors will get a lifetime display in this readme, priority support and help on problems and votes on the roadmap decisions for the driver. If you are interested contact me on [christkv@gmail.com](mailto:christkv@gmail.com) for details.\n+If your company could benefit from a even better-engineered node.js mongodb driver I would appreciate any type of sponsorship you may be able to provide. All the sponsors will get a lifetime display in this readme, priority support and help on problems and votes on the roadmap decisions for the driver. If you are interested contact me on [christkv AT g m a i l.com](mailto:christkv@gmail.com) for details.\n \n And I'm very thankful for code contributions. If you are interested in working on features please contact me so we can discuss API design and testing.\n",
					"match": false,
					"packageHash": "93d3bf6a88d41df5bfaf8a4930f83f7514df603f340eeb3d5e457628b3f99f70",
					"size": 16337,
					"sourceHash": "eec96097c0795dae69c2266acd746c7b1ee7a3bec8afe76b1dc19d24418f5075",
					"status": "content"
				},
				"TODO": {
					"match": false,
					"packageHash": "0fe43539fbfff2d14e277d77855a8e6b763dd4958e790804e46b2783067523d4",
					"size": 557,
					"status": "missing-in-source"
				},
				"articles/NodeKOArticle1.md": {
					"match": false,
					"packageHash": "9f5df19d286a5ef6d1b89ab8b846c92d488b58d3d4c4a7872a6e7ce6d7c18ef1",
					"size": 16518,
					"status": "missing-in-source"
				},
				"articles/NodeKOArticle2.md": {
					"match": false,
					"packageHash": "2ce49396151d5e3bdcbcd1c752f4b884c45a167b8d8f5659a610bba27db91556",
					"size": 11106,
					"status": "missing-in-source"
				},
				"deps/gleak/.gitignore": {
					"match": false,
					"packageHash": "40bdd3939495d4595182f964a238fb8808cffe3d7b9f51c98fe10929f4009c42",
					"size": 32,
					"status": "missing-in-source"
				},
				"deps/gleak/History.md": {
					"match": false,
					"packageHash": "08987e79cc38f2e3cb3d8d52f93d9aaa943c9d4e6a84a46d41c1fc21a32dedb1",
					"size": 678,
					"status": "missing-in-source"
				},
				"deps/gleak/Makefile": {
					"match": false,
					"packageHash": "cc7dd0edba33a120a5958a144be8db4fbca198d95dfca0d65067e31f62e5f1e2",
					"size": 92,
					"status": "missing-in-source"
				},
				"deps/gleak/README.md": {
					"match": false,
					"packageHash": "8164a2545addd67dc9aa7dcfe8bca5aae0e7a349385641b315c4a4fb56dd5ff0",
					"size": 3700,
					"status": "missing-in-source"
				},
				"deps/gleak/index.js": {
					"match": false,
					"packageHash": "89fa6d2a3902f52c2a46347ddf5c18b6b3113921baf6ffbc6630dd68aa082fd7",
					"size": 3204,
					"status": "missing-in-source"
				},
				"deps/gleak/package.json": {
					"match": false,
					"packageHash": "11667342c7fbbf3a91d19b86704fffa3f29d1777c5ea7fe704e05b65e6a863c4",
					"size": 471,
					"status": "missing-in-source"
				},
				"deps/gleak/test/index.js": {
					"match": false,
					"packageHash": "654e16e8db2684b1761ec263da4b2b1e7aaa5bc15751bd05a56eccc05ca9c9cd",
					"size": 5310,
					"status": "missing-in-source"
				},
				"deps/nodeunit/.npmignore": {
					"match": false,
					"packageHash": "c22365d4b129983208d56dfa095d9af9e674c734939c8a7c62489c082db31480",
					"size": 36,
					"status": "missing-in-source"
				},
				"deps/nodeunit/CONTRIBUTORS.md": {
					"match": false,
					"packageHash": "dc1a5a144eaa2fdbb9b153779d58af125a5b65f3576d29dba2b5b8e8f3d8f079",
					"size": 1548,
					"status": "missing-in-source"
				},
				"deps/nodeunit/LICENSE": {
					"match": false,
					"packageHash": "b04b9e208e566fa898c7429e4dd5b45ba3ba2f7391e5c009cf63c53d580fa9b4",
					"size": 1058,
					"status": "missing-in-source"
				},
				"deps/nodeunit/Makefile": {
					"match": false,
					"packageHash": "5dbc2ad6b4398c2ecc6f5139795025770f7d29a1875ab3f3998753c42e4f3e81",
					"size": 5154,
					"status": "missing-in-source"
				},
				"deps/nodeunit/README.md": {
					"match": false,
					"packageHash": "751b4e9acf46b4e53769a8504f275b25e671ffd0a412b6d1c90d65955ddfee55",
					"size": 14941,
					"status": "missing-in-source"
				},
				"deps/nodeunit/bin/nodeunit": {
					"match": false,
					"packageHash": "57a538e5bc501cff0a6b6ced6702cf7f30b47c562dfcb6fc57a9c20f460065f9",
					"size": 3626,
					"status": "missing-in-source"
				},
				"deps/nodeunit/bin/nodeunit.json": {
					"match": false,
					"packageHash": "1751ba7b7ad826d3162da359fccef5d1352d1d9a8b710e6753139aa142b1d255",
					"size": 274,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/async.js": {
					"match": false,
					"packageHash": "6e67d1380c0ea1c5e876cdeb9b9f39ea8dba07d426d1377f3a1be62d57e0125c",
					"size": 18641,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/ejs.js": {
					"match": false,
					"packageHash": "5f87b6d3977341abd9b2500c496fb2a8de23fb59e6a6301426021e504c789eeb",
					"size": 2597,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/json2.js": {
					"match": false,
					"packageHash": "e6774f41a11016c803c602fa7e03bf03afb8e67217e2b1827cceb2fe5e1ddb52",
					"size": 17415,
					"status": "missing-in-source"
				},
				"deps/nodeunit/doc/nodeunit.md": {
					"match": false,
					"packageHash": "c57518a39297ef6ba91ee72ceeaba7ad5d9807f1cdb02d6449a266904a8305cf",
					"size": 1637,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/nodeunit.js": {
					"match": false,
					"packageHash": "4e716bf47c9c35a79dbe0714f1f650d96d0b4def80a6659a1f5fadfdf4fa7c4a",
					"size": 54817,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/suite1.js": {
					"match": false,
					"packageHash": "a0cf930d2666256e73299191618d5a58f005cc1bbf7733a7fdeb2aa169bd11d3",
					"size": 319,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/suite2.js": {
					"match": false,
					"packageHash": "cca3850beb4e4ea16bffbd6769191e384acf9e88de80a44e49a6266f89bc50ce",
					"size": 396,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/test.html": {
					"match": false,
					"packageHash": "921d98372885e8bf4d2918e6ebc1ecf236a3d981eddde4bb820ef46194db9723",
					"size": 311,
					"status": "missing-in-source"
				},
				"deps/nodeunit/img/example_fail.png": {
					"match": false,
					"packageHash": "5205647e4ddd31179fe5e1e4ddba8c496a76c2e821b2629b74772b62287381ab",
					"size": 38642,
					"status": "missing-in-source"
				},
				"deps/nodeunit/img/example_pass.png": {
					"match": false,
					"packageHash": "cd2164286b1c9fcb94cc71856dcdd258d285df39579e1445a1c16dd5d2d3f1c4",
					"size": 14133,
					"status": "missing-in-source"
				},
				"deps/nodeunit/index.js": {
					"match": false,
					"packageHash": "4a2146a5e00131523679c7d60430d41c67ae0a4ddacd52e51b2f0ce0ae517a5c",
					"size": 166,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/assert.js": {
					"match": false,
					"packageHash": "6d4a71e840792d8bb9f55bed413b454c20286936a06216e1e7372b1e6c175656",
					"size": 10207,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/core.js": {
					"match": false,
					"packageHash": "3122e6f06a3a1402e35fd451e137fea641bd8b99769de57494a79ccc3d977bd1",
					"size": 5928,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/nodeunit.js": {
					"match": false,
					"packageHash": "f7cf8583a6fee24b8b7fd2def10e9b54dc6b2f31dfe4a0085212de523400a05b",
					"size": 1935,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/browser.js": {
					"match": false,
					"packageHash": "2f128c6ebaf7346c9c0f18f934bdc7e001ccc07972ee5733dc194e20202ed78c",
					"size": 3891,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/default.js": {
					"match": false,
					"packageHash": "1f31dddcbf2d3ded9e749d8e83d7c671623fcc97a89d4bdd98fc9e8d66fb6058",
					"size": 4195,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/html.js": {
					"match": false,
					"packageHash": "e358a3640d6292f26b513cca60974d2af8990ea9e9367987ff40378db6d6fdc0",
					"size": 3512,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/index.js": {
					"match": false,
					"packageHash": "156ab9a0ba9aa7cc595e31c0604c6c97a8d8fddcfe984264d61c93804f9c37aa",
					"size": 331,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/junit.js": {
					"match": false,
					"packageHash": "9aba5ae9fb5d849ded37bbcfa265e8ea32e58e86a90db76ca8260cd3dd703dae",
					"size": 5908,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/minimal.js": {
					"match": false,
					"packageHash": "542cfb84da89893ad1d5cd521179e59d5dbbaefeeab30226f438601759c2d77e",
					"size": 3434,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/skip_passed.js": {
					"match": false,
					"packageHash": "1ffdf5f47d712048de1b56a701c2a57d1525d98f74ab7161e18da5afa3da37b9",
					"size": 3296,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/track.js": {
					"match": false,
					"packageHash": "e02fba7af50d190aaa45678abf04213e3c90b23ffd1633b367233d8abd2cea84",
					"size": 1222,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/types.js": {
					"match": false,
					"packageHash": "3adae1cf4af93b67cbe70604f63822fe0b12e41cacde8765e19f8c22d2f2ca94",
					"size": 5084,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/utils.js": {
					"match": false,
					"packageHash": "bc4418a67c3057e5990c8bb2920b60abe45b41a066f19cb2771668b8fdd9f640",
					"size": 6056,
					"status": "missing-in-source"
				},
				"deps/nodeunit/man1/nodeunit.1": {
					"match": false,
					"packageHash": "cd9bac2075ba9fda7b0c26d96f64a1af629eb438a897c40d23f303a43224cf8c",
					"size": 1957,
					"status": "missing-in-source"
				},
				"deps/nodeunit/nodelint.cfg": {
					"match": false,
					"packageHash": "2d97676148773c16d39eb5997004dc88e2746e648dc1023d3080cdbabca5a328",
					"size": 52,
					"status": "missing-in-source"
				},
				"deps/nodeunit/package.json": {
					"match": false,
					"packageHash": "a44a63a8dc5171bfcbebc3da4faff0a78b7cfce0d21064e127638db9e326d8d7",
					"size": 1418,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/junit.xml.ejs": {
					"match": false,
					"packageHash": "84500bc9c9138c39faa25d81744c17dc9f2f7c081027a025e21fc988d9bef71f",
					"size": 725,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/license.js": {
					"match": false,
					"packageHash": "c2d81ff8586c4f0e55b222efbdad15072f0960409d49144c39f55d99f96e19c4",
					"size": 235,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/nodeunit.css": {
					"match": false,
					"packageHash": "e1cff7d7b6903ac5358471ad8a04e400402a20dcf7efd4a9b7a71ca19ebba2fd",
					"size": 1358,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/coffee/mock_coffee_module.coffee": {
					"match": false,
					"packageHash": "a8ba7102b30806f8323e4658688507287363b6972f8a04bd344672b995becdd4",
					"size": 64,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/dir/mock_module3.js": {
					"match": false,
					"packageHash": "59f360a9d2bf7964fa63ce58e013da089e565c87202c35a3492cd9cc330418af",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/dir/mock_module4.js": {
					"match": false,
					"packageHash": "8d96c8ee9c1881d9fced4856817ec92a74c5e7f891057d9bae43a97439cede16",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/mock_module1.js": {
					"match": false,
					"packageHash": "d0e39b00e441f76ba906a4df1311edcf7ac2063845c34d949b185c073babdb53",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/mock_module2.js": {
					"match": false,
					"packageHash": "f79ae9762607ab929a7cef72f47f8f85fd176e0b03479a95a0b64093c5aff5db",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode1.js": {
					"match": false,
					"packageHash": "59e98924953900c233df9fd658fa5eb81587bd00ff0a461faf0b5c432ccc89f4",
					"size": 55,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode2.js": {
					"match": false,
					"packageHash": "58f4d19c89bf584aa58f59205156e28b0ec8da8f40e7208169c7140cf2189e79",
					"size": 57,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode3.js": {
					"match": false,
					"packageHash": "f2897e8d4ecbea9793fc07bf4e417c2675aa4e96e544ebdc5c80385dc06111c3",
					"size": 15,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-base.js": {
					"match": false,
					"packageHash": "37c7ea3406de4c5a3b965e72f2bb92226f402cafb0b7226eb53f424a2190762d",
					"size": 6250,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-failing-callbacks.js": {
					"match": false,
					"packageHash": "15bd6b031fa69025b97695793801d59448f048a5615ff8b3330f6b173715cc5e",
					"size": 3614,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-httputil.js": {
					"match": false,
					"packageHash": "cd3cc667209dd77721a210e1588fc33ccf754229eb27c3f5ba4c6e1fcbcb9b08",
					"size": 1751,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runfiles.js": {
					"match": false,
					"packageHash": "a2397734764aefb45243e4b07a9ccaf1fdf7bc2e526845e397eabb7763c085bf",
					"size": 6505,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runmodule.js": {
					"match": false,
					"packageHash": "c3a0373d1b92fa3fb886be2f79c38dac80632bde6f7053bcba38a4818c6edd86",
					"size": 4107,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runtest.js": {
					"match": false,
					"packageHash": "7377ea3e92c86979a5b1ee08c1cfa1cbae1b60a9cfbfca55904875e66570d631",
					"size": 1570,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-sandbox.js": {
					"match": false,
					"packageHash": "1d20ecc18dff9c49b1f2554b7705bd15edd6743f8f8d0ab8bd0848d321df1fad",
					"size": 1015,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-testcase.js": {
					"match": false,
					"packageHash": "3776d1b1636a89c29233276a402b05df3026c70c9c3aefbc64b24f472039bafe",
					"size": 6404,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test.html": {
					"match": false,
					"packageHash": "3e8cdfb33c5a73ceeeb9b4ed08829b89924711982be48eed40ed2818166e0007",
					"size": 808,
					"status": "missing-in-source"
				},
				"deps/step/README.markdown": {
					"match": false,
					"packageHash": "b023a8cd9a4a6ac09d3f20c6092f89e72f8c76e528bf45f0d8ac7a15f788f03e",
					"size": 3042,
					"status": "missing-in-source"
				},
				"deps/step/lib/step.js": {
					"match": false,
					"packageHash": "3dfd25f5bc117cc3acda7859ab4ddbd02251fdae55db565050b24d3260f7e198",
					"size": 4542,
					"status": "missing-in-source"
				},
				"deps/step/package.json": {
					"match": false,
					"packageHash": "d9400fc4eb190667a8d9f466e2986590b1badd1f2b22b6a35c28067daee4d8a7",
					"size": 376,
					"status": "missing-in-source"
				},
				"deps/step/test/callbackTest.js": {
					"match": false,
					"packageHash": "bd04258a35b05d769e7b5973ee1e161cf2ebf2102e2be460267e076e6d8389cf",
					"size": 621,
					"status": "missing-in-source"
				},
				"deps/step/test/errorTest.js": {
					"match": false,
					"packageHash": "eb689f8efdfcdec28f2e349fb4e0cbe5fc178ca178f5ec07a13075a812ead2a5",
					"size": 532,
					"status": "missing-in-source"
				},
				"deps/step/test/fnTest.js": {
					"match": false,
					"packageHash": "02d50088fc420a1bc2010dff0ae93d76593a7820748339cf707dd9d5e9334772",
					"size": 429,
					"status": "missing-in-source"
				},
				"deps/step/test/groupTest.js": {
					"match": false,
					"packageHash": "6c624cb9a4e7f11b187bba73bd87c91f1105e69d0943ddfa8313f2b4734a4bbf",
					"size": 2527,
					"status": "missing-in-source"
				},
				"deps/step/test/helper.js": {
					"match": false,
					"packageHash": "0746c0b3441c3c18e8579dd4fbca2f54384340348fe36885e0a9e5590f2f5097",
					"size": 533,
					"status": "missing-in-source"
				},
				"deps/step/test/parallelTest.js": {
					"match": false,
					"packageHash": "96362563d20b138a981099bf69c488502a5196c7657f7a9b83ae18bcc93d031c",
					"size": 1375,
					"status": "missing-in-source"
				},
				"docs/README.md": {
					"match": false,
					"packageHash": "f1c30be7bf94446e980d97f9afbf16a938f751c51bfbae9a1461a8b8f19fc7ad",
					"size": 1035,
					"status": "missing-in-source"
				},
				"docs/collections.md": {
					"match": false,
					"packageHash": "cffac28ae42cf67bbc640a1d6f43595c12ad70a07460ad201555cc97e60886e7",
					"size": 4069,
					"status": "missing-in-source"
				},
				"docs/database.md": {
					"match": false,
					"packageHash": "c8354b6cebb76f8074b14621a42815ae840de9aba7aba321185d999d24897d60",
					"size": 5261,
					"status": "missing-in-source"
				},
				"docs/gridfs.md": {
					"match": false,
					"packageHash": "1225c0a35a5856adf1614e30fbccfcb075a69b6c9d6e0189b791b89e84959caf",
					"size": 4470,
					"status": "missing-in-source"
				},
				"docs/indexes.md": {
					"match": false,
					"packageHash": "c1ed053f25e5f25b352d117dba0e5858e6df819db67d78318181ef00d4530378",
					"size": 3122,
					"status": "missing-in-source"
				},
				"docs/insert.md": {
					"match": false,
					"packageHash": "1184ac2e35a9978dff68985d61992357330f34990ca6c43883ea91cf096d1d64",
					"size": 5371,
					"status": "missing-in-source"
				},
				"docs/queries.md": {
					"match": false,
					"packageHash": "09285f8528f76e1fcbcfa67c63259c29b587dddf5a7abac8b983eb8230832812",
					"size": 8293,
					"status": "missing-in-source"
				},
				"docs/replicaset.md": {
					"match": false,
					"packageHash": "8f09555c992333a42a24816302b8b52e3139995161b1c340e38448ee0e154753",
					"size": 1952,
					"status": "missing-in-source"
				},
				"examples/admin.js": {
					"match": false,
					"packageHash": "2d440b977416005dea48cda521cdaf2478d443068ef008756dc48719b61203db",
					"size": 2193,
					"status": "missing-in-source"
				},
				"examples/blog.js": {
					"match": false,
					"packageHash": "137c4147a59743c8272baf0243e3f6ba2201ce20e97b3008d9afab263f6b916c",
					"size": 5940,
					"status": "missing-in-source"
				},
				"examples/capped.js": {
					"match": false,
					"packageHash": "53a9f9f12bb62e19ad8d4b9e5d3f79934f0bf1f57f582d3c02ab9a00f9efe532",
					"size": 1271,
					"status": "missing-in-source"
				},
				"examples/cursor.js": {
					"match": false,
					"packageHash": "efbda1522289effdb078a217545b364d84ea6a86abed04e1758d7f00e5362345",
					"size": 2560,
					"status": "missing-in-source"
				},
				"examples/gridfs.js": {
					"match": false,
					"packageHash": "3a0146ba12de6dc8bf07de5fd2dbbe5523821f2e67b61cbbf6658db6735369e4",
					"size": 5894,
					"status": "missing-in-source"
				},
				"examples/index.js": {
					"match": false,
					"packageHash": "c3ad958d2e3f17e9893474165393edb322f01afaebfa158244fed62a456baf3a",
					"size": 2368,
					"status": "missing-in-source"
				},
				"examples/info.js": {
					"match": false,
					"packageHash": "42fe6b013315fe972811cb4b5cd54f5f17820914d39de7b139c5034612c83209",
					"size": 1657,
					"status": "missing-in-source"
				},
				"examples/oplog.js": {
					"match": false,
					"packageHash": "cea2a2cd1a25e83eeb88823b5579725ef6ccaa8629c229ab9847c0fc426c8adc",
					"size": 3270,
					"status": "missing-in-source"
				},
				"examples/queries.js": {
					"match": false,
					"packageHash": "c39e274f9548374d0cf47baafd622d443b97429bd171ec20f22a17f77cfd4d36",
					"size": 5289,
					"status": "missing-in-source"
				},
				"examples/replSetServersQueries.js": {
					"match": false,
					"packageHash": "e4107de45e3049b4c9ad4d1cceb3c38d6c0bafc824621ef866678852b3318718",
					"size": 5741,
					"status": "missing-in-source"
				},
				"examples/replSetServersSimple.js": {
					"match": false,
					"packageHash": "12bac6387da463cdaeac2a101ea34ac7286bbccf01309230ae8df319e9177e45",
					"size": 2252,
					"status": "missing-in-source"
				},
				"examples/simple.js": {
					"match": false,
					"packageHash": "3194e95088f4cb9a12fd97b6ad375ab1874a1f52c33faea7eb9987d0095238b1",
					"size": 1686,
					"status": "missing-in-source"
				},
				"examples/strict.js": {
					"match": false,
					"packageHash": "73070e7cace51498c1dd0aedfadb13ac82cb9bfc5a13439c4c17890f021ba817",
					"size": 1445,
					"status": "missing-in-source"
				},
				"examples/types.js": {
					"match": false,
					"packageHash": "d3f0ee5438e7d555c9a085858d587f555f9d8be40dc808f1d64eac6c057e20d8",
					"size": 1678,
					"status": "missing-in-source"
				},
				"examples/url.js": {
					"match": false,
					"packageHash": "027764e2135279cec233b184fb6c5e8d432616d450e33303482994ecda85b89b",
					"size": 394,
					"status": "missing-in-source"
				},
				"external-libs/bson/.gitignore": {
					"match": false,
					"packageHash": "9c1cf3205322777097ff8e8d431a18d2cae134686837ee4906f93779ddce2f47",
					"size": 30,
					"status": "missing-in-source"
				},
				"external-libs/bson/Makefile": {
					"diff": "--- published/external-libs/bson/Makefile\n+++ rebuilt/external-libs/bson/Makefile\n@@ -6,33 +6,38 @@\n \trm -rf build .lock-wscript bson.node\n \tnode-waf configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t@$(NODE) --expose-gc test_bson.js\n-\t@$(NODE) --expose-gc test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n all_debug:\n \trm -rf build .lock-wscript bson.node\n \tnode-waf --debug configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t@$(NODE) --expose-gc test_bson.js\n-\t@$(NODE) --expose-gc test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n test:\n-\t@$(NODE) --expose-gc test_bson.js\n-\t@$(NODE) --expose-gc test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n clang:\n \trm -rf build .lock-wscript bson.node\n \tCXX=clang node-waf configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t@$(NODE) --expose-gc test_bson.js\n-\t@$(NODE) --expose-gc test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n clang_debug:\n \trm -rf build .lock-wscript bson.node\n \tCXX=clang node-waf --debug configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t@$(NODE) --expose-gc test_bson.js\n-\t@$(NODE) --expose-gc test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n+\t# @$(NODE) --expose-gc test/test_stackless_bson.js\n \n clean:\n \trm -rf build .lock-wscript bson.node\n",
					"match": false,
					"packageHash": "ff463c435c09270ae1ad62bc52b82086be9a21c62f439e103368348b412e9769",
					"size": 976,
					"sourceHash": "739cf490555ff92e959059d59f3ab3116a6159879968a0b0b57c5ac838e62163",
					"status": "content"
				},
				"external-libs/bson/binary.cc": {
					"match": false,
					"packageHash": "b91f6c82ad9013b46f43403da6643b86a8bca88ae261cb599328647bc05bc7de",
					"size": 15945,
					"status": "missing-in-source"
				},
				"external-libs/bson/binary.h": {
					"match": false,
					"packageHash": "df8172f5d07a113f5770ee5ae7b353150308b0df30f9789eee24fa18b6d6a4a2",
					"size": 1988,
					"status": "missing-in-source"
				},
				"external-libs/bson/bson.cc": {
					"diff": "--- published/external-libs/bson/bson.cc\n+++ rebuilt/external-libs/bson/bson.cc\n@@ -1,7 +1,18 @@\n #include <assert.h>\n #include <string.h>\n #include <stdlib.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic push\n+#pragma clang diagnostic ignored \"-Wunused-parameter\"\n+#endif\n+\n #include <v8.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic pop\n+#endif\n+\n #include <node.h>\n #include <node_version.h>\n #include <node_buffer.h>\n@@ -10,21 +21,13 @@\n #include <cstdlib>\n #include <iostream>\n #include <limits>\n+#include <vector>\n \n #include \"bson.h\"\n-#include \"long.h\"\n-#include \"timestamp.h\"\n-#include \"objectid.h\"\n-#include \"binary.h\"\n-#include \"code.h\"\n-#include \"dbref.h\"\n-#include \"symbol.h\"\n-#include \"minkey.h\"\n-#include \"maxkey.h\"\n-#include \"double.h\"\n \n using namespace v8;\n using namespace node;\n+using namespace std;\n \n // BSON DATA TYPES\n const uint32_t BSON_DATA_NUMBER = 1;\n@@ -49,12 +52,11 @@\n const int32_t BSON_INT32_MAX = (int32_t)2147483647L;\n const int32_t BSON_INT32_MIN = (int32_t)(-1) * 2147483648L;\n \n-// BSON BINARY DATA SUBTYPES\n-const uint32_t BSON_BINARY_SUBTYPE_FUNCTION = 1;\n-const uint32_t BSON_BINARY_SUBTYPE_BYTE_ARRAY = 2;\n-const uint32_t BSON_BINARY_SUBTYPE_UUID = 3;\n-const uint32_t BSON_BINARY_SUBTYPE_MD5 = 4;\n-const uint32_t BSON_BINARY_SUBTYPE_USER_DEFINED = 128;\n+const int64_t BSON_INT64_MAX = ((int64_t)1 << 63) - 1;\n+const int64_t BSON_INT64_MIN = (int64_t)-1 << 63;\n+\n+const int64_t JS_INT_MAX = (int64_t)1 << 53;\n+const int64_t JS_INT_MIN = (int64_t)-1 << 53;\n \n static Handle<Value> VException(const char *msg) {\n     HandleScope scope;\n@@ -72,227 +74,122 @@\n   constructor_template->InstanceTemplate()->SetInternalFieldCount(1);\n   constructor_template->SetClassName(String::NewSymbol(\"BSON\"));\n   \n-  // Class methods\n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"serialize\", BSONSerialize);  \n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"serializeWithBufferAndIndex\", SerializeWithBufferAndIndex);\n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"deserialize\", BSONDeserialize);  \n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"encodeLong\", EncodeLong);  \n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"toLong\", ToLong);\n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"toInt\", ToInt);\n-  NODE_SET_METHOD(constructor_template->GetFunction(), \"calculateObjectSize\", CalculateObjectSize);\n+  // Instance methods\n+  NODE_SET_PROTOTYPE_METHOD(constructor_template, \"calculateObjectSize\", CalculateObjectSize);\n+  NODE_SET_PROTOTYPE_METHOD(constructor_template, \"serialize\", BSONSerialize);\n+  NODE_SET_PROTOTYPE_METHOD(constructor_template, \"serializeWithBufferAndIndex\", SerializeWithBufferAndIndex);\n+  NODE_SET_PROTOTYPE_METHOD(constructor_template, \"deserialize\", BSONDeserialize);\n+  NODE_SET_PROTOTYPE_METHOD(constructor_template, \"deserializeStream\", BSONDeserializeStream);\n+\n+  // Experimental\n+  // NODE_SET_PROTOTYPE_METHOD(constructor_template, \"calculateObjectSize2\", CalculateObjectSize2);\n+  // NODE_SET_PROTOTYPE_METHOD(constructor_template, \"serialize2\", BSONSerialize2);\n+  // NODE_SET_METHOD(constructor_template->GetFunction(), \"serialize2\", BSONSerialize2);  \n \n-  target->Set(String::NewSymbol(\"BSON\"), constructor_template->GetFunction());\n+  target->ForceSet(String::NewSymbol(\"BSON\"), constructor_template->GetFunction());\n }\n \n // Create a new instance of BSON and assing it the existing context\n Handle<Value> BSON::New(const Arguments &args) {\n   HandleScope scope;\n   \n-  BSON *bson = new BSON();\n-  bson->Wrap(args.This());\n-  return args.This();\n-}\n-\n",
					"match": false,
					"packageHash": "883cc4a95fa5fb66c2ccbf32aa1402395217a7ce962948a3061daff2cbfef6f3",
					"size": 80468,
					"sourceHash": "3bbf33024abd0bbd372cf8d602e8323759fdd165640ea5b8822dd862932a142e",
					"status": "content"
				},
				"external-libs/bson/bson.h": {
					"diff": "--- published/external-libs/bson/bson.h\n+++ rebuilt/external-libs/bson/bson.h\n@@ -14,48 +14,92 @@\n     ~BSON() {}\n     \n     static void Initialize(Handle<Object> target);\n+    static Handle<Value> BSONDeserializeStream(const Arguments &args);\n+\n+    // JS based objects\n     static Handle<Value> BSONSerialize(const Arguments &args);\n     static Handle<Value> BSONDeserialize(const Arguments &args);\n \n-    // Encode functions\n-    static Handle<Value> EncodeLong(const Arguments &args);\n-    static Handle<Value> ToLong(const Arguments &args);\n-    static Handle<Value> ToInt(const Arguments &args);\n-  \n     // Calculate size of function\n     static Handle<Value> CalculateObjectSize(const Arguments &args);\n     static Handle<Value> SerializeWithBufferAndIndex(const Arguments &args);\n-  \n+\n+  \t// Experimental\n+    static Handle<Value> CalculateObjectSize2(const Arguments &args);\n+    static Handle<Value> BSONSerialize2(const Arguments &args);\n+\n     // Constructor used for creating new BSON objects from C++\n     static Persistent<FunctionTemplate> constructor_template;\n \n   private:\n     static Handle<Value> New(const Arguments &args);\n-    static Handle<Value> deserialize(char *data, bool is_array_item);\n-    static uint32_t serialize(char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, bool check_key, bool serializeFunctions);\n+    static Handle<Value> deserialize(BSON *bson, char *data, uint32_t dataLength, uint32_t startIndex, bool is_array_item);\n+    static uint32_t serialize(BSON *bson, char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, bool check_key, bool serializeFunctions);\n \n     static char* extract_string(char *data, uint32_t offset);\n     static const char* ToCString(const v8::String::Utf8Value& value);\n-    static uint32_t calculate_object_size(Handle<Value> object, bool serializeFunctions);\n+    static uint32_t calculate_object_size(BSON *bson, Handle<Value> object, bool serializeFunctions);\n \n     static void write_int32(char *data, uint32_t value);\n     static void write_int64(char *data, int64_t value);\n     static void write_double(char *data, double value);\n-    static int deserialize_sint8(char *data, uint32_t offset);\n-    static int deserialize_sint16(char *data, uint32_t offset);\n-    static long deserialize_sint32(char *data, uint32_t offset);\n     static uint16_t deserialize_int8(char *data, uint32_t offset);\n     static uint32_t deserialize_int32(char* data, uint32_t offset);\n     static char *check_key(Local<String> key);\n-    static char *decode_utf8(char * string, uint32_t length);\n+     \n+    // BSON type instantiate functions\n+    Persistent<Function> longConstructor;\n+    Persistent<Function> objectIDConstructor;\n+    Persistent<Function> binaryConstructor;\n+    Persistent<Function> codeConstructor;\n+    Persistent<Function> dbrefConstructor;\n+    Persistent<Function> symbolConstructor;\n+    Persistent<Function> doubleConstructor;\n+    Persistent<Function> timestampConstructor;\n+    Persistent<Function> minKeyConstructor;\n+    Persistent<Function> maxKeyConstructor;\n+    \n+    // Equality Objects\n+    Persistent<String> longString;\n+    Persistent<String> objectIDString;\n+    Persistent<String> binaryString;\n+    Persistent<String> codeString;\n+    Persistent<String> dbrefString;\n+    Persistent<String> symbolString;\n+    Persistent<String> doubleString;\n+    Persistent<String> timestampString;\n+    Persistent<String> minKeyString;\n+    Persistent<String> maxKeyString;\n+    \n+    // Equality speed up comparision objects\n+    Persistent<String> _bsontypeString;\n+    Persistent<String> _longLowString;\n+    Persistent<String> _longHighString;\n+    Persistent<String> _objectIDidString;\n+    Persistent<String> _binaryPositionString;\n+    Persistent<String> _binarySubTypeString;\n+    Persistent<String> _binaryBufferString;\n+    Persistent<String> _doubleValueString;\n+    Persistent<String> _symbolValueString;\n+\n+    Persistent<String> _dbRefRefString;\n+    Persistent<String> _dbRefIdRefString;\n+    Persistent<String> _dbRefDbRefString;\n+    Persistent<String> _dbRefNamespaceString;\n+    Persistent<String> _dbRefDbString;\n+    Persistent<String> _dbRefOidString;\n         \n-    // Decode function\n-    static Handle<Value> decodeLong(char *data, uint32_t index);\n-    static Handle<Value> decodeTimestamp(int64_t value);\n-    static Handle<Value> decodeOid(char *oid);\n-    static Handle<Value> decodeBinary(uint32_t sub_type, uint32_t number_of_bytes, char *data);\n-    static Handle<Value> decodeCode(char *code, Handle<Value> scope);\n",
					"match": false,
					"packageHash": "347b4d1f42b114b575eebbe7ff10a3511cdd363f160216f64ef18537a9d89156",
					"size": 2522,
					"sourceHash": "3ac7c29a803a949feb401a361cff6d5b953f45756933d3d0815dc4833beed2ae",
					"status": "content"
				},
				"external-libs/bson/code.cc": {
					"match": false,
					"packageHash": "b7ff5ce5e12331677c826272c03804a9bdca5ab1ac634946dd8565c677ec5ef7",
					"size": 5788,
					"status": "missing-in-source"
				},
				"external-libs/bson/code.h": {
					"match": false,
					"packageHash": "56853f4dd68cd1dcc5e8612eb74e827b5b66d17ece160c1a5076a4c0f30a53a3",
					"size": 1402,
					"status": "missing-in-source"
				},
				"external-libs/bson/dbref.cc": {
					"match": false,
					"packageHash": "e9e5be24f2168e8f544100d572563206fd03c0c213b351d3173960afc542a0db",
					"size": 5768,
					"status": "missing-in-source"
				},
				"external-libs/bson/dbref.h": {
					"match": false,
					"packageHash": "9b78991199e9ce741f9b8ea94e900be912c2bd49686464f6d88abd3a57712384",
					"size": 1784,
					"status": "missing-in-source"
				},
				"external-libs/bson/double.cc": {
					"match": false,
					"packageHash": "7e4ea6784ec48822afdad9a56a5f5ebebdeac579647975a4b580346728b4f1c3",
					"size": 3116,
					"status": "missing-in-source"
				},
				"external-libs/bson/double.h": {
					"match": false,
					"packageHash": "17860ca38e1308604cbbdf7552579a5396ffd34d7454f1b978146b125eacc65c",
					"size": 1243,
					"status": "missing-in-source"
				},
				"external-libs/bson/index.js": {
					"diff": "--- published/external-libs/bson/index.js\n+++ rebuilt/external-libs/bson/index.js\n@@ -1,11 +1,15 @@\n var bson = require('./bson');\n exports.BSON = bson.BSON;\n-exports.Long = bson.Long;\n-exports.ObjectID = bson.ObjectID;\n-exports.DBRef = bson.DBRef;\n-exports.Code = bson.Code;\n-exports.Timestamp = bson.Timestamp;\n-exports.Binary = bson.Binary;\n+exports.Long = require('../../lib/mongodb/bson/long').Long;\n+exports.ObjectID = require('../../lib/mongodb/bson/objectid').ObjectID;\n+exports.DBRef = require('../../lib/mongodb/bson/db_ref').DBRef;\n+exports.Code = require('../../lib/mongodb/bson/code').Code;\n+exports.Timestamp = require('../../lib/mongodb/bson/timestamp').Timestamp;\n+exports.Binary = require('../../lib/mongodb/bson/binary').Binary;\n+exports.Double = require('../../lib/mongodb/bson/double').Double;\n+exports.MaxKey = require('../../lib/mongodb/bson/max_key').MaxKey;\n+exports.MinKey = require('../../lib/mongodb/bson/min_key').MinKey;\n+exports.Symbol = require('../../lib/mongodb/bson/symbol').Symbol;\n \n // Just add constants tot he Native BSON parser\n exports.BSON.BSON_BINARY_SUBTYPE_DEFAULT = 0;\n",
					"match": false,
					"packageHash": "def43fd9a41d09483510e72d1591e13cb267a032504ae9ac31909968699659f9",
					"size": 575,
					"sourceHash": "a2d8bb3379e3d4c0c8a825b410e0b924f98b62470fbdfbfb1a13b49e7fff5897",
					"status": "content"
				},
				"external-libs/bson/local.cc": {
					"match": false,
					"packageHash": "05c0c129e97ca07fc2e829ddb2cf416463d533cd4d3ab195c6d6c69afa31eab0",
					"size": 504,
					"status": "missing-in-source"
				},
				"external-libs/bson/local.h": {
					"match": false,
					"packageHash": "8be6e556982f4cd8cc62d9ae0357cd9f3db763fafab4fdc12e9507dbeb986350",
					"size": 276,
					"status": "missing-in-source"
				},
				"external-libs/bson/long.cc": {
					"match": false,
					"packageHash": "bda47f94ea41ce980b96d7144d9c169b7305559a90044bbb034b526591ba6552",
					"size": 20240,
					"status": "missing-in-source"
				},
				"external-libs/bson/long.h": {
					"match": false,
					"packageHash": "07198ae20b06de0a4d448a1842df18795d188f3128ed6383e14d6234b6ae3094",
					"size": 2707,
					"status": "missing-in-source"
				},
				"external-libs/bson/maxkey.cc": {
					"match": false,
					"packageHash": "9d3fcdef14f3f249d988f73d5e770ffce6727237218bc316bc31fe92bf918887",
					"size": 1829,
					"status": "missing-in-source"
				},
				"external-libs/bson/maxkey.h": {
					"match": false,
					"packageHash": "3d6ef6cf289a63657a62f26eaa5eaf3165d3917774e457df34fa8c99d4a49dcb",
					"size": 977,
					"status": "missing-in-source"
				},
				"external-libs/bson/minkey.cc": {
					"match": false,
					"packageHash": "6bf1955e0b4194f895fa79383f185d8e566ae8bbf34deaa4889f5eef30f7be43",
					"size": 1829,
					"status": "missing-in-source"
				},
				"external-libs/bson/minkey.h": {
					"match": false,
					"packageHash": "a960443bcaac4c1712b08eace7ac3c2208e1fcbccb0d6ea88f5b933d7d719dbb",
					"size": 973,
					"status": "missing-in-source"
				},
				"external-libs/bson/objectid.cc": {
					"match": false,
					"packageHash": "c0e6c99c8042a1a10651f8098d380aee7bab6e4719676c8618f4bf838ffd6c8e",
					"size": 12354,
					"status": "missing-in-source"
				},
				"external-libs/bson/objectid.h": {
					"match": false,
					"packageHash": "e8c8585d06ac62e8ebba4cb36d3802935dcf7d738c2b1d449c9026b4ba9ab8a0",
					"size": 2143,
					"status": "missing-in-source"
				},
				"external-libs/bson/symbol.cc": {
					"match": false,
					"packageHash": "bf408f8b90787f5909a3ef393497290b69d52199ef157d4fdfb08f8527f2a6f2",
					"size": 3602,
					"status": "missing-in-source"
				},
				"external-libs/bson/symbol.h": {
					"match": false,
					"packageHash": "0c295ce957cec331e96d689050167354335d2e62374cb7519d0b76cc28698fc5",
					"size": 1243,
					"status": "missing-in-source"
				},
				"external-libs/bson/test_bson.js": {
					"match": false,
					"packageHash": "06cc68c4d88eff6d79d5a0ecfc4a870e8e6c9b4bcf66c322b736d8c9b0073917",
					"size": 17805,
					"status": "missing-in-source"
				},
				"external-libs/bson/test_full_bson.js": {
					"match": false,
					"packageHash": "e846111e6b2990ab47ea7231ffdcac6654e272cf6cf2932d05a8a020e3f728e9",
					"size": 7399,
					"status": "missing-in-source"
				},
				"external-libs/bson/timestamp.cc": {
					"match": false,
					"packageHash": "7e3bf7c879341cb7b1b565d9b5da2024faff1ea8f8a13f1d5f3ed8af03ea14c4",
					"size": 19521,
					"status": "missing-in-source"
				},
				"external-libs/bson/timestamp.h": {
					"match": false,
					"packageHash": "06465df1a98afb3feedbc19d9439530dbd1e55c89e40f72f9c3a6b300753423e",
					"size": 2869,
					"status": "missing-in-source"
				},
				"external-libs/bson/wscript": {
					"diff": "--- published/external-libs/bson/wscript\n+++ rebuilt/external-libs/bson/wscript\n@@ -26,7 +26,7 @@\n def build(bld):\n   obj = bld.new_task_gen(\"cxx\", \"shlib\", \"node_addon\")\n   obj.target = \"bson\"\n-  obj.source = [\"bson.cc\", \"long.cc\", \"objectid.cc\", \"binary.cc\", \"code.cc\", \"dbref.cc\", \"timestamp.cc\", \"local.cc\", \"symbol.cc\", \"minkey.cc\", \"maxkey.cc\", \"double.cc\"]\n+  obj.source = [\"bson.cc\"]\n   # obj.uselib = \"NODE\"\n \n def shutdown():\n",
					"match": false,
					"packageHash": "ad0f2977fbb4539c525e2cf56ef5fbbd50a6b9ae8cb341d7bc24d341ff8b5df7",
					"size": 1320,
					"sourceHash": "561b73e306e26f9f160255232a6fd86da2141e466b1d5ebff8e0cbc6423972ea",
					"status": "content"
				},
				"install.js": {
					"diff": "--- published/install.js\n+++ rebuilt/install.js\n@@ -10,7 +10,6 @@\n // Check if we want to build the native code\n var build_native = process.env['npm_package_config_native'] != null ? process.env['npm_package_config_native'] : 'false';\n build_native = build_native == 'true' ? true : false;\n-\n // If we are building the native bson extension ensure we use gmake if available\n if(build_native) {\n   // Check if we need to use gmake\n",
					"match": false,
					"packageHash": "955347cbab86146af5f817e5d6c514338c51e8cb1bc5c48de2b7df001ce98910",
					"size": 1565,
					"sourceHash": "773466260a06c5ce4dfabbe8530bccd1def293efcda3c09ad1dea99915efb931",
					"status": "content"
				},
				"lib/mongodb/admin.js": {
					"diff": "--- published/lib/mongodb/admin.js\n+++ rebuilt/lib/mongodb/admin.js\n@@ -1,13 +1,43 @@\n+/*!\n+ * Module dependencies.\n+ */\n var Collection = require('./collection').Collection,\n     Cursor = require('./cursor').Cursor,\n-    DbCommand = require('./commands/db_command').DbCommand,\n-    debug = require('util').debug, \n-    inspect = require('util').inspect;\n+    DbCommand = require('./commands/db_command').DbCommand;\n \n-var Admin = exports.Admin = function(db) {  \n+/**\n+ * Allows the user to access the admin functionality of MongoDB\n+ *\n+ * @class Represents the Admin methods of MongoDB.\n+ * @param {Object} db Current db instance we wish to perform Admin operations on.\n+ * @return {Function} Constructor for Admin type.\n+ */\n+function Admin(db) {  \n+  if(!(this instanceof Admin)) return new Admin(db);\n+  \n   this.db = db;\n };\n \n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n+Admin.prototype.buildInfo = function(callback) {\n+  this.serverInfo(callback);\n+}\n+\n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api private\n+ */\n Admin.prototype.serverInfo = function(callback) {\n   var self = this;\n   var command = {buildinfo:1};\n@@ -17,14 +47,44 @@\n   });\n }\n \n+/**\n+ * Retrieve this db's server status.\n+ *\n+ * @param {Function} callback returns the server status.\n+ * @return {null}\n+ * @api public\n+ */\n+Admin.prototype.serverStatus = function(callback) {\n+  var self = this;\n+\n+  this.command({serverStatus: 1}, function(err, result) {\n+    if (err == null && result.documents[0].ok == 1) {\n+      callback(null, result.documents[0]);\n+    } else {\n+      if (err) {\n+        callback(err, false);\n+      } else {\n+        callback(self.wrap(result.documents[0]), false);\n+      }\n+    }\n+  });\n+};\n+\n+/**\n+ * Retrieve the current profiling Level for MongoDB\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n Admin.prototype.profilingLevel = function(callback) {\n   var self = this;\n   var command = {profile:-1};\n \n   this.command(command, function(err, doc) {\n     doc = doc.documents[0];\n-\n-    if(err == null && (doc.ok == 1 || doc.was.constructor == Numeric)) {\n+    \n+    if(err == null && (doc.ok == 1 || typeof doc.was === 'number')) {\n       var was = doc.was;\n       if(was == 0) {\n         callback(null, \"off\");\n@@ -41,6 +101,14 @@\n   });\n };\n",
					"match": false,
					"packageHash": "71db7c9061c78cf1febe59a7127f486c2050ba16fa9b2f13a8f48ec0b56ecf08",
					"size": 5273,
					"sourceHash": "a037f0b049bc29d04bfc3ee04e7483bbe7b8750c68b95e6c0b45b94b67897af1",
					"status": "content"
				},
				"lib/mongodb/bson/binary.js": {
					"match": false,
					"packageHash": "2a397c948008e0d4c04c55491a1a0221edd5bd791cfe5a96fb62260698787bd8",
					"size": 2874,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/binary_parser.js": {
					"match": false,
					"packageHash": "ef9e3cac450d49f4d2f5879017e6bff297710d096dbeb8a47d5082db39f38b38",
					"size": 12297,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/binary_utils.js": {
					"match": false,
					"packageHash": "f9837d9b2dd787c6e452579657fb60e59d0831cf8a0b5d9cf03ab20c92d242d5",
					"size": 812,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/bson.js": {
					"match": false,
					"packageHash": "77ce74986550396eb5fcb870b40d86fc2dc4fdbaa3bfd2f2a15ecaa337bb4c52",
					"size": 57903,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/double.js": {
					"match": false,
					"packageHash": "0a4787819522f30d2908f978ecc0c692677725e316d2406c9f16ed829a9eadce",
					"size": 203,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/float_parser.js": {
					"match": false,
					"packageHash": "5b6b4382617418d5310f2a0565db42b33b957eb2c5c478fc428fdd82ee1d882d",
					"size": 3844,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/objectid.js": {
					"match": false,
					"packageHash": "8cbcce9c6b5992d7be94c9804730c69469a7e8b2f598d88da13c48e8fe49475e",
					"size": 4584,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/timestamp.js": {
					"match": false,
					"packageHash": "21d62838cc4b3788bcc7af9e4c9de5f1b7cea794d9b712d673e5c253f0930504",
					"size": 24509,
					"status": "missing-in-source"
				},
				"lib/mongodb/collection.js": {
					"diff": "--- published/lib/mongodb/collection.js\n+++ rebuilt/lib/mongodb/collection.js\n@@ -1,130 +1,100 @@\n /**\n  * Module dependencies.\n+ * @ignore\n  */\n var InsertCommand = require('./commands/insert_command').InsertCommand\n   , QueryCommand = require('./commands/query_command').QueryCommand\n   , DeleteCommand = require('./commands/delete_command').DeleteCommand\n   , UpdateCommand = require('./commands/update_command').UpdateCommand\n   , DbCommand = require('./commands/db_command').DbCommand\n-  , BinaryParser = require('./bson/binary_parser').BinaryParser\n+  , ObjectID = require('bson').ObjectID\n+  , Code = require('bson').Code\n   , Cursor = require('./cursor').Cursor\n-  , debug = require('util').debug\n-  , inspect = require('util').inspect;\n+  , utils = require('./utils');\n \n /**\n  * Precompiled regexes\n+ * @ignore\n **/\n-const eErrorMessages = /E\\d{5}/;\n-\n-/**\n- * Sort functions, Normalize and prepare sort parameters\n- */\n-\n-function formatSortValue (sortDirection) {\n-  var value = (\"\" + sortDirection).toLowerCase();\n-\n-  switch (value) {\n-    case 'ascending':\n-    case 'asc':\n-    case '1':\n-      return 1;\n-    case 'descending':\n-    case 'desc':\n-    case '-1':\n-      return -1;\n-    default:\n-      throw new Error(\"Illegal sort clause, must be of the form \"\n-                    + \"[['field1', '(ascending|descending)'], \"\n-                    + \"['field2', '(ascending|descending)']]\");\n-  }\n-};\n-\n-function formattedOrderClause (sortValue) {\n-  var orderBy = {};\n-\n-  if (Array.isArray(sortValue)) {\n-    sortValue.forEach(function (sortElement) {\n-      if (sortElement.constructor == String) {\n-        orderBy[sortElement] = 1;\n-      } else {\n-        orderBy[sortElement[0]] = formatSortValue(sortElement[1]);\n-      }\n-    });\n-  } else if (sortValue.constructor == String) {\n-    orderBy[sortValue] = 1;\n-  } else {\n-    throw new Error(\"Illegal sort clause, must be of the form \" +\n-      \"[['field1', '(ascending|descending)'], ['field2', '(ascending|descending)']]\");\n-  }\n-\n-  return orderBy;\n-};\n+const eErrorMessages = /No matching object found/;\n \n /**\n  * toString helper.\n+ * @ignore\n  */\n-\n var toString = Object.prototype.toString;\n \n /**\n- * Collection constructor.\n+ * Create a new Collection instance\n  *\n- * @param {Database} db\n- * @param {String} collectionName\n- * @param {Function} pkFactory\n+ * Options\n+ *  - **slaveOk** {Boolean, default:false}, Allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions on the document.\n+ *  - **raw** {Boolean, default:false}, perform all operations using raw bson objects.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *\n+ * @class Represents a Collection\n+ * @param {Object} db db instance.\n+ * @param {String} collectionName collection name.\n+ * @param {Object} [pkFactory] alternative primary key factory.\n+ * @param {Object} [options] additional options for the collection.\n+ * @return {Object} a collection instance.\n  */\n-\n function Collection (db, collectionName, pkFactory, options) {\n",
					"match": false,
					"packageHash": "74d9596dfff41eefb2f8a684880f79851963bf5ecac8487bad30b73060e7a95b",
					"size": 36208,
					"sourceHash": "a3563efa6644f7f9f404b47e0386f1ca647574fd634b04b427ee5ef9c05c8b56",
					"status": "content"
				},
				"lib/mongodb/commands/base_command.js": {
					"diff": "--- published/lib/mongodb/commands/base_command.js\n+++ rebuilt/lib/mongodb/commands/base_command.js\n@@ -1,7 +1,3 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n-\n /**\n   Base object used for common functionality\n **/\n",
					"match": false,
					"packageHash": "4da764d9a6b2c9ef7ffb01aa2018388739e12237d0974f8f5b6a1632085caad1",
					"size": 774,
					"sourceHash": "3b9d5bf8d0033df72262cdd5c60feb91a4052597696bb1123bae0beeda3f9aa9",
					"status": "content"
				},
				"lib/mongodb/commands/db_command.js": {
					"diff": "--- published/lib/mongodb/commands/db_command.js\n+++ rebuilt/lib/mongodb/commands/db_command.js\n@@ -1,9 +1,7 @@\n var QueryCommand = require('./query_command').QueryCommand,\n   InsertCommand = require('./insert_command').InsertCommand,\n   inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  crypto = require('crypto'),\n-  inspect = require('util').inspect;\n+  crypto = require('crypto');\n \n /**\n   Db Command\n@@ -94,9 +92,11 @@\n };\n \n DbCommand.createGetLastErrorCommand = function(options, db) {\n-  var args = Array.prototype.slice.call(arguments, 0);\n-  db = args.pop();\n-  options = args.length ? args.shift() : {};\n+\n+  if (typeof db === 'undefined') {\n+    db =  options;\n+    options = {};\n+  }\n   // Final command \n   var command = {'getlasterror':1};\n   // If we have an options Object let's merge in the fields (fsync/wtimeout/w)\n@@ -121,20 +121,10 @@\n };\n \n DbCommand.createCreateIndexCommand = function(db, collectionName, fieldOrSpec, options) {\n-  var finalUnique = options == null || 'object' === typeof options ? false : options;\n   var fieldHash = {};\n   var indexes = [];\n   var keys;\n-  var sparse;\n-  var background;\n   \n-  // If the options is a hash\n-  if(options != null && 'object' === typeof options) {\n-    finalUnique = options['unique'] != null ? options['unique'] : false;\n-    sparse = options['sparse'] != null ? options['sparse'] : false;\n-    background = options['background'] != null ? options['background'] : false;\n-  }\n-\n   // Get all the fields accordingly\n   if (fieldOrSpec.constructor === String) {             // 'type'\n     indexes.push(fieldOrSpec + '_' + 1);\n@@ -163,17 +153,27 @@\n       indexes.push(key + '_' + fieldOrSpec[key]);\n       fieldHash[key] = fieldOrSpec[key];\n     });\n-  } else {\n-    // undefined\n   }\n   \n   // Generate the index name\n   var indexName = indexes.join(\"_\");\n   // Build the selector\n   var selector = {'ns':(db.databaseName + \".\" + collectionName), 'key':fieldHash, 'name':indexName};\n-  selector['unique'] = finalUnique;\n-  selector['sparse'] = sparse;\n-  selector['background'] = background;\n+\n+  // Ensure we have a correct finalUnique\n+  var finalUnique = options == null || 'object' === typeof options ? false : options;\n+  // Set up options\n+  options = options == null || typeof options == 'boolean' ? {} : options;\n+  \n+  // Add all the options\n+  var keys = Object.keys(options);\n+  // Add all the fields to the selector\n+  for(var i = 0; i < keys.length; i++) {\n+    selector[keys[i]] = options[keys[i]];\n+  }\n+  \n+  // If we don't have the unique property set on the selector\n+  if(selector['unique'] == null) selector['unique'] = finalUnique;\n   // Create the insert command for the index and return the document\n   return new InsertCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_INDEX_COLLECTION, false).add(selector);\n };\n@@ -186,6 +186,10 @@\n   return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'deleteIndexes':collectionName, 'index':indexName}, null);\n };\n \n+DbCommand.createReIndexCommand = function(db, collectionName) {\n+  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'reIndex':collectionName}, null);\n+};\n+\n DbCommand.createDropDatabaseCommand = function(db) {\n   return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'dropDatabase':1}, null);\n };\n@@ -197,3 +201,7 @@\n DbCommand.createAdminDbCommand = function(db, command_hash) {\n   return new DbCommand(db, \"admin.\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, command_hash, null);\n };\n+\n+DbCommand.createDbSlaveOkCommand = function(db, command_hash, options) {\n+  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT | QueryCommand.OPTS_SLAVE, 0, -1, command_hash, null, options);\n",
					"match": false,
					"packageHash": "cf6c046e7b21cbb3b6d660669859c6ebffed3ad8a6f6be9c3cd5fe9f66b1b5c5",
					"size": 8775,
					"sourceHash": "6f41be547a863b0b0169eba3b2ecc8d56a3ca07357817dae0fd22c2063b2d8ed",
					"status": "content"
				},
				"lib/mongodb/commands/delete_command.js": {
					"diff": "--- published/lib/mongodb/commands/delete_command.js\n+++ rebuilt/lib/mongodb/commands/delete_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug, \n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -11,7 +9,7 @@\n \n   // Validate correctness off the selector\n   var object = selector;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;        \n     if(object_size != object.length)  {\n       var error = new Error(\"delete raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -40,7 +38,7 @@\n */\n DeleteCommand.prototype.toBinary = function() {\n   // Calculate total length of the document\n-  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson_serializer.BSON.calculateObjectSize(this.selector) + (4 * 4);\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.selector, false, true) + (4 * 4);\n   // Let's build the single pass buffer command\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n@@ -92,12 +90,12 @@\n \n   // Serialize the selector\n   // If we are passing a raw buffer, do minimal validation\n-  if(this.selector instanceof Buffer) {\n+  if(Buffer.isBuffer(this.selector)) {\n     documentLength = this.selector.length;\n     // Copy the data into the current buffer\n     this.selector.copy(_command, _index);\n   } else {\n-    documentLength = this.db.bson_serializer.BSON.serializeWithBufferAndIndex(this.selector, this.checkKeys, _command, _index) - _index + 1;\n+    documentLength = this.db.bson.serializeWithBufferAndIndex(this.selector, this.checkKeys, _command, _index) - _index + 1;\n   }\n   \n   // Write the length to the document\n",
					"match": false,
					"packageHash": "e9b36119d067198011702a3647e03b3d88192dc66fbda977670e2a8f73163de3",
					"size": 3994,
					"sourceHash": "4e4d0f12685765c7a6eb62947f5f73432a07453a99925a1a0e1a63d53d4d218a",
					"status": "content"
				},
				"lib/mongodb/commands/get_more_command.js": {
					"diff": "--- published/lib/mongodb/commands/get_more_command.js\n+++ rebuilt/lib/mongodb/commands/get_more_command.js\n@@ -1,8 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n   inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  binaryutils = require('../bson/binary_utils');\n+  binaryutils = require('../utils');\n \n /**\n   Get More Document Command\n@@ -27,31 +25,28 @@\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n   // Write the header information to the buffer\n-  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n-  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n-  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n-  _command[_index] = totalLengthOfCommand & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = totalLengthOfCommand & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 24) & 0xff;     \n+\n   // Write the request ID\n-  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n-  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n-  _command[_index + 1] = (this.requestId >> 8) & 0xff;\n-  _command[_index] = this.requestId & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = this.requestId & 0xff;\n+  _command[_index++] = (this.requestId >> 8) & 0xff;\n+  _command[_index++] = (this.requestId >> 16) & 0xff;\n+  _command[_index++] = (this.requestId >> 24) & 0xff;     \n+\n   // Write zero\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n+\n   // Write the op_code for the command\n-  _command[_index + 3] = (GetMoreCommand.OP_GET_MORE >> 24) & 0xff;     \n-  _command[_index + 2] = (GetMoreCommand.OP_GET_MORE >> 16) & 0xff;\n-  _command[_index + 1] = (GetMoreCommand.OP_GET_MORE >> 8) & 0xff;\n-  _command[_index] = GetMoreCommand.OP_GET_MORE & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = GetMoreCommand.OP_GET_MORE & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 8) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 16) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 24) & 0xff;     \n \n   // Write zero\n   _command[_index++] = 0;\n@@ -64,31 +59,25 @@\n   _command[_index - 1] = 0;    \n \n   // Number of documents to return\n-  _command[_index + 3] = (this.numberToReturn >> 24) & 0xff;     \n-  _command[_index + 2] = (this.numberToReturn >> 16) & 0xff;\n-  _command[_index + 1] = (this.numberToReturn >> 8) & 0xff;\n-  _command[_index] = this.numberToReturn & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = this.numberToReturn & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 8) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 16) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 24) & 0xff;     \n   \n   // Encode the cursor id\n   var low_bits = this.cursorId.getLowBits();\n   // Encode low bits\n-  _command[_index + 3] = (low_bits >> 24) & 0xff;     \n-  _command[_index + 2] = (low_bits >> 16) & 0xff;\n-  _command[_index + 1] = (low_bits >> 8) & 0xff;\n-  _command[_index] = low_bits & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = low_bits & 0xff;\n+  _command[_index++] = (low_bits >> 8) & 0xff;\n+  _command[_index++] = (low_bits >> 16) & 0xff;\n+  _command[_index++] = (low_bits >> 24) & 0xff;     \n   \n   var high_bits = this.cursorId.getHighBits();\n   // Encode high bits\n-  _command[_index + 3] = (high_bits >> 24) & 0xff;     \n-  _command[_index + 2] = (high_bits >> 16) & 0xff;\n-  _command[_index + 1] = (high_bits >> 8) & 0xff;\n-  _command[_index] = high_bits & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n-  \n+  _command[_index++] = high_bits & 0xff;\n+  _command[_index++] = (high_bits >> 8) & 0xff;\n+  _command[_index++] = (high_bits >> 16) & 0xff;\n",
					"match": false,
					"packageHash": "2bdf5588881e7d743730642f844a484ccb12669438bd34142ce80041d89690d5",
					"size": 3218,
					"sourceHash": "9c9ec06d4ad6306c6823afb357aa2f5a41fffc9ac401894f0ba240e13b72a592",
					"status": "content"
				},
				"lib/mongodb/commands/insert_command.js": {
					"diff": "--- published/lib/mongodb/commands/insert_command.js\n+++ rebuilt/lib/mongodb/commands/insert_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -37,7 +35,7 @@\n InsertCommand.OP_INSERT =\t2002;\n \n InsertCommand.prototype.add = function(document) {\n-  if(document instanceof Buffer) {\n+  if(Buffer.isBuffer(document)) {\n     var object_size = document[0] | document[1] << 8 | document[2] << 16 | document[3] << 24;    \n     if(object_size != document.length)  {\n       var error = new Error(\"insert raw message size does not match message header size [\" + document.length + \"] != [\" + object_size + \"]\");\n@@ -63,14 +61,14 @@\n   var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + (4 * 4);\n   // var docLength = 0\n   for(var i = 0; i < this.documents.length; i++) {\n-    if(this.documents[i] instanceof Buffer) {\n+    if(Buffer.isBuffer(this.documents[i])) {\n       totalLengthOfCommand += this.documents[i].length;\n     } else {\n       // Calculate size of document\n-      totalLengthOfCommand += this.db.bson_serializer.BSON.calculateObjectSize(this.documents[i], this.serializeFunctions);      \n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.documents[i], this.serializeFunctions, true);\n     }\n   }\n-    \n+  \n   // Let's build the single pass buffer command\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n@@ -119,13 +117,13 @@\n \n     // Serialize the selector\n     // If we are passing a raw buffer, do minimal validation\n-    if(object instanceof Buffer) {      \n+    if(Buffer.isBuffer(object)) {\n       documentLength = object.length;\n       // Copy the data into the current buffer\n       object.copy(_command, _index);\n     } else {\n       // Serialize the document straight to the buffer\n-      documentLength = this.db.bson_serializer.BSON.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n+      documentLength = this.db.bson.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n     }\n \n     // Write the length to the document\n",
					"match": false,
					"packageHash": "b989190480bd171cbf1402e85a514faa27a7548e130aed6d56e8c009f81f9372",
					"size": 5193,
					"sourceHash": "90970dad921cc155b641d84092b4cee8d1b9f7486e4199685a856dc4fb4ed19d",
					"status": "content"
				},
				"lib/mongodb/commands/kill_cursor_command.js": {
					"diff": "--- published/lib/mongodb/commands/kill_cursor_command.js\n+++ rebuilt/lib/mongodb/commands/kill_cursor_command.js\n@@ -1,8 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n   inherits = require('util').inherits,\n-  binaryutils = require('../bson/binary_utils'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  binaryutils = require('../utils');\n \n /**\n   Insert Document Command\n",
					"match": false,
					"packageHash": "9936ea981ab114f879b79fd0debb73194ffe4315f27e356a60cbe02c403e0ab8",
					"size": 3427,
					"sourceHash": "d01af72568b490f4e88bb07da854ed5b050494e55e90b60b8b3d11561f8ffa5e",
					"status": "content"
				},
				"lib/mongodb/commands/query_command.js": {
					"diff": "--- published/lib/mongodb/commands/query_command.js\n+++ rebuilt/lib/mongodb/commands/query_command.js\n@@ -1,8 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -11,20 +8,21 @@\n   BaseCommand.call(this);\n \n   // Validate correctness off the selector\n-  var object = query;\n-  if(object instanceof Buffer) {\n-    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n-    if(object_size != object.length)  {\n+  var object = query,\n+    object_size;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n       var error = new Error(\"query selector raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n       error.name = 'MongoError';\n       throw error;\n     }\n   }\n \n-  var object = returnFieldSelector;\n-  if(object instanceof Buffer) {\n-    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n-    if(object_size != object.length)  {\n+  object = returnFieldSelector;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n       var error = new Error(\"query fields raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n       error.name = 'MongoError';\n       throw error;\n@@ -45,7 +43,7 @@\n   // Let us defined on a command basis if we want functions to be serialized or not\n   if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n     this.serializeFunctions = true;\n-  }  \n+  }\n };\n \n inherits(QueryCommand, BaseCommand);\n@@ -66,18 +64,18 @@\n QueryCommand.prototype.toBinary = function() {\n   var totalLengthOfCommand = 0;\n   // Calculate total length of the document\n-  if(this.query instanceof Buffer) {\n+  if(Buffer.isBuffer(this.query)) {\n     totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.query.length + (4 * 4);    \n   } else {\n-    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.db.bson_serializer.BSON.calculateObjectSize(this.query, this.serializeFunctions) + (4 * 4);    \n+    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.db.bson.calculateObjectSize(this.query, this.serializeFunctions, true) + (4 * 4);    \n   }\n   \n   // Calculate extra fields size\n-  if(this.returnFieldSelector != null && !(this.returnFieldSelector instanceof Buffer))  {\n+  if(this.returnFieldSelector != null && !(Buffer.isBuffer(this.returnFieldSelector)))  {\n     if(Object.keys(this.returnFieldSelector).length > 0) {\n-      totalLengthOfCommand += this.db.bson_serializer.BSON.calculateObjectSize(this.returnFieldSelector, this.serializeFunctions);\n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.returnFieldSelector, this.serializeFunctions, true);\n     }\n-  } else if(this.returnFieldSelector instanceof Buffer) {\n+  } else if(Buffer.isBuffer(this.returnFieldSelector)) {\n     totalLengthOfCommand += this.returnFieldSelector.length;\n   }\n \n@@ -144,15 +142,15 @@\n   var object = this.query;\n \n   // Serialize the selector\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     documentLength = object.length;\n     // Copy the data into the current buffer\n     object.copy(_command, _index);\n   } else {\n     // Serialize the document straight to the buffer\n-    documentLength = this.db.bson_serializer.BSON.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n+    documentLength = this.db.bson.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n   }\n-    \n+\n   // Write the length to the document\n   _command[_index + 3] = (documentLength >> 24) & 0xff;     \n   _command[_index + 2] = (documentLength >> 16) & 0xff;\n@@ -164,9 +162,9 @@\n   _command[_index - 1] = 0;    \n \n   // Push field selector if available\n-  if(this.returnFieldSelector != null && !(this.returnFieldSelector instanceof Buffer))  {\n+  if(this.returnFieldSelector != null && !(Buffer.isBuffer(this.returnFieldSelector)))  {\n",
					"match": false,
					"packageHash": "e2b671b14b58e2256a9be7753b40ed2830f761167953cead7016c1cbaf0062fd",
					"size": 8464,
					"sourceHash": "42e8d9b5c0118fe3e93e96f00a9998faafbc79b058168904ec3341353c71240b",
					"status": "content"
				},
				"lib/mongodb/commands/update_command.js": {
					"diff": "--- published/lib/mongodb/commands/update_command.js\n+++ rebuilt/lib/mongodb/commands/update_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Update Document Command\n@@ -10,7 +8,7 @@\n   BaseCommand.call(this);\n \n   var object = spec;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n     if(object_size != object.length)  {\n       var error = new Error(\"update spec raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -20,7 +18,7 @@\n   }\n \n   var object = document;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n     if(object_size != object.length)  {\n       var error = new Error(\"update document raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -65,8 +63,8 @@\n */\n UpdateCommand.prototype.toBinary = function() {\n   // Calculate total length of the document\n-  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson_serializer.BSON.calculateObjectSize(this.spec, false) +\n-      this.db.bson_serializer.BSON.calculateObjectSize(this.document, this.serializeFunctions) + (4 * 4);\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.spec, false, true) +\n+      this.db.bson.calculateObjectSize(this.document, this.serializeFunctions, true) + (4 * 4);\n \n   // Let's build the single pass buffer command\n   var _index = 0;\n@@ -122,14 +120,14 @@\n \n   // Serialize the selector\n   // If we are passing a raw buffer, do minimal validation\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n     if(object_size != object.length) throw new Error(\"raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n     documentLength = object.length;\n     // Copy the data into the current buffer\n     object.copy(_command, _index);\n   } else {\n-    documentLength = this.db.bson_serializer.BSON.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, false) - _index + 1;\n+    documentLength = this.db.bson.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, false) - _index + 1;\n   }\n \n   // Write the length to the document\n@@ -148,14 +146,14 @@\n \n   // Serialize the document\n   // If we are passing a raw buffer, do minimal validation\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n     if(object_size != object.length) throw new Error(\"raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n     documentLength = object.length;\n     // Copy the data into the current buffer\n     object.copy(_command, _index);\n   } else {    \n-    documentLength = this.db.bson_serializer.BSON.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n+    documentLength = this.db.bson.serializeWithBufferAndIndex(object, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n   }\n \n   // Write the length to the document\n",
					"match": false,
					"packageHash": "3a48cde2ac3664b35b361f5e4e80e8e51d7933e69e129b684478f50816c651e6",
					"size": 6765,
					"sourceHash": "1e9efcade1290911d79395a36a74e0f4bdfc7608c739482547738ac544d537d3",
					"status": "content"
				},
				"lib/mongodb/connection/connection.js": {
					"diff": "--- published/lib/mongodb/connection/connection.js\n+++ rebuilt/lib/mongodb/connection/connection.js\n@@ -1,12 +1,9 @@\n var utils = require('./connection_utils'),\n   inherits = require('util').inherits,\n   net = require('net'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  // SimpleEmitter = require('./simple_emitter').SimpleEmitter,\n   EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits,\n-  binaryutils = require('../bson/binary_utils'),\n+  binaryutils = require('../utils'),\n   tls = require('tls');\n \n var Connection = exports.Connection = function(id, socketOptions) {\n@@ -22,8 +19,7 @@\n   //\n   // Connection parsing state\n   //\n-  \n-  this.maxBsonSize = socketOptions.maxBsonSize ? socketOptions.maxBsonSize : global.DEFAULT_MAX_BSON_SIZE;  \n+  this.maxBsonSize = socketOptions.maxBsonSize ? socketOptions.maxBsonSize : Connection.DEFAULT_MAX_BSON_SIZE;  \n   // Contains the current message bytes\n   this.buffer = null;\n   // Contains the current message size\n@@ -41,33 +37,31 @@\n }\n \n // Set max bson size\n-Connection.DEFAULT_MAX_BSON_SIZE = 4 * 1024 * 1024 * 4 * 3;\n+Connection.DEFAULT_MAX_BSON_SIZE = 1024 * 1024 * 4;\n \n // Inherit event emitter so we can emit stuff wohoo\n-// inherits(Connection, SimpleEmitter);\n inherits(Connection, EventEmitter);\n \n Connection.prototype.start = function() {\n-  // console.log(\"------------------------------------------- start\")\n-  // // Set up event emitter\n-  // EventEmitter.call(this);  \n-  \n   // If we have a normal connection\n   if(this.socketOptions.ssl) {\n     // Create a new stream\n-    this.connection = new net.Stream();\n+    this.connection = new net.Socket();    \n     // Set options on the socket\n     this.connection.setTimeout(this.socketOptions.timeout);\n-    this.connection.setNoDelay(this.socketOptions.noDelay);\n+    // Work around for 0.4.X\n+    if(process.version.indexOf(\"v0.4\") == -1) this.connection.setNoDelay(this.socketOptions.noDelay);\n     // Set keep alive if defined\n-    if(this.socketOptions.keepAlive > 0) {\n-      this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n-    } else {\n-      this.connection.setKeepAlive(false);\n-    } \n-\n+    if(process.version.indexOf(\"v0.4\") == -1) {\n+      if(this.socketOptions.keepAlive > 0) {\n+        this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n+      } else {\n+        this.connection.setKeepAlive(false);\n+      }         \n+    }\n+    \n     // Set up pair for tls with server, accept self-signed certificates as well\n-    this.pair = tls.createSecurePair(false);\n+    var pair = this.pair = tls.createSecurePair(false);\n     // Set up encrypted streams\n     this.pair.encrypted.pipe(this.connection);\n     this.connection.pipe(this.pair.encrypted);\n@@ -79,7 +73,7 @@\n     this.pair.cleartext.on(\"data\", createDataHandler(this));\n     // Add handlers\n     this.connection.on(\"error\", errorHandler(this));\n-    // Add all handlers to the socket to manage it\n+    // this.connection.on(\"connect\", connectHandler(this));\n     this.connection.on(\"end\", endHandler(this));\n     this.connection.on(\"timeout\", timeoutHandler(this));\n     this.connection.on(\"drain\", drainHandler(this));\n@@ -87,19 +81,20 @@\n     // Start socket\n     this.connection.connect(this.socketOptions.port, this.socketOptions.host);\n   } else {\n-    // Create a new stream\n-    this.connection = new net.Stream();\n-    // // Create new connection instance\n-    // this.connection = new net.Socket();\n+    // Create new connection instance\n+    this.connection = net.createConnection(this.socketOptions.port, this.socketOptions.host);\n     // Set options on the socket\n     this.connection.setTimeout(this.socketOptions.timeout);\n-    this.connection.setNoDelay(this.socketOptions.noDelay);\n+    // Work around for 0.4.X\n+    if(process.version.indexOf(\"v0.4\") == -1) this.connection.setNoDelay(this.socketOptions.noDelay);\n     // Set keep alive if defined\n-    if(this.socketOptions.keepAlive > 0) {\n-      this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n",
					"match": false,
					"packageHash": "e69b750f2060f3c3beb5c66a782d0d1bf8a8089ffcdb0ef434c4d7794b006566",
					"size": 17198,
					"sourceHash": "a05c2fb8cfdcc1099cb7e68c0b153a5789d48a77dbfe1a1d3b78f940f008ee46",
					"status": "content"
				},
				"lib/mongodb/connection/connection_pool.js": {
					"diff": "--- published/lib/mongodb/connection/connection_pool.js\n+++ rebuilt/lib/mongodb/connection/connection_pool.js\n@@ -18,6 +18,7 @@\n   this.bson = bson;\n   // PoolSize is always + 1 for special reserved \"measurment\" socket (like ping, stats etc)\n   this.poolSize = poolSize;\n+  this.minPoolSize = Math.floor(this.poolSize / 2) + 1;\n   \n   // Set default settings for the socket options\n   utils.setIntegerParameter(this.socketOptions, 'timeout', 0);\n@@ -31,13 +32,7 @@\n   utils.setIntegerParameter(this.socketOptions, 'bufferSize', 0);  \n   \n   // Internal structures\n-  // this.waitingToOpen = {};\n-  // this.connectionsWithErrors = {};\n-  // this.openConnections = {};\n-  // this.connectionsWithErrors = [];\n-  this.openConnections = [];\n-  this.connections = [];\n-  \n+  this.openConnections = [];  \n   // Assign connection id's\n   this.connectionId = 0;\n   \n@@ -52,7 +47,7 @@\n inherits(ConnectionPool, EventEmitter);\n \n ConnectionPool.prototype.setMaxBsonSize = function(maxBsonSize) {\n-  if (typeof maxBsonSize == null){\n+  if(maxBsonSize == null){\n     maxBsonSize = Connection.DEFAULT_MAX_BSON_SIZE;\n   }     \n \n@@ -71,15 +66,15 @@\n     connection.logger = _self.logger;\n     // Connect handler\n     connection.on(\"connect\", function(err, connection) {\n-      // console.log(\"------------------------------ ssl connect :: \" + _self.poolSize)\n       // Add connection to list of open connections\n       _self.openConnections.push(connection);\n-      _self.connections.push(connection)\n-\n       // If the number of open connections is equal to the poolSize signal ready pool\n-      if(_self.connections.length === _self.poolSize && _self._poolState !== 'disconnected') {\n+      if(_self.openConnections.length === _self.poolSize && _self._poolState !== 'disconnected') {\n+        // Set connected\n+        _self._poolState = 'connected';\n+        // Emit pool ready\n         _self.emit(\"poolReady\");\n-      } else if(_self.connections.length < _self.poolSize) {\n+      } else if(_self.openConnections.length < _self.poolSize) {\n         // We need to open another connection, make sure it's in the next\n         // tick so we don't get a cascade of errors\n         process.nextTick(function() {\n@@ -92,10 +87,9 @@\n \n     // Error handler\n     connection.on(\"error\", function(err, connection) {\n-      // console.log(\"============================== connection :: \" + connection.id + \" :: \" + numberOfErrors)\n       numberOfErrors++;\n       // If we are already disconnected ignore the event\n-      if(connectionStatus !== 'disconnected') {\n+      if(connectionStatus != 'disconnected' && _self.listeners(\"error\").length > 0) {\n         _self.emit(\"error\", err);        \n       }\n \n@@ -103,16 +97,14 @@\n       connectionStatus = 'disconnected';\n       // Set disconnected\n       _self._poolState = 'disconnected'; \n-      // Clean up\n-      _self.openConnections = [];    \n-      _self.connections = [];\n+      // Stop\n+      _self.stop();\n     });\n \n     // Close handler\n     connection.on(\"close\", function() {\n-      // console.log(\"--------------------------------- close received :: \" + connectionStatus)\n       // If we are already disconnected ignore the event\n-      if(connectionStatus !== 'disconnected') {\n+      if(connectionStatus !== 'disconnected' && _self.listeners(\"close\").length > 0) {\n         _self.emit(\"close\");        \n       }\n \n@@ -120,30 +112,32 @@\n       connectionStatus = 'disconnected';\n       // Set disconnected\n       _self._poolState = 'disconnected'; \n-      // Clean up\n-      _self.openConnections = [];    \n-      _self.connections = [];\n+      // Stop\n+      _self.stop();\n     });\n \n     // Timeout handler\n     connection.on(\"timeout\", function(err, connection) {\n",
					"match": false,
					"packageHash": "f83634c3ccc7573f637cfea255bbee173a1d8fb8d43dd80af434837b4f088715",
					"size": 11677,
					"sourceHash": "ccd192af58ec6c8b8d2fe094bf77ce695fca8f47f7e69c869489fadb0e9f00ab",
					"status": "content"
				},
				"lib/mongodb/connection/repl_set_servers.js": {
					"match": false,
					"packageHash": "376159ac582285b6cbbcfa4ee086ea348dfb455a04fa9b3cf6601673c6211328",
					"size": 34034,
					"status": "missing-in-source"
				},
				"lib/mongodb/connection/server.js": {
					"diff": "--- published/lib/mongodb/connection/server.js\n+++ rebuilt/lib/mongodb/connection/server.js\n@@ -2,11 +2,32 @@\n   DbCommand = require('../commands/db_command').DbCommand,\n   MongoReply = require('../responses/mongo_reply').MongoReply,\n   ConnectionPool = require('./connection_pool').ConnectionPool,\n-  SimpleEmitter = require('./simple_emitter').SimpleEmitter,\n-  MongoReply = require(\"../responses/mongo_reply\").MongoReply,\n+  EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits;\n \n-var Server = exports.Server = function(host, port, options) {\n+/**\n+ * Class representing a single MongoDB Server connection\n+ *\n+ * Options\n+ *  - **readPreference** {String, default:null}, set's the read preference (Server.READ_PRIMAR, Server.READ_SECONDARY_ONLY, Server.READ_SECONDARY)\n+ *  - **ssl** {Boolean, default:false}, use ssl connection (needs to have a mongod server with ssl support)\n+ *  - **slaveOk** {Boolean, default:false}, legacy option allowing reads from secondary, use **readPrefrence** instead.\n+ *  - **poolSize** {Number, default:1}, number of connections in the connection pool, set to 1 as default for legacy reasons.\n+ *  - **socketOptions** {Object, default:null}, an object containing socket options to use (noDelay:(boolean), keepAlive:(number), timeout:(number))\n+ *  - **logger** {Object, default:null}, an object representing a logger that you want to use, needs to support functions debug, log, error **({error:function(message, object) {}, log:function(message, object) {}, debug:function(message, object) {}})**.\n+ *  - **auto_reconnect** {Boolean, default:false}, reconnect on error.\n+ *\n+ * @class Represents a Server connection.\n+ * @param {String} host the server host\n+ * @param {Number} port the server port\n+ * @param {Object} [options] optional options for insert command\n+ */\n+function Server(host, port, options) {\n+  // Set up event emitter\n+  EventEmitter.call(this);  \n+  // Set up Server instance\n+  if(!(this instanceof Server)) return new Server(host, port, options);\n+  \n   var self = this;\n   this.host = host;\n   this.port = port;\n@@ -17,14 +38,25 @@\n   this.poolSize = this.options.poolSize == null ? 1 : this.options.poolSize;\n   this.ssl = this.options.ssl == null ? false : this.options.ssl;\n   this.slaveOk = this.options[\"slave_ok\"];\n-  // Setters and getters\n-  this.__defineGetter__(\"autoReconnect\", function() { return self.options['auto_reconnect'] == null ? false : this.options['auto_reconnect']; });\n-  this.__defineGetter__(\"connection\", function() { return self.internalConnection; });\n-  this.__defineSetter__(\"connection\", function(connection) { self.internalConnection = connection; });\n-  this.__defineGetter__(\"master\", function() { return self.internalMaster; });\n-  this.__defineSetter__(\"master\", function(value) { self.internalMaster = value; });\n-  this.__defineGetter__(\"primary\", function() { return self; });\n-  this.__defineGetter__(\"readPreference\", function() { return Server.READ_PRIMARY; });\n+  this._used = false;\n+  \n+  // Get the readPreference\n+  var readPreference = this.options['readPreference'];  \n+  // Read preference setting\n+  if(readPreference != null) {\n+    if(readPreference != Server.READ_PRIMARY && readPreference != Server.READ_SECONDARY_ONLY\n+      && readPreference != Server.READ_SECONDARY) {\n+        throw new Error(\"Illegal readPreference mode specified, \" + readPreference);\n+    }\n+    \n+    // Set read Preference\n+    this._readPreference = readPreference;\n+  } else {\n+    this._readPreference = null;        \n+  }\n+  \n+  // Contains the isMaster information returned from the server\n+  this.isMasterDoc;\n \n   // Set default connection pool options\n   this.socketOptions = this.options.socketOptions != null ? this.options.socketOptions : {};\n@@ -37,7 +69,7 @@\n   this.logger = this.options.logger != null \n     && (typeof this.options.logger.debug == 'function') \n     && (typeof this.options.logger.error == 'function') \n-    && (typeof this.options.logger.debug == 'function') \n+    && (typeof this.options.logger.log == 'function') \n       ? this.options.logger : {error:function(message, object) {}, log:function(message, object) {}, debug:function(message, object) {}};\n \n   // Just keeps list of events we allow\n@@ -49,21 +81,87 @@\n   this._state = {'runtimeStats': {'queryStats':new RunningStats()}};  \n   // Do we record server stats or not\n   this.recordQueryStats = false;\n+  \n+  // Setters and getters\n+  Object.defineProperty(this, \"autoReconnect\", { enumerable: true\n+    , get: function () {\n+        return this.options['auto_reconnect'] == null ? false : this.options['auto_reconnect'];\n+      }\n+  });  \n+\n+  Object.defineProperty(this, \"connection\", { enumerable: true\n+    , get: function () {\n+        return this.internalConnection;\n+      }\n+    , set: function(connection) {\n+        this.internalConnection = connection;\n+      }\n",
					"match": false,
					"packageHash": "707d5c8a09b72e0a8ee4da42706bb284236baeee7d366e3873c2f71acfcd7793",
					"size": 17633,
					"sourceHash": "2509622ad0acf9deda2227f721c9bc9a58e2ac9d418b1c75fe584cc42bcac5b3",
					"status": "content"
				},
				"lib/mongodb/connection/simple_emitter.js": {
					"match": false,
					"packageHash": "06f23ea1be542085b58435016ba2d0bd7ead99822d13837161357e0110f777cc",
					"size": 1760,
					"status": "missing-in-source"
				},
				"lib/mongodb/cursor.js": {
					"diff": "--- published/lib/mongodb/cursor.js\n+++ rebuilt/lib/mongodb/cursor.js\n@@ -1,46 +1,41 @@\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   GetMoreCommand = require('./commands/get_more_command').GetMoreCommand,\n   KillCursorCommand = require('./commands/kill_cursor_command').KillCursorCommand,\n-  Long = require('./goog/math/long').Long,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  Long = require('bson').Long,\n+  CursorStream = require('./cursorstream'),\n+  utils = require('./utils');\n \n /**\n  * Constructor for a cursor object that handles all the operations on query result\n- * using find. This cursor object is unidirectional and cannot traverse backwards.\n- * As an alternative, {@link Cursor#toArray} can be used to obtain all the results.\n- * Clients should not be creating a cursor directly, but use {@link Collection#find}\n- * to acquire a cursor.\n- *\n- * @constructor\n- *\n- * @param db {Db} The database object to work with\n- * @param collection {Colleciton} The collection to query\n- * @param selector\n- * @param fields\n- * @param skip {number}\n- * @param limit {number} The number of results to return. -1 has a special meaning and\n- *     is used by {@link Db#eval}. A value of 1 will also be treated as if it were -1.\n- * @param sort {string|Array<Array<string|object> >} Please refer to {@link Cursor#sort}\n- * @param hint\n- * @param explain\n- * @param snapshot\n- * @param timeout\n- * @param tailable {?boolean}\n- * @param batchSize {?number} The number of the subset of results to request the database\n- *     to return for every request. This should initially be greater than 1 otherwise\n- *     the database will automatically close the cursor. The batch size can be set to 1\n- *     with {@link Cursor#batchSize} after performing the initial query to the database.\n- *\n- * @see Cursor#toArray\n- * @see Cursor#skip\n- * @see Cursor#sort\n- * @see Cursor#limit\n- * @see Cursor#batchSize\n- * @see Collection#find\n- * @see Db#eval\n- */\n-var Cursor = exports.Cursor = function(db, collection, selector, fields, skip, limit, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk, raw) {\n+ * using find. This cursor object is unidirectional and cannot traverse backwards. Clients should not be creating a cursor directly, \n+ * but use find to acquire a cursor.\n+ *\n+ * @class Represents a Cursor.\n+ * @param {Db} db the database object to work with.\n+ * @param {Collection} collection the collection to query.\n+ * @param {Object} selector the query selector.\n+ * @param {Object} fields an object containing what fields to include or exclude from objects returned.\n+ * @param {Number} skip number of documents to skip.\n+ * @param {Number} limit the number of results to return. -1 has a special meaning and is used by Db.eval. A value of 1 will also be treated as if it were -1.\n+ * @param {String|Array|Object} sort the required sorting for the query.\n+ * @param {Object} hint force the query to use a specific index.\n+ * @param {Boolean} explain return the explaination of the query.\n+ * @param {Boolean} snapshot Snapshot mode assures no duplicates are returned.\n+ * @param {Boolean} timeout allow the query to timeout.\n+ * @param {Boolean} tailable allow the cursor to be tailable.\n+ * @param {Number} batchSize the number of the subset of results to request the database to return for every request. This should initially be greater than 1 otherwise the database will automatically close the cursor. The batch size can be set to 1 with cursorInstance.batchSize after performing the initial query to the database.\n+ * @param {Boolean} raw return all query documents as raw buffers (default false).\n+ * @param {Boolean} read specify override of read from source (primary/secondary).\n+ * @param {Boolean} returnKey only return the index key.\n+ * @param {Number} maxScan limit the number of items to scan.\n+ * @param {Number} min set index bounds.\n+ * @param {Number} max set index bounds.\n+ * @param {Boolean} showDiskLoc show disk location of results.\n+ * @param {String} comment you can put a $comment field on a query to make looking in the profiler logs simpler.\n+ */\n+function Cursor(db, collection, selector, fields, skip, limit\n+\t, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk, raw, read\n+\t, returnKey, maxScan, min, max, showDiskLoc, comment) {\n   this.db = db;\n   this.collection = collection;\n   this.selector = selector;\n@@ -56,10 +51,17 @@\n   this.batchSizeValue = batchSize == null ? 0 : batchSize;\n   this.slaveOk = slaveOk == null ? collection.slaveOk : slaveOk;\n   this.raw = raw == null ? false : raw;\n-\n+  this.read = read == null ? true : read;\n+  this.returnKey = returnKey;\n+  this.maxScan = maxScan;\n+  this.min = min;\n+  this.max = max;\n+  this.showDiskLoc = showDiskLoc;\n+  this.comment = comment;\n+  \n   this.totalNumberOfRecords = 0;\n   this.items = [];\n-  this.cursorId = this.db.bson_serializer.Long.fromInt(0);\n+  this.cursorId = Long.fromInt(0);\n \n   // State variables for the cursor\n",
					"match": false,
					"packageHash": "c4a7e8f161e94af96a3e3b6531fd85cc80d9aa9342d0409aaad1aa2eb69426ac",
					"size": 23547,
					"sourceHash": "c4a6fb5b596bb88a2e03a3d00859e3cb8ae2ac573e12fa196743abfbf01737ea",
					"status": "content"
				},
				"lib/mongodb/db.js": {
					"diff": "--- published/lib/mongodb/db.js\n+++ rebuilt/lib/mongodb/db.js\n@@ -1,23 +1,77 @@\n+/**\n+ * Module dependencies.\n+ * @ignore\n+ */\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   DbCommand = require('./commands/db_command').DbCommand,\n-  BinaryParser = require('./bson/binary_parser').BinaryParser,\n   MongoReply = require('./responses/mongo_reply').MongoReply,\n   Admin = require('./admin').Admin,\n   Collection = require('./collection').Collection,\n   Server = require('./connection/server').Server,\n-  ReplSetServers = require('./connection/repl_set_servers').ReplSetServers,\n+  ReplSet = require('./connection/repl_set').ReplSet,\n   Cursor = require('./cursor').Cursor,\n   EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits,\n-  crypto = require('crypto'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  crypto = require('crypto');\n \n-var Db = exports.Db = function(databaseName, serverConfig, options) {\n+/**\n+ * Internal class for callback storage \n+ * @ignore\n+ */\n+var CallbackStore = function() {\n+  // Make class an event emitter\n+  EventEmitter.call(this);\n+  // Add a info about call variable\n+  this._notReplied = {};\n+}\n+\n+/**\n+ * @ignore\n+ */\n+inherits(CallbackStore, EventEmitter);\n+\n+/**\n+ * Create a new Db instance.\n+ *\n+ * Options\n+ *  - **strict** {true | {w:n, wtimeout:n} | {fsync:true}, default:false}, execute insert with a getLastError command returning the result of the insert command.\n+ *  - **native_parser** {Boolean, default:false}, use c++ bson parser.\n+ *  - **forceServerObjectId** {Boolean, default:false}, force server to create _id fields instead of client.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *  - **slaveOk** {Boolean, default:false}, allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions.\n+ *  - **raw** {Boolean, default:false}, peform operations using raw bson buffers.\n+ *  - **recordQueryStats** {Boolean, default:false}, record query statistics during execution.\n+ *  - **reaper** {Boolean, default:false}, enables the reaper, timing out calls that never return.\n+ *  - **reaperInterval** {Number, default:10000}, number of miliseconds between reaper wakups.\n+ *  - **reaperTimeout** {Number, default:30000}, the amount of time before a callback times out.\n+ *  - **retryMiliSeconds** {Number, default:5000}, number of miliseconds between retries.\n+ *  - **numberOfRetries** {Number, default:5}, number of retries off connection.\n+ *\n+ * @class Represents a Collection\n+ * @param {String} databaseName name of the database.\n+ * @param {Object} serverConfig server config object.\n+ * @param {Object} [options] additional options for the collection.\n+ */\n+function Db(databaseName, serverConfig, options) {\n+\n+  if(!(this instanceof Db)) return new Db(databaseName, serverConfig, options);\n+  \n   EventEmitter.call(this);\n   this.databaseName = databaseName;\n-  this.serverConfig = serverConfig;\n+  this.serverConfig = serverConfig;  \n   this.options = options == null ? {} : options;  \n+  // State to check against if the user force closed db\n+  this._applicationClosed = false;\n+  // Fetch the override flag if any\n+  var overrideUsedFlag = this.options['override_used_flag'] == null ? false : this.options['override_used_flag'];  \n+  // Verify that nobody is using this config\n+  if(!overrideUsedFlag && typeof this.serverConfig == 'object' && this.serverConfig._isUsed()) {\n+    throw new Error(\"A Server or ReplSet instance cannot be shared across multiple Db instances\");\n+  } else if(!overrideUsedFlag && typeof this.serverConfig == 'object'){\n+    // Set being used\n+    this.serverConfig._used = true;    \n+  }\n   \n   // Ensure we have a valid db name\n   validateDatabaseName(databaseName);\n@@ -25,30 +79,40 @@\n   // Contains all the connections for the db\n   try {\n     this.native_parser = this.options.native_parser;\n-    var serializer = this.options.native_parser ? require('../../external-libs/bson') : require('./bson/bson');\n-    this.bson_serializer = serializer;\n-    this.bson_deserializer = serializer;\n+    // The bson lib\n+    var bsonLib = this.bsonLib = this.options.native_parser ? require('bson').BSONNative : new require('bson').BSONPure;\n+    // Fetch the serializer object\n+    var BSON = bsonLib.BSON;\n+    // Create a new instance\n+    this.bson = new BSON([bsonLib.Long, bsonLib.ObjectID, bsonLib.Binary, bsonLib.Code, bsonLib.DBRef, bsonLib.Symbol, bsonLib.Double, bsonLib.Timestamp, bsonLib.MaxKey, bsonLib.MinKey]);\n",
					"match": false,
					"packageHash": "82c90875c1a7413867b3915fceb7db14dcfd05912f42465889020ac4e3062ff4",
					"size": 47992,
					"sourceHash": "8975ea6eee925ca6f0b33ba4cb074d8de4c712ad9540ccd346aeb97e626a7e93",
					"status": "content"
				},
				"lib/mongodb/goog/math/long.js": {
					"match": false,
					"packageHash": "118c11b152a346c593e4749cb3f7f0abbf33bd4766581638a35979c5c84e3551",
					"size": 23473,
					"status": "missing-in-source"
				},
				"lib/mongodb/gridfs/chunk.js": {
					"diff": "--- published/lib/mongodb/gridfs/chunk.js\n+++ rebuilt/lib/mongodb/gridfs/chunk.js\n@@ -1,10 +1,8 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  sys = require('util'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+var Binary = require('bson').Binary,\n+  ObjectID = require('bson').ObjectID;\n \n /**\n- * Class for representing a signle chunk in GridFS.\n+ * Class for representing a single chunk in GridFS.\n  *\n  * @class\n  *\n@@ -19,38 +17,48 @@\n  * @see Chunk#buildMongoObject\n  */\n var Chunk = exports.Chunk = function(file, mongoObject) {\n+  if(!(this instanceof Chunk)) return new Chunk(file, mongoObject);\n+  \n   this.file = file;\n+  var self = this;\n   var mongoObjectFinal = mongoObject == null ? {} : mongoObject;\n \n-  this.objectId = mongoObjectFinal._id == null ? new file.db.bson_serializer.ObjectID() : mongoObjectFinal._id;\n+  this.objectId = mongoObjectFinal._id == null ? new ObjectID() : mongoObjectFinal._id;\n   this.chunkNumber = mongoObjectFinal.n == null ? 0 : mongoObjectFinal.n;\n-  this.data = new file.db.bson_serializer.Binary();\n+  this.data = new Binary();\n \n   if(mongoObjectFinal.data == null) {\n-  } else if(mongoObjectFinal.data.constructor == String) {\n+  } else if(typeof mongoObjectFinal.data == \"string\") {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data, 'binary', 0);\n-    this.data = new file.db.bson_serializer.Binary(buffer);\n-  } else if(mongoObjectFinal.data.constructor == Array) {\n+    this.data = new Binary(buffer);\n+  } else if(Array.isArray(mongoObjectFinal.data)) {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data.join(''), 'binary', 0);\n-    this.data = new file.db.bson_serializer.Binary(buffer);\n-  } else if(mongoObjectFinal.data instanceof file.db.bson_serializer.Binary || Object.prototype.toString.call(mongoObjectFinal.data) == \"[object Binary]\") {    \n+    this.data = new Binary(buffer);\n+  } else if(mongoObjectFinal.data instanceof Binary || Object.prototype.toString.call(mongoObjectFinal.data) == \"[object Binary]\") {    \n     this.data = mongoObjectFinal.data;\n-  } else if(mongoObjectFinal.data instanceof Buffer) {\n+  } else if(Buffer.isBuffer(mongoObjectFinal.data)) {\n   } else {\n     throw Error(\"Illegal chunk format\");\n   }\n   // Update position\n   this.internalPosition = 0;\n-\t/**\n-\t * The position of the read/write head\n-\t * @name position\n-\t * @lends Chunk#\n-\t * @field\n-\t */\n-  this.__defineGetter__(\"position\", function() { return this.internalPosition; });\n-  this.__defineSetter__(\"position\", function(value) { this.internalPosition = value; });\n+\n+  /**\n+   * The position of the read/write head\n+   * @name position\n+   * @lends Chunk#\n+   * @field\n+   */\n+  Object.defineProperty(this, \"position\", { enumerable: true\n+    , get: function () {\n+        return this.internalPosition;\n+      }\n+    , set: function(value) {\n+        this.internalPosition = value;\n+      }\n+  });  \n };\n \n /**\n@@ -64,7 +72,8 @@\n Chunk.prototype.write = function(data, callback) {\n   this.data.write(data, this.internalPosition);\n   this.internalPosition = this.data.length();\n-  callback(null, this);\n+  if(callback != null) return callback(null, this);\n+  return this;\n };\n \n /**\n@@ -95,7 +104,6 @@\n       data = this.data.buffer.slice(this.internalPosition, this.internalPosition + length);\n     } else { //Native BSON\n       data = new Buffer(length);\n-      //length = data.write(this.data.read(this.internalPosition, length), 'binary', 0);\n       length = this.data.readInto(data, this.internalPosition);\n     }\n     this.internalPosition = this.internalPosition + length;\n@@ -132,7 +140,7 @@\n",
					"match": false,
					"packageHash": "9c6fe77edca5f0df675f326c7862915ed210d319867d3f9d318eecc76b78c6ed",
					"size": 6733,
					"sourceHash": "ebb1d349b3e79a84bc1a2f0a2d8bb5679a6f63325d9d48d0665c1296119ff3a5",
					"status": "content"
				},
				"lib/mongodb/gridfs/grid.js": {
					"diff": "--- published/lib/mongodb/gridfs/grid.js\n+++ rebuilt/lib/mongodb/gridfs/grid.js\n@@ -1,12 +1,18 @@\n var GridStore = require('./gridstore').GridStore,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  ObjectID = require('bson').ObjectID;\n \n /**\n- * Simple Grid interface\n+ * A class representation of a simple Grid interface.\n  *\n+ * @class Represents the Grid.\n+ * @param {Db} db A database instance to interact with.\n+ * @param {String} [fsName] optional different root collection for GridFS.\n+ * @return {Grid}\n  */\n-var Grid = exports.Grid = function(db, fsName) {\n+function Grid(db, fsName) {\n+\n+  if(!(this instanceof Grid)) return new Grid(db, fsName);\n+  \n   this.db = db;\n   this.fsName = fsName == null ? GridStore.DEFAULT_ROOT_COLLECTION : fsName;\n } \n@@ -14,13 +20,11 @@\n /**\n  * Puts binary data to the grid\n  *\n- * @param data Buffer with Binary Data\n- * @param options {object=} opt_argument The options for the files.\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n- *     to this object.\n- *\n+ * @param {Buffer} data buffer with Binary Data.\n+ * @param {Object} [options] the options for the files.\n+ * @callback {Function} this will be called after this method is executed. The first parameter will contain an Error object if an error occured or null otherwise. The second parameter will contain a reference to this object.\n+ * @return {null}\n+ * @api public\n  */\n Grid.prototype.put = function(data, options, callback) {\n   var self = this;\n@@ -31,7 +35,7 @@\n   options['root'] = options['root'] == null ? this.fsName : options['root'];\n     \n   // Return if we don't have a buffer object as data\n-  if(!(data instanceof Buffer)) return callback(new Error(\"Data object must be a buffer object\"), null);    \n+  if(!(Buffer.isBuffer(data))) return callback(new Error(\"Data object must be a buffer object\"), null);    \n   // Get filename if we are using it\n   var filename = options['filename'];\n   // Create gridstore\n@@ -39,7 +43,7 @@\n   gridStore.open(function(err, gridStore) {\n     if(err) return callback(err, null);\n \n-    gridStore.writeBuffer(data, function(err, result) {\n+    gridStore.write(data, function(err, result) {\n       if(err) return callback(err, null);\n \n       gridStore.close(function(err, result) {\n@@ -53,23 +57,21 @@\n /**\n  * Get binary data to the grid\n  *\n- * @param id ObjectID for file\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n- *     to this object.\n- *\n+ * @param {ObjectID} id ObjectID for file.\n+ * @callback {Function} this will be called after this method is executed. The first parameter will contain an Error object if an error occured or null otherwise. The second parameter will contain a reference to this object.\n+ * @return {null}\n+ * @api public\n  */\n Grid.prototype.get = function(id, callback) {\n   // Validate that we have a valid ObjectId\n-  if(!(id instanceof this.db.bson_serializer.ObjectID)) return callback(new Error(\"Not a valid ObjectID\", null));  \n+  if(!(id instanceof ObjectID)) return callback(new Error(\"Not a valid ObjectID\", null));  \n   // Create gridstore\n   var gridStore = new GridStore(this.db, id, \"r\", {root:this.fsName});\n   gridStore.open(function(err, gridStore) {\n     if(err) return callback(err, null);\n     \n     // Return the data\n-    gridStore.readBuffer(function(err, data) {\n+    gridStore.read(function(err, data) {\n       return callback(err, data)\n     });  \n   })\n@@ -78,19 +80,19 @@\n /**\n  * Delete file from grid\n  *\n- * @param id ObjectID for file\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n",
					"match": false,
					"packageHash": "59d6093424ddaf33fca2e951343aba1d4a62196274c6e7b42086049ee483ce07",
					"size": 3293,
					"sourceHash": "2b8f34d347f7cff63ec859c8eb5e7f43212b384cd72adfcfb7c997e2caf6eae8",
					"status": "content"
				},
				"lib/mongodb/gridfs/gridstore.js": {
					"diff": "--- published/lib/mongodb/gridfs/gridstore.js\n+++ rebuilt/lib/mongodb/gridfs/gridstore.js\n@@ -6,16 +6,13 @@\n  * chunks of split files behind the scenes. More information about GridFS can be\n  * found <a href=\"http://www.mongodb.org/display/DOCS/GridFS\">here</a>.\n  */\n-\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  Chunk = require('./chunk').Chunk,\n+var Chunk = require('./chunk').Chunk,\n   DbCommand = require('../commands/db_command').DbCommand,\n+  ObjectID = require('bson').ObjectID,\n   Buffer = require('buffer').Buffer,\n   fs = require('fs'),\n   util = require('util'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  Stream = require('stream').Stream;\n+  ReadStream = require('./readstream').ReadStream;\n \n var REFERENCE_BY_FILENAME = 0,\n   REFERENCE_BY_ID = 1;\n@@ -23,37 +20,48 @@\n /**\n  * A class representation of a file stored in GridFS.\n  *\n- * @class\n- *\n- * @param db {Db} A database instance to interact with.\n- * @param filename {string} The name for the file.\n- * @param mode {?string} Set the mode for this file. Available modes:\n- *     <ul>\n- *       <li>\"r\" - read only. This is the default mode.</li>\n- *       <li>\"w\" - write in truncate mode. Existing data will be overwriten</li>\n- *       <li>\"w+\" - write in edit mode.</li>\n- *     </ul>\n-\n- * @param options {?object} Optional properties to specify. Recognized keys:\n- *\n- *     <pre><code>\n- *     {\n- *       'root' : , // {string} root collection to use. Defaults to GridStore#DEFAULT_ROOT_COLLECTION\n- *       'chunk_type' : , // {string} mime type of the file. Defaults to GridStore#DEFAULT_CONTENT_TYPE\n- *       'chunk_size' : , // {number} size for the chunk. Defaults to Chunk#DEFAULT_CHUNK_SIZE.\n- *       'metadata' : , // {object} arbitrary data the user wants to store\n- *     }\n- *     </code></pre>\n- *\n- * @see <a href=\"http://www.mongodb.org/display/DOCS/GridFS+Specification\">MongoDB GridFS Specification</a>\n+ * Modes\n+ *  - **\"r\"** - read only. This is the default mode.\n+ *  - **\"w\"** - write in truncate mode. Existing data will be overwriten.\n+ *  - **w+\"** - write in edit mode.\n+ *\n+ * Options\n+ *  - **root** {String}, root collection to use. Defaults to **{GridStore.DEFAULT_ROOT_COLLECTION}**.\n+ *  - **chunk_type** {String}, mime type of the file. Defaults to **{GridStore.DEFAULT_CONTENT_TYPE}**.\n+ *  - **chunk_size** {Number}, size for the chunk. Defaults to **{Chunk.DEFAULT_CHUNK_SIZE}**.\n+ *  - **metadata** {Object}, arbitrary data the user wants to store.\n+ *\n+ * @class Represents the GridStore.\n+ * @param {Db} db A database instance to interact with.\n+ * @param {ObjectID} id an unique ObjectID for this file\n+ * @param {String} [filename] optional a filename for this file, no unique constrain on the field\n+ * @param {String} mode set the mode for this file.\n+ * @param {Object} options optional properties to specify. Recognized keys:\n+ * @return {GridStore}\n  */\n-var GridStore = exports.GridStore = function(db, fileIdObject, mode, options) {\n+function GridStore(db, id, filename, mode, options) {\n+  if(!(this instanceof GridStore)) return new GridStore(db, id, filename, mode, options);\n+\n+  var self = this;\n   this.db = db;  \n+  var _filename = filename;\n+\n+  if(typeof filename == 'string' && typeof mode == 'string') {\n+    _filename = filename;  \n+  } else if(typeof filename == 'string' && typeof mode == 'object' && mode != null) {\n+    var _mode = mode;\n+    mode = filename;\n+    options = _mode;    \n+    _filename = id;\n+  } else if(typeof filename == 'string' && mode == null) {\n+    mode = filename;\n+    _filename = id;\n+  }\n   \n   // set grid referencetype\n-  this.referenceBy = typeof fileIdObject == 'string' ? 0 : 1;\n-  this.filename = fileIdObject;\n-  this.fileId = fileIdObject;\n+  this.referenceBy = typeof id == 'string' ? 0 : 1;\n+  this.filename = _filename;\n+  this.fileId = typeof id == 'string' ? new ObjectID() : id;\n   \n   // Set up the rest\n   this.mode = mode == null ? \"r\" : mode;\n@@ -61,45 +69,55 @@\n   this.root = this.options['root'] == null ? exports.GridStore.DEFAULT_ROOT_COLLECTION : this.options['root'];\n",
					"match": false,
					"packageHash": "3383a6d366d5b054844576a384c9f3dd576d19daa778543cc8560a8c2976f142",
					"size": 42746,
					"sourceHash": "3431d98442b60a3a8125b38249d63dfe6a81338d3bf41d9b86b2e8e71c0c3aac",
					"status": "content"
				},
				"lib/mongodb/index.js": {
					"diff": "--- published/lib/mongodb/index.js\n+++ rebuilt/lib/mongodb/index.js\n@@ -1,13 +1,11 @@\n-\n try {\n-  exports.BSONPure = require('./bson/bson');\n-  exports.BSONNative = require('../../external-libs/bson');\n+  exports.BSONPure = require('bson').BSONPure;\n+  exports.BSONNative = require('bson').BSONNative;\n } catch(err) {\n   // do nothing\n }\n \n-[ 'bson/binary_parser'\n-  , 'commands/base_command'\n+[ 'commands/base_command'\n   , 'commands/db_command'\n   , 'commands/delete_command'\n   , 'commands/get_more_command'\n@@ -20,10 +18,9 @@\n   , 'collection'\n   , 'connection/connection'\n   , 'connection/server'\n-  , 'connection/repl_set_servers'\n+  , 'connection/repl_set'\n   , 'cursor'\n   , 'db'\n-  , 'goog/math/long'\n   , 'gridfs/grid'\n   ,\t'gridfs/chunk'\n   , 'gridfs/gridstore'].forEach(function (path) {\n@@ -31,15 +28,31 @@\n   \tfor (var i in module) {\n   \t\texports[i] = module[i];\n     }\n+\n+    // backwards compat\n+    exports.ReplSetServers = exports.ReplSet;\n+    \n+    // Add BSON Classes\n+    exports.Binary = require('bson').Binary;\n+    exports.Code = require('bson').Code;\n+    exports.DBRef = require('bson').DBRef;\n+    exports.Double = require('bson').Double;\n+    exports.Long = require('bson').Long;\n+    exports.MinKey = require('bson').MinKey;\n+    exports.MaxKey = require('bson').MaxKey;\n+    exports.ObjectID = require('bson').ObjectID;\n+    exports.Symbol = require('bson').Symbol;\n+    exports.Timestamp = require('bson').Timestamp;  \n+    \n+    // Add BSON Parser\n+    exports.BSON = require('bson').BSONPure.BSON;\n });\n \n-// Exports all the classes for the NATIVE JS BSON Parser\n-exports.native = function() {\n+// Exports all the classes for the PURE JS BSON Parser\n+exports.pure = function() {\n   var classes = {};\n   // Map all the classes\n-  [ 'bson/binary_parser'\n-    , '../../external-libs/bson/bson'\n-    , 'commands/base_command'\n+  [ 'commands/base_command'\n     , 'commands/db_command'\n     , 'commands/delete_command'\n     , 'commands/get_more_command'\n@@ -52,7 +65,7 @@\n     , 'collection'\n     , 'connection/connection'\n     , 'connection/server'\n-    , 'connection/repl_set_servers'\n+    , 'connection/repl_set'\n     , 'cursor'\n     , 'db'\n     , 'gridfs/grid'\n@@ -63,17 +76,34 @@\n     \t\tclasses[i] = module[i];\n       }\n   });\n+\n+  // backwards compat\n+  classes.ReplSetServers = exports.ReplSet;\n+\n+  // Add BSON Classes\n+  classes.Binary = require('bson').Binary;\n+  classes.Code = require('bson').Code;\n+  classes.DBRef = require('bson').DBRef;\n+  classes.Double = require('bson').Double;\n+  classes.Long = require('bson').Long;\n+  classes.MinKey = require('bson').MinKey;\n+  classes.MaxKey = require('bson').MaxKey;\n+  classes.ObjectID = require('bson').ObjectID;\n+  classes.Symbol = require('bson').Symbol;\n+  classes.Timestamp = require('bson').Timestamp;\n+\n+  // Add BSON Parser\n+  classes.BSON = require('bson').BSONPure.BSON;\n+\n",
					"match": false,
					"packageHash": "27237bb38f9c69a17e08e052e3493a19a137069f6417e24c6ac19192bbd93eb6",
					"size": 2531,
					"sourceHash": "f177a8277bbee6a3ec1c3e002bda2026d47cad3da7c164edb7f615a0b9c7b360",
					"status": "content"
				},
				"lib/mongodb/responses/mongo_reply.js": {
					"diff": "--- published/lib/mongodb/responses/mongo_reply.js\n+++ rebuilt/lib/mongodb/responses/mongo_reply.js\n@@ -1,6 +1,4 @@\n-var Long = require('../goog/math/long').Long,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+var Long = require('bson').Long;\n \n /**\n   Reply message from mongo db\n@@ -29,7 +27,7 @@\n   this.index = this.index + 4;\n   var high_bits = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n   this.index = this.index + 4;\n-  this.cursorId = new bson.Long(low_bits, high_bits);\n+  this.cursorId = new Long(low_bits, high_bits);\n   // Unpack the starting from\n   this.startingFrom = binary_reply[this.index] | binary_reply[this.index + 1] << 8 | binary_reply[this.index + 2] << 16 | binary_reply[this.index + 3] << 24;\n   this.index = this.index + 4;\n@@ -40,49 +38,55 @@\n \n MongoReply.prototype.parseBody = function(binary_reply, bson, raw, callback) {\n   raw = raw == null ? false : raw;\n-  var docLimitSize = 1024*10;  \n+  // Just set a doc limit for deserializing\n+  var docLimitSize = 1024*20;  \n   \n   // If our message length is very long, let's switch to process.nextTick for messages\n   if(this.messageLength > docLimitSize) {\n-    var batches = 1;\n     var batchSize = this.numberReturned;\n-    var overflow = 0;\n+    this.documents = new Array(this.numberReturned);\n     \n     // Just walk down until we get a positive number >= 1\n     for(var i = 50; i > 0; i--) {\n       if((this.numberReturned/i) >= 1) {\n         batchSize = i;\n-        batches = Math.floor(this.numberReturned/i);\n-        overflow = this.numberReturned%i;\n         break;\n       }      \n     }  \n-    \n+\n     // Actual main creator of the processFunction setting internal state to control the flow\n     var parseFunction = function(_self, _binary_reply, _batchSize, _numberReturned) {\n-      var object_index = 0;\n+      var object_index = 0;      \n       // Internal loop process that will use nextTick to ensure we yield some time\n       var processFunction = function() {\n-        // Iterate over the batch\n-        for(var i = 0; i < _batchSize; i++) {\n-          // Update number of docs parsed\n-          object_index = object_index + 1;\n-          if(object_index <= _numberReturned) {\n-            // Read the size of the bson object    \n-            var bsonObjectSize = _binary_reply[_self.index] | _binary_reply[_self.index + 1] << 8 | _binary_reply[_self.index + 2] << 16 | _binary_reply[_self.index + 3] << 24;\n-            // If we are storing the raw responses to pipe straight through\n-            if(raw) {\n-              // Deserialize the object and add to the documents array\n-              _self.documents.push(binary_reply.slice(_self.index, _self.index + bsonObjectSize));            \n-            } else {\n-              // Deserialize the object and add to the documents array\n-              _self.documents.push(bson.BSON.deserialize(binary_reply.slice(_self.index, _self.index + bsonObjectSize)));\n-            }\n-            // Adjust binary index to point to next block of binary bson data\n-            _self.index = _self.index + bsonObjectSize;\n-          }\n+        // Adjust batchSize if we have less results left than batchsize\n+        if((_numberReturned - object_index) < _batchSize) {\n+          _batchSize = _numberReturned - object_index;\n         }\n         \n+        // If raw just process the entries\n+        if(raw) {\n+          // Iterate over the batch\n+          for(var i = 0; i < _batchSize; i++) {\n+            // Are we done ?\n+            if(object_index <= _numberReturned) {\n+              // Read the size of the bson object    \n+              var bsonObjectSize = _binary_reply[_self.index] | _binary_reply[_self.index + 1] << 8 | _binary_reply[_self.index + 2] << 16 | _binary_reply[_self.index + 3] << 24;\n+              // If we are storing the raw responses to pipe straight through\n+              _self.documents[object_index] = binary_reply.slice(_self.index, _self.index + bsonObjectSize);            \n+              // Adjust binary index to point to next block of binary bson data\n+              _self.index = _self.index + bsonObjectSize;\n+              // Update number of docs parsed\n+              object_index = object_index + 1;\n+            }\n+          }          \n+        } else {\n+          // Parse documents\n+          _self.index = bson.deserializeStream(binary_reply, _self.index, _batchSize, _self.documents, object_index);\n+          // Adjust index\n+          object_index = object_index + _batchSize;          \n+        }\n+\n         // If we hav more documents process NextTick\n         if(object_index < _numberReturned) {\n           process.nextTick(processFunction);          \n",
					"match": false,
					"packageHash": "babd2c666ffa2f4730d912bc5a569436452c465c52dfaf6034f00b1efd876ed4",
					"size": 5977,
					"sourceHash": "2a7cc4f286e5792d1ac34e4e92fc1586b483f6f974fb93afbf63d6d80d64e842",
					"status": "content"
				},
				"package.json": {
					"diff": "--- published/package.json\n+++ rebuilt/package.json\n@@ -1,7 +1,7 @@\n { \"name\" :            \"mongodb\"\n , \"description\" :     \"A node.js driver for MongoDB\"\n , \"keywords\" :        [\"mongodb\", \"mongo\", \"driver\", \"db\"]\n-, \"version\" :         \"0.9.7-2-2\"\n+, \"version\" :         \"0.9.9-8\"\n , \"author\" :          \"Christian Amor Kvalheim <christkv@gmail.com>\"\n , \"contributors\" :  [ \"Aaron Heckmann\",\n                       \"Christoph Pojer\",\n@@ -50,20 +50,31 @@\n                       \"Senmiao Liu\",\n                       \"heroic\",\n                       \"gitfy\",\n-                      \"Andrew Stone\"]\n+                      \"Andrew Stone\",\n+\t\t\t\t\t\t\t\t\t\t\t\"John Le Drew\"]\n \n , \"repository\" :    { \"type\" :  \"git\"\n                     , \"url\" :   \"http://github.com/christkv/node-mongodb-native.git\" }\n , \"bugs\" :          { \"mail\" :  \"node-mongodb-native@googlegroups.com\"\n                     , \"url\" :   \"http://github.com/christkv/node-mongodb-native/issues\" }\n-, \"os\" :            [ \"linux\"\n-                    , \"darwin\"\n-                    , \"freebsd\" ]\n+, \"dependencies\" : {\n+  \"bson\": \"0.0.4\"\n+}                    \n+, \"devDependencies\": {\n+      \"dox\": \"0.2.0\"\n+    , \"uglify-js\": \"1.2.5\"\n+    , \"ejs\": \"0.6.1\"\n+    , \"nodeunit\": \"0.7.3\"\n+    , \"github3\": \">=0.3.0\"\n+\t  , \"markdown\": \"0.3.1\"\n+\t  , \"gleak\": \"0.2.3\"\n+\t  , \"step\": \"0.0.5\"\n+  }\n , \"config\":         { \"native\" : false }                    \n , \"main\":             \"./lib/mongodb/index\"\n , \"directories\" :   { \"lib\" : \"./lib/mongodb\" }\n , \"engines\" :       { \"node\" : \">=0.4.0\" }\n-, \"scripts\": { \"install\" : \"node install.js\" }\n+, \"scripts\": { \"test\" : \"make test_pure\" }\n , \"licenses\" :    [ { \"type\" :  \"Apache License, Version 2.0\"\n                     , \"url\" :   \"http://www.apache.org/licenses/LICENSE-2.0\" } ]\n }\n",
					"match": false,
					"packageHash": "f7f1e68d136fe34c02e038756a878bb734d627dc8150be59e5e1634d7f3418ad",
					"size": 2870,
					"sourceHash": "3e819dd4d5c4f71977047355855ffb75eb246f9cfed7b07d12c2abb32fdb8bc5",
					"status": "content"
				},
				"test/admin_test.js": {
					"match": false,
					"packageHash": "7962189c66475b00b138d5b3257e79c894a1dc7736d6b3211cf7c955c9ec6b53",
					"size": 8149,
					"status": "missing-in-source"
				},
				"test/authentication_test.js": {
					"match": false,
					"packageHash": "453b36c555706849053581fcdabb70b0f3530039fb5e4a36e693f5f2f2587fc6",
					"size": 5735,
					"status": "missing-in-source"
				},
				"test/auxilliary/authentication_test.js": {
					"match": false,
					"packageHash": "fdf5125456a610b766ab4de57f15480b34508fa4f4141512218fa73abb2523e3",
					"size": 6967,
					"status": "missing-in-source"
				},
				"test/auxilliary/repl_set_ssl_test.js": {
					"match": false,
					"packageHash": "15d3122c26b9160a9903750b34f536eb161f0f462febf0a487306887b0a7b741",
					"size": 2097,
					"status": "missing-in-source"
				},
				"test/auxilliary/replicaset_auth_test.js": {
					"match": false,
					"packageHash": "f444a6462c5d47a540af846b4f2ebeda55ce65347bb82c0b42957dcedea7e20f",
					"size": 7121,
					"status": "missing-in-source"
				},
				"test/auxilliary/single_server_kill_reconnect.js": {
					"match": false,
					"packageHash": "dc91ee99e91fd8b6aa9b7d3332bd7778abfd4551fd305f91b23cb60d0a7e0c4f",
					"size": 5743,
					"status": "missing-in-source"
				},
				"test/auxilliary/ssl_test.js": {
					"match": false,
					"packageHash": "6ad88ea94a28f4e97474023193b35a9d3edc812b65a4f63853b69a71f42d0c5a",
					"size": 2460,
					"status": "missing-in-source"
				},
				"test/bson/bson_test.js": {
					"match": false,
					"packageHash": "a6d16efe4d793e509c9c90890b3c6a690c64b61f08636b293536df2195e5acb8",
					"size": 55249,
					"status": "missing-in-source"
				},
				"test/bson/commands_test.js": {
					"match": false,
					"packageHash": "62cc38a555dd13e08b89089c84900c560d6b71e45aa72d78d08322f3ddbb742a",
					"size": 5475,
					"status": "missing-in-source"
				},
				"test/certificates/mycert.pem": {
					"match": false,
					"packageHash": "b58108b5dd45b1d0779b402004309b7c7f2cbd1a525f9611770079754a4f9697",
					"size": 2021,
					"status": "missing-in-source"
				},
				"test/collection_test.js": {
					"match": false,
					"packageHash": "711efbdf203a907a9ae9a1220616c4faf76fad83fcdefb66f4907b4a30c20570",
					"size": 24383,
					"status": "missing-in-source"
				},
				"test/connect_test.js": {
					"match": false,
					"packageHash": "40397edea1be8a667a79b8813d2cd2ad778e4014ee4a91bd5dab6b46d8fb4fc6",
					"size": 3193,
					"status": "missing-in-source"
				},
				"test/connection/connection_pool_test.js": {
					"match": false,
					"packageHash": "c602bd0cdd9640ba1f38c30ea127a0b19ca84010b115c3e35440e2b52c90f51e",
					"size": 2916,
					"status": "missing-in-source"
				},
				"test/connection/message_parser_test.js": {
					"match": false,
					"packageHash": "fb7aa246d527e4c951a5530ff37fe99f17cfedaa93298e66f984790d8925c583",
					"size": 14647,
					"status": "missing-in-source"
				},
				"test/connection_test.js": {
					"match": false,
					"packageHash": "92d2ca4b6a9c6448a4275c4a1c53e9364acc5c3c345c34c0fce65d81d72643c1",
					"size": 5490,
					"status": "missing-in-source"
				},
				"test/cursor_test.js": {
					"match": false,
					"packageHash": "cfe1e5797a98f0a03feba740d0eb9e559ef84e34627ab0e4220c2b552fda849e",
					"size": 35083,
					"status": "missing-in-source"
				},
				"test/custom_pk_test.js": {
					"match": false,
					"packageHash": "3c7b28795a723b131cf14c191e9c248fc23fcfa80fbcee7b1ac5069a914252f7",
					"size": 3305,
					"status": "missing-in-source"
				},
				"test/db_test.js": {
					"match": false,
					"packageHash": "451c15bf06d5073493fa270fa551a4341c7a7e23c2efc76933b56aa35e7be0c2",
					"size": 13452,
					"status": "missing-in-source"
				},
				"test/error_test.js": {
					"match": false,
					"packageHash": "bf853b43fa960c835f7df2183e24f60817299fc5ca0a4a6b0d50db5d003db385",
					"size": 13139,
					"status": "missing-in-source"
				},
				"test/exception_handling_test.js": {
					"match": false,
					"packageHash": "ee89953331f01eb62dbc9005cf5585a6e364050550ff3a4ec6da5ad9bea2dbf2",
					"size": 2928,
					"status": "missing-in-source"
				},
				"test/find_test.js": {
					"match": false,
					"packageHash": "7683082b4b54e147d53bfe80e00052d20ad54353c2bc0936a8550740cbc64f4b",
					"size": 45673,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_file_test.js": {
					"match": false,
					"packageHash": "02d451c7d7629ecabca8764c842246403c99f65c4d475936956086ec206b6331",
					"size": 27001,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_stream_test.js": {
					"match": false,
					"packageHash": "49ba0371fa0ea3bda27c79a2130236b8e0968ef4e4b19c964360f37581e88958",
					"size": 8137,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_test.js": {
					"match": false,
					"packageHash": "00db5e3c9b0c19dc5d4dbd5b75ade9ca6952f7b0aea3d66b7e3089412ce5cc1d",
					"size": 25316,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_test.js": {
					"match": false,
					"packageHash": "2470912bbebb677fdfb8dde81ef636c7a85d69d9fec7d10b6f8c4bafb012fd91",
					"size": 3410,
					"status": "missing-in-source"
				},
				"test/gridstore/iya_logo_final_bw.jpg": {
					"match": false,
					"packageHash": "fee907eb6666fc164dd837943ef9317b17ef74d8fc7580ef9d8d450a4998a4ce",
					"size": 74008,
					"status": "missing-in-source"
				},
				"test/gridstore/test_gs_weird_bug.png": {
					"match": false,
					"packageHash": "357ae8923961baeaf8eb1d434f5a3f8d9ac51ae0dbb2ed412e2687615f757ebc",
					"size": 52184,
					"status": "missing-in-source"
				},
				"test/gridstore/test_gs_working_field_read.pdf": {
					"match": false,
					"packageHash": "c866f44c43c7554e4b25394fa52901ee8c8c176649d4cd7f4dd4e92e8d4bd4f8",
					"size": 130253,
					"status": "missing-in-source"
				},
				"test/index_test.js": {
					"match": false,
					"packageHash": "4d432e12ceccc4ab14b20ed76018b4b791039a7e1763b8702c50f31293fd1caf",
					"size": 11542,
					"status": "missing-in-source"
				},
				"test/insert_test.js": {
					"match": false,
					"packageHash": "165cd5b2958651cb27799dc6acf3a3d3deff7682b939d91693f9cdf7680c4b0a",
					"size": 33391,
					"status": "missing-in-source"
				},
				"test/logging_test.js": {
					"match": false,
					"packageHash": "c7e64910edf14e65008a2c21f11b6c313266f0dd7b2334117bc6441497f217df",
					"size": 2836,
					"status": "missing-in-source"
				},
				"test/manual_tests/replicaset_test.js": {
					"match": false,
					"packageHash": "75aede839390a0e5ba582fa25b8e645f14d8ca04227366e7396ce4f36f82a4b2",
					"size": 1918,
					"status": "missing-in-source"
				},
				"test/manual_tests/server_load.js": {
					"match": false,
					"packageHash": "14238f7fdb84ae2449f161be688b41d3ea0fc2e6e672f2181bfb40e0f38dad74",
					"size": 3712,
					"status": "missing-in-source"
				},
				"test/manual_tests/single_test.js": {
					"match": false,
					"packageHash": "7cd2977bf2a07d47114b203a72da79bdfcfc9d21a557441059a319b13a42c547",
					"size": 1517,
					"status": "missing-in-source"
				},
				"test/manual_tests/test.js": {
					"match": false,
					"packageHash": "31bf1ee11ead9bf0b80c78a48d13780a13052511764e06f952237355237b0268",
					"size": 2338,
					"status": "missing-in-source"
				},
				"test/map_reduce_test.js": {
					"match": false,
					"packageHash": "7954a64fc6fa4f5b75ef45ec7f97f73580a4bc744dd9abee569d281b78054296",
					"size": 13489,
					"status": "missing-in-source"
				},
				"test/multiple_dbs_on_connection_pool_test.js": {
					"match": false,
					"packageHash": "bcf289c091bc1153da6fe35e276522e41fb88f8c075b0ce9c4818fdd723868d8",
					"size": 7559,
					"status": "missing-in-source"
				},
				"test/objectid_test.js": {
					"match": false,
					"packageHash": "1f3b333ccbf01986ffe8eadbdc19fc2503709b32a3e9a2ebdc4336c3d9201f94",
					"size": 7054,
					"status": "missing-in-source"
				},
				"test/raw_test.js": {
					"match": false,
					"packageHash": "d42402f3bf9cfd0c61d68fb34637a8b60aa34fc893c371330fa363d373c8bc4d",
					"size": 15955,
					"status": "missing-in-source"
				},
				"test/regexp_test.js": {
					"match": false,
					"packageHash": "fa917602cd4f8071cd17b595da6e3405acc6728b1f6ca778a666927065f2a653",
					"size": 3958,
					"status": "missing-in-source"
				},
				"test/remove_test.js": {
					"match": false,
					"packageHash": "0de1fd452e0c9cb024b08280026c68737eb68a9f04e387927ad15018de4193fc",
					"size": 2761,
					"status": "missing-in-source"
				},
				"test/replicaset/connect_test.js": {
					"match": false,
					"packageHash": "0ee6b9a58c9a7ee75f7c02c78626a09bff796392fad63f00ca9413dc99ba3a40",
					"size": 12328,
					"status": "missing-in-source"
				},
				"test/replicaset/count_test.js": {
					"match": false,
					"packageHash": "b96251de74b3178d90ca2d7d82fd0b8512ee2dd350ab3a4ffa813ed3e83f571e",
					"size": 6007,
					"status": "missing-in-source"
				},
				"test/replicaset/insert_test.js": {
					"match": false,
					"packageHash": "208150fe11a763c8edf545a0dd2381eea27910e1aab285e8a1d3dad2864e1f4b",
					"size": 20042,
					"status": "missing-in-source"
				},
				"test/replicaset/query_secondaries_test.js": {
					"match": false,
					"packageHash": "c74f904f9a6d661587a152d6c9945a923778584013e2f4010861f218ddf932de",
					"size": 7184,
					"status": "missing-in-source"
				},
				"test/replicaset/tags_test.js": {
					"match": false,
					"packageHash": "df48047a5d799c7fe9236c809802b8e160c964b066c33fbe043ec17f9d3b142b",
					"size": 13136,
					"status": "missing-in-source"
				},
				"test/replicaset/two_server_tests.js": {
					"match": false,
					"packageHash": "7d7d976f7448080a5d2a60dcfbe541ee2c1c5c7b7f9f5935cb33d49e85cf2f4e",
					"size": 4107,
					"status": "missing-in-source"
				},
				"test/streaming_test.js": {
					"match": false,
					"packageHash": "4e785f189c541bb7c6387bfd51d3ef787285bede62a6a772c8b5e45088ebff7b",
					"size": 4330,
					"status": "missing-in-source"
				},
				"test/tools/keyfile.txt": {
					"match": false,
					"packageHash": "58800aef534e5a1004302db4a5caa14a1e2310829f363d4d210a93dabe2e214c",
					"size": 53,
					"status": "missing-in-source"
				},
				"test/tools/replica_set_manager.js": {
					"match": false,
					"packageHash": "9ad8d3b167c11aca45a846a22b69e4c0938eca69e8b9659a8569ed6233519560",
					"size": 20199,
					"status": "missing-in-source"
				},
				"test/tools/server_manager.js": {
					"match": false,
					"packageHash": "764ad772ef25961a614c7addf68dedd541589a1135f407187129303a813313c6",
					"size": 5083,
					"status": "missing-in-source"
				},
				"test/tools/sharding_manager.js": {
					"match": false,
					"packageHash": "09babdb7eb46bbbca96bd44ad83eed0572ff11b23c47c9b87bd59fb3ba4ab454",
					"size": 5544,
					"status": "missing-in-source"
				},
				"test/unicode_test.js": {
					"match": false,
					"packageHash": "1e7b232eed39088cc2765e0d93d034eb9d460d3d70ccb12469d7fb3687ec1d79",
					"size": 6057,
					"status": "missing-in-source"
				},
				"tools/gleak.js": {
					"match": false,
					"packageHash": "987c7d623f4141cee43bf344c46bdcd6f69cc4610228b4f5b520c95cabea0d4e",
					"size": 98,
					"status": "missing-in-source"
				},
				"tools/test_all.js": {
					"match": false,
					"packageHash": "ae04c1001a17a402999b145b26151484b099855b4049969dc7b42e2628a76b35",
					"size": 4010,
					"status": "missing-in-source"
				},
				".travis.yml": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/repl_set.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/cursorstream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/gridfs/readstream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/utils.js": {
					"match": false,
					"status": "missing-in-package"
				}
			},
			"summary": {
				"differentFiles": 29,
				"matchingFiles": 4,
				"missingInPackage": 5,
				"missingInSource": 189,
				"score": 0.01762114537444934,
				"totalFiles": 227
			}
		}
	}
]
