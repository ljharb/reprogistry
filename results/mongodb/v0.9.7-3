[
	{
		"reproduceVersion": "0.0.0-local",
		"timestamp": "2025-12-31T08:01:30.640Z",
		"os": "linux",
		"arch": "x64",
		"strategy": "npm:11.7.0",
		"reproduced": false,
		"attested": false,
		"package": {
			"spec": "mongodb@0.9.7-3",
			"name": "mongodb",
			"version": "0.9.7-3",
			"location": "https://registry.npmjs.org/mongodb/-/mongodb-0.9.7-3.tgz",
			"integrity": "sha512-uX2ymSJxgcdbywVBxLpJLgEO8V54xdI4mJogs1cM5cN59y9sfCyCaTYOJOWXKLqQNKc03PhQXTBerf5Kh/5KvQ==",
			"publishedAt": "2011-12-30T20:05:52.594Z",
			"publishedWith": {
				"node": "v0.6.6",
				"npm": "1.1.0-beta-4"
			}
		},
		"source": {
			"integrity": null,
			"location": "git://github.com/christkv/node-mongodb-native.git",
			"spec": "github:christkv/node-mongodb-native#HEAD"
		},
		"comparisonHash": "8a2a8dcbb729bf3ee9216e490c955701f0ddebe1",
		"diff": {
			"files": {
				".npmignore": {
					"match": false,
					"packageHash": "2ac216020eec6dc966e708bde381cb1f645096ee302a5a385b05ffe87f477fec",
					"size": 22,
					"status": "missing-in-source"
				},
				"HISTORY": {
					"match": false,
					"packageHash": "987bb56ef40da05c5a556ff2c5f1c0b9ea5a34be728df1a5617eb7e989cd279a",
					"size": 14356,
					"status": "missing-in-source"
				},
				"Makefile": {
					"diff": "--- published/Makefile\n+++ rebuilt/Makefile\n@@ -1,12 +1,13 @@\n-\n NODE = node\n-NODEUNIT = deps/nodeunit/bin/nodeunit\n+NPM = npm\n+NODEUNIT = node_modules/nodeunit/bin/nodeunit\n+DOX = node_modules/dox/bin/dox\n name = all\n \n total: build_native\n \n build_native:\n-\t$(MAKE) -C ./external-libs/bson all\n+\t# $(MAKE) -C ./external-libs/bson all\n \n build_native_debug:\n \t$(MAKE) -C ./external-libs/bson all_debug\n@@ -22,11 +23,15 @@\n \n test: build_native\n \t@echo \"\\n == Run All tests minus replicaset tests==\"\n-\t$(NODE) tools/test_all.js --noreplicaset\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot\n+\n+test_pure: build_native\n+\t@echo \"\\n == Run All tests minus replicaset tests==\"\n+\t$(NODE) dev/tools/test_all.js --noreplicaset --boot --noactive\n \n test_junit: build_native\n \t@echo \"\\n == Run All tests minus replicaset tests==\"\n-\t$(NODE) tools/test_all.js --junit --noreplicaset\n+\t$(NODE) dev/tools/test_all.js --junit --noreplicaset\n \n test_nodeunit_pure:\n \t@echo \"\\n == Execute Test Suite using Pure JS BSON Parser == \"\n@@ -49,14 +54,18 @@\n \n test_all: build_native\n \t@echo \"\\n == Run All tests ==\"\n-\t$(NODE) tools/test_all.js\n+\t$(NODE) dev/tools/test_all.js --boot\n \n test_all_junit: build_native\n \t@echo \"\\n == Run All tests ==\"\n-\t$(NODE) tools/test_all.js --junit\n+\t$(NODE) dev/tools/test_all.js --junit --boot\n \n clean:\n \trm ./external-libs/bson/bson.node\n \trm -r ./external-libs/bson/build\n \n+generate_docs:\n+\t$(NODE) dev/tools/build-docs.js\n+\tmake --directory=./docs/sphinx-docs --file=Makefile html\n+\n .PHONY: total\n",
					"match": false,
					"packageHash": "902adf9bb7c0ea140cb6a2a80e351bfbb37a8f4119541986c903d70483db6167",
					"size": 1485,
					"sourceHash": "9a40c06659b42802352fedb651c7e3ac1993e441b74345c4f4015c7835dbd375",
					"status": "content"
				},
				"Readme.md": {
					"diff": "--- published/Readme.md\n+++ rebuilt/Readme.md\n@@ -1,3 +1,8 @@\n+Main Documentation site\n+=======================\n+\n+[Documentation](http://christkv.github.com/node-mongodb-native/)\n+\n Install\n ========\n \n@@ -7,7 +12,7 @@\n     \n That may give you a warning telling you that bugs['web'] should be bugs['url'], it would be safe to ignore it (this has been fixed in the development version) \n \n-To install from the latest from the repository, run::\n+To install the latest from the repository, run::\n \n     npm install path/to/node-mongodb-native\n \n@@ -18,7 +23,7 @@\n Introduction\n ========\n \n-This is a node.js driver for MongoDB. It's a port (or close to a port) of the libary for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n+This is a node.js driver for MongoDB. It's a port (or close to a port) of the library for ruby at http://github.com/mongodb/mongo-ruby-driver/.\n \n A simple example of inserting a document.\n \n@@ -33,7 +38,7 @@\n             // Locate all the entries using find\n             collection.find().toArray(function(err, results) {\n               test.assertEquals(1, results.length);\n-              test.assertTrue(results.a === 2);\n+              test.assertTrue(results[0].a === 2);\n \n               // Let's close the db\n               client.close();\n@@ -48,23 +53,32 @@\n Data types\n ========\n \n-To store and retrieve the non-JSON MongoDb primitives ([ObjectID](http://www.mongodb.org/display/DOCS/Object+IDs), Long, Binary, [Timestamp](http://www.mongodb.org/display/DOCS/Timestamp+data+type), [DBRef](http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef), Code), you have to use one of the types from the bson_serializer.\n+To store and retrieve the non-JSON MongoDb primitives ([ObjectID](http://www.mongodb.org/display/DOCS/Object+IDs), Long, Binary, [Timestamp](http://www.mongodb.org/display/DOCS/Timestamp+data+type), [DBRef](http://www.mongodb.org/display/DOCS/Database+References#DatabaseReferences-DBRef), Code).\n \n-In particular, every document has a unique `_id` which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example:\n+In particular, every document has a unique `_id` which can be almost any type, and by default a 12-byte ObjectID is created. ObjectIDs can be represented as 24-digit hexadecimal strings, but you must convert the string back into an ObjectID before you can use it in the database. For example: \n \n+    // Get the objectID type\n+    var ObjectID = require('mongodb').ObjectID;\n+    \n     var idString = '4e4e1638c85e808431000003';\n-    collection.findOne({_id: new client.bson_serializer.ObjectID(idString)}, console.log)  // ok\n+    collection.findOne({_id: new ObjectID(idString)}, console.log)  // ok\n     collection.findOne({_id: idString}, console.log)  // wrong! callback gets undefined\n \n Here are the constructors the non-Javascript BSON primitive types:\n \n-    var client = new Db(...);\n-    new client.bson_serializer.Long(numberString)\n-    new client.bson_serializer.ObjectID(hexString)\n-    new client.bson_serializer.Timestamp()  // the actual unique number is generated on insert.\n-    new client.bson_serializer.DBRef(collectionName, id, dbName)\n-    new client.bson_serializer.Binary(buffer)  // takes a string or Buffer\n-    new client.bson_serializer.Code(code, [context])\n+    // Fetch the library\n+    var mongo = require('mongodb');\n+    // Create new instances of BSON types\n+    new mongo.Long(numberString)\n+    new mongo.ObjectID(hexString)\n+    new mongo.Timestamp()  // the actual unique number is generated on insert.\n+    new mongo.DBRef(collectionName, id, dbName)\n+    new mongo.Binary(buffer)  // takes a string or Buffer\n+    new mongo.Code(code, [context])\n+    new mongo.Symbol(string)\n+    new mongo.MinKey()\n+    new mongo.MaxKey()\n+    new mongo.Double(number)\t// Force double storage\n \n The C/C++ bson parser/serializer\n --------\n@@ -78,7 +92,7 @@\n                         new Server(\"127.0.0.1\", 27017),\n                         {native_parser:true});\n \n-Since objects created using the C/C++ bson parser are incompatible with a client configured to use the Javascript bson parser and vice versa, you should call constructors using `client.bson_serializer` as described above (don't use `mongodb.BSONNative` and `mongodb.BSONPure` directly).\n+The C++ parser uses the js objects both for serialization and deserialization.\n \n GitHub information\n ========\n",
					"match": false,
					"packageHash": "674db07ab07a537b166c20530ea76ea8b94a474253f2ae59c4cd1394cb44ac0a",
					"size": 16509,
					"sourceHash": "eec96097c0795dae69c2266acd746c7b1ee7a3bec8afe76b1dc19d24418f5075",
					"status": "content"
				},
				"TODO": {
					"match": false,
					"packageHash": "58d3352c9df95be8f2b6d8b3a9c9e9d1297ede9ea28c165ce4df10af4ed43bba",
					"size": 1040,
					"status": "missing-in-source"
				},
				"articles/NodeKOArticle1.md": {
					"match": false,
					"packageHash": "9f5df19d286a5ef6d1b89ab8b846c92d488b58d3d4c4a7872a6e7ce6d7c18ef1",
					"size": 16518,
					"status": "missing-in-source"
				},
				"articles/NodeKOArticle2.md": {
					"match": false,
					"packageHash": "2ce49396151d5e3bdcbcd1c752f4b884c45a167b8d8f5659a610bba27db91556",
					"size": 11106,
					"status": "missing-in-source"
				},
				"deps/gleak/.npmignore": {
					"match": false,
					"packageHash": "40bdd3939495d4595182f964a238fb8808cffe3d7b9f51c98fe10929f4009c42",
					"size": 32,
					"status": "missing-in-source"
				},
				"deps/gleak/History.md": {
					"match": false,
					"packageHash": "08987e79cc38f2e3cb3d8d52f93d9aaa943c9d4e6a84a46d41c1fc21a32dedb1",
					"size": 678,
					"status": "missing-in-source"
				},
				"deps/gleak/Makefile": {
					"match": false,
					"packageHash": "cc7dd0edba33a120a5958a144be8db4fbca198d95dfca0d65067e31f62e5f1e2",
					"size": 92,
					"status": "missing-in-source"
				},
				"deps/gleak/README.md": {
					"match": false,
					"packageHash": "8164a2545addd67dc9aa7dcfe8bca5aae0e7a349385641b315c4a4fb56dd5ff0",
					"size": 3700,
					"status": "missing-in-source"
				},
				"deps/gleak/index.js": {
					"match": false,
					"packageHash": "89fa6d2a3902f52c2a46347ddf5c18b6b3113921baf6ffbc6630dd68aa082fd7",
					"size": 3204,
					"status": "missing-in-source"
				},
				"deps/gleak/package.json": {
					"match": false,
					"packageHash": "11667342c7fbbf3a91d19b86704fffa3f29d1777c5ea7fe704e05b65e6a863c4",
					"size": 471,
					"status": "missing-in-source"
				},
				"deps/gleak/test/index.js": {
					"match": false,
					"packageHash": "654e16e8db2684b1761ec263da4b2b1e7aaa5bc15751bd05a56eccc05ca9c9cd",
					"size": 5310,
					"status": "missing-in-source"
				},
				"deps/nodeunit/.npmignore": {
					"match": false,
					"packageHash": "c22365d4b129983208d56dfa095d9af9e674c734939c8a7c62489c082db31480",
					"size": 36,
					"status": "missing-in-source"
				},
				"deps/nodeunit/CONTRIBUTORS.md": {
					"match": false,
					"packageHash": "dc1a5a144eaa2fdbb9b153779d58af125a5b65f3576d29dba2b5b8e8f3d8f079",
					"size": 1548,
					"status": "missing-in-source"
				},
				"deps/nodeunit/LICENSE": {
					"match": false,
					"packageHash": "b04b9e208e566fa898c7429e4dd5b45ba3ba2f7391e5c009cf63c53d580fa9b4",
					"size": 1058,
					"status": "missing-in-source"
				},
				"deps/nodeunit/Makefile": {
					"match": false,
					"packageHash": "5dbc2ad6b4398c2ecc6f5139795025770f7d29a1875ab3f3998753c42e4f3e81",
					"size": 5154,
					"status": "missing-in-source"
				},
				"deps/nodeunit/README.md": {
					"match": false,
					"packageHash": "751b4e9acf46b4e53769a8504f275b25e671ffd0a412b6d1c90d65955ddfee55",
					"size": 14941,
					"status": "missing-in-source"
				},
				"deps/nodeunit/bin/nodeunit": {
					"match": false,
					"packageHash": "8958587df20d749862b6f786b7b0f610c9446a07fddb6658f4a7c759934e3e8d",
					"size": 3678,
					"status": "missing-in-source"
				},
				"deps/nodeunit/bin/nodeunit.json": {
					"match": false,
					"packageHash": "1751ba7b7ad826d3162da359fccef5d1352d1d9a8b710e6753139aa142b1d255",
					"size": 274,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/async.js": {
					"match": false,
					"packageHash": "6e67d1380c0ea1c5e876cdeb9b9f39ea8dba07d426d1377f3a1be62d57e0125c",
					"size": 18641,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/ejs.js": {
					"match": false,
					"packageHash": "5f87b6d3977341abd9b2500c496fb2a8de23fb59e6a6301426021e504c789eeb",
					"size": 2597,
					"status": "missing-in-source"
				},
				"deps/nodeunit/deps/json2.js": {
					"match": false,
					"packageHash": "e6774f41a11016c803c602fa7e03bf03afb8e67217e2b1827cceb2fe5e1ddb52",
					"size": 17415,
					"status": "missing-in-source"
				},
				"deps/nodeunit/doc/nodeunit.md": {
					"match": false,
					"packageHash": "c57518a39297ef6ba91ee72ceeaba7ad5d9807f1cdb02d6449a266904a8305cf",
					"size": 1637,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/nodeunit.js": {
					"match": false,
					"packageHash": "4e716bf47c9c35a79dbe0714f1f650d96d0b4def80a6659a1f5fadfdf4fa7c4a",
					"size": 54817,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/suite1.js": {
					"match": false,
					"packageHash": "a0cf930d2666256e73299191618d5a58f005cc1bbf7733a7fdeb2aa169bd11d3",
					"size": 319,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/suite2.js": {
					"match": false,
					"packageHash": "cca3850beb4e4ea16bffbd6769191e384acf9e88de80a44e49a6266f89bc50ce",
					"size": 396,
					"status": "missing-in-source"
				},
				"deps/nodeunit/examples/browser/test.html": {
					"match": false,
					"packageHash": "921d98372885e8bf4d2918e6ebc1ecf236a3d981eddde4bb820ef46194db9723",
					"size": 311,
					"status": "missing-in-source"
				},
				"deps/nodeunit/img/example_fail.png": {
					"match": false,
					"packageHash": "5205647e4ddd31179fe5e1e4ddba8c496a76c2e821b2629b74772b62287381ab",
					"size": 38642,
					"status": "missing-in-source"
				},
				"deps/nodeunit/img/example_pass.png": {
					"match": false,
					"packageHash": "cd2164286b1c9fcb94cc71856dcdd258d285df39579e1445a1c16dd5d2d3f1c4",
					"size": 14133,
					"status": "missing-in-source"
				},
				"deps/nodeunit/index.js": {
					"match": false,
					"packageHash": "4a2146a5e00131523679c7d60430d41c67ae0a4ddacd52e51b2f0ce0ae517a5c",
					"size": 166,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/assert.js": {
					"match": false,
					"packageHash": "6d4a71e840792d8bb9f55bed413b454c20286936a06216e1e7372b1e6c175656",
					"size": 10207,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/core.js": {
					"match": false,
					"packageHash": "3122e6f06a3a1402e35fd451e137fea641bd8b99769de57494a79ccc3d977bd1",
					"size": 5928,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/nodeunit.js": {
					"match": false,
					"packageHash": "f7cf8583a6fee24b8b7fd2def10e9b54dc6b2f31dfe4a0085212de523400a05b",
					"size": 1935,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/browser.js": {
					"match": false,
					"packageHash": "2f128c6ebaf7346c9c0f18f934bdc7e001ccc07972ee5733dc194e20202ed78c",
					"size": 3891,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/default.js": {
					"match": false,
					"packageHash": "1f31dddcbf2d3ded9e749d8e83d7c671623fcc97a89d4bdd98fc9e8d66fb6058",
					"size": 4195,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/html.js": {
					"match": false,
					"packageHash": "e358a3640d6292f26b513cca60974d2af8990ea9e9367987ff40378db6d6fdc0",
					"size": 3512,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/index.js": {
					"match": false,
					"packageHash": "156ab9a0ba9aa7cc595e31c0604c6c97a8d8fddcfe984264d61c93804f9c37aa",
					"size": 331,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/junit.js": {
					"match": false,
					"packageHash": "9aba5ae9fb5d849ded37bbcfa265e8ea32e58e86a90db76ca8260cd3dd703dae",
					"size": 5908,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/minimal.js": {
					"match": false,
					"packageHash": "542cfb84da89893ad1d5cd521179e59d5dbbaefeeab30226f438601759c2d77e",
					"size": 3434,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/reporters/skip_passed.js": {
					"match": false,
					"packageHash": "1ffdf5f47d712048de1b56a701c2a57d1525d98f74ab7161e18da5afa3da37b9",
					"size": 3296,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/track.js": {
					"match": false,
					"packageHash": "e02fba7af50d190aaa45678abf04213e3c90b23ffd1633b367233d8abd2cea84",
					"size": 1222,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/types.js": {
					"match": false,
					"packageHash": "3adae1cf4af93b67cbe70604f63822fe0b12e41cacde8765e19f8c22d2f2ca94",
					"size": 5084,
					"status": "missing-in-source"
				},
				"deps/nodeunit/lib/utils.js": {
					"match": false,
					"packageHash": "bc4418a67c3057e5990c8bb2920b60abe45b41a066f19cb2771668b8fdd9f640",
					"size": 6056,
					"status": "missing-in-source"
				},
				"deps/nodeunit/man1/nodeunit.1": {
					"match": false,
					"packageHash": "cd9bac2075ba9fda7b0c26d96f64a1af629eb438a897c40d23f303a43224cf8c",
					"size": 1957,
					"status": "missing-in-source"
				},
				"deps/nodeunit/nodelint.cfg": {
					"match": false,
					"packageHash": "2d97676148773c16d39eb5997004dc88e2746e648dc1023d3080cdbabca5a328",
					"size": 52,
					"status": "missing-in-source"
				},
				"deps/nodeunit/package.json": {
					"match": false,
					"packageHash": "a44a63a8dc5171bfcbebc3da4faff0a78b7cfce0d21064e127638db9e326d8d7",
					"size": 1418,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/junit.xml.ejs": {
					"match": false,
					"packageHash": "84500bc9c9138c39faa25d81744c17dc9f2f7c081027a025e21fc988d9bef71f",
					"size": 725,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/license.js": {
					"match": false,
					"packageHash": "c2d81ff8586c4f0e55b222efbdad15072f0960409d49144c39f55d99f96e19c4",
					"size": 235,
					"status": "missing-in-source"
				},
				"deps/nodeunit/share/nodeunit.css": {
					"match": false,
					"packageHash": "e1cff7d7b6903ac5358471ad8a04e400402a20dcf7efd4a9b7a71ca19ebba2fd",
					"size": 1358,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/coffee/mock_coffee_module.coffee": {
					"match": false,
					"packageHash": "a8ba7102b30806f8323e4658688507287363b6972f8a04bd344672b995becdd4",
					"size": 64,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/dir/mock_module3.js": {
					"match": false,
					"packageHash": "59f360a9d2bf7964fa63ce58e013da089e565c87202c35a3492cd9cc330418af",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/dir/mock_module4.js": {
					"match": false,
					"packageHash": "8d96c8ee9c1881d9fced4856817ec92a74c5e7f891057d9bae43a97439cede16",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/mock_module1.js": {
					"match": false,
					"packageHash": "d0e39b00e441f76ba906a4df1311edcf7ac2063845c34d949b185c073babdb53",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/mock_module2.js": {
					"match": false,
					"packageHash": "f79ae9762607ab929a7cef72f47f8f85fd176e0b03479a95a0b64093c5aff5db",
					"size": 31,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode1.js": {
					"match": false,
					"packageHash": "59e98924953900c233df9fd658fa5eb81587bd00ff0a461faf0b5c432ccc89f4",
					"size": 55,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode2.js": {
					"match": false,
					"packageHash": "58f4d19c89bf584aa58f59205156e28b0ec8da8f40e7208169c7140cf2189e79",
					"size": 57,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/fixtures/raw_jscode3.js": {
					"match": false,
					"packageHash": "f2897e8d4ecbea9793fc07bf4e417c2675aa4e96e544ebdc5c80385dc06111c3",
					"size": 15,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-base.js": {
					"match": false,
					"packageHash": "37c7ea3406de4c5a3b965e72f2bb92226f402cafb0b7226eb53f424a2190762d",
					"size": 6250,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-failing-callbacks.js": {
					"match": false,
					"packageHash": "15bd6b031fa69025b97695793801d59448f048a5615ff8b3330f6b173715cc5e",
					"size": 3614,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-httputil.js": {
					"match": false,
					"packageHash": "cd3cc667209dd77721a210e1588fc33ccf754229eb27c3f5ba4c6e1fcbcb9b08",
					"size": 1751,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runfiles.js": {
					"match": false,
					"packageHash": "a2397734764aefb45243e4b07a9ccaf1fdf7bc2e526845e397eabb7763c085bf",
					"size": 6505,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runmodule.js": {
					"match": false,
					"packageHash": "c3a0373d1b92fa3fb886be2f79c38dac80632bde6f7053bcba38a4818c6edd86",
					"size": 4107,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-runtest.js": {
					"match": false,
					"packageHash": "7377ea3e92c86979a5b1ee08c1cfa1cbae1b60a9cfbfca55904875e66570d631",
					"size": 1570,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-sandbox.js": {
					"match": false,
					"packageHash": "1d20ecc18dff9c49b1f2554b7705bd15edd6743f8f8d0ab8bd0848d321df1fad",
					"size": 1015,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test-testcase.js": {
					"match": false,
					"packageHash": "3776d1b1636a89c29233276a402b05df3026c70c9c3aefbc64b24f472039bafe",
					"size": 6404,
					"status": "missing-in-source"
				},
				"deps/nodeunit/test/test.html": {
					"match": false,
					"packageHash": "3e8cdfb33c5a73ceeeb9b4ed08829b89924711982be48eed40ed2818166e0007",
					"size": 808,
					"status": "missing-in-source"
				},
				"deps/step/README.markdown": {
					"match": false,
					"packageHash": "b023a8cd9a4a6ac09d3f20c6092f89e72f8c76e528bf45f0d8ac7a15f788f03e",
					"size": 3042,
					"status": "missing-in-source"
				},
				"deps/step/lib/step.js": {
					"match": false,
					"packageHash": "3dfd25f5bc117cc3acda7859ab4ddbd02251fdae55db565050b24d3260f7e198",
					"size": 4542,
					"status": "missing-in-source"
				},
				"deps/step/package.json": {
					"match": false,
					"packageHash": "d9400fc4eb190667a8d9f466e2986590b1badd1f2b22b6a35c28067daee4d8a7",
					"size": 376,
					"status": "missing-in-source"
				},
				"deps/step/test/callbackTest.js": {
					"match": false,
					"packageHash": "bd04258a35b05d769e7b5973ee1e161cf2ebf2102e2be460267e076e6d8389cf",
					"size": 621,
					"status": "missing-in-source"
				},
				"deps/step/test/errorTest.js": {
					"match": false,
					"packageHash": "eb689f8efdfcdec28f2e349fb4e0cbe5fc178ca178f5ec07a13075a812ead2a5",
					"size": 532,
					"status": "missing-in-source"
				},
				"deps/step/test/fnTest.js": {
					"match": false,
					"packageHash": "02d50088fc420a1bc2010dff0ae93d76593a7820748339cf707dd9d5e9334772",
					"size": 429,
					"status": "missing-in-source"
				},
				"deps/step/test/groupTest.js": {
					"match": false,
					"packageHash": "6c624cb9a4e7f11b187bba73bd87c91f1105e69d0943ddfa8313f2b4734a4bbf",
					"size": 2527,
					"status": "missing-in-source"
				},
				"deps/step/test/helper.js": {
					"match": false,
					"packageHash": "0746c0b3441c3c18e8579dd4fbca2f54384340348fe36885e0a9e5590f2f5097",
					"size": 533,
					"status": "missing-in-source"
				},
				"deps/step/test/parallelTest.js": {
					"match": false,
					"packageHash": "96362563d20b138a981099bf69c488502a5196c7657f7a9b83ae18bcc93d031c",
					"size": 1375,
					"status": "missing-in-source"
				},
				"docs/README.md": {
					"match": false,
					"packageHash": "f1c30be7bf94446e980d97f9afbf16a938f751c51bfbae9a1461a8b8f19fc7ad",
					"size": 1035,
					"status": "missing-in-source"
				},
				"docs/collections.md": {
					"match": false,
					"packageHash": "cffac28ae42cf67bbc640a1d6f43595c12ad70a07460ad201555cc97e60886e7",
					"size": 4069,
					"status": "missing-in-source"
				},
				"docs/database.md": {
					"match": false,
					"packageHash": "c8354b6cebb76f8074b14621a42815ae840de9aba7aba321185d999d24897d60",
					"size": 5261,
					"status": "missing-in-source"
				},
				"docs/gridfs.md": {
					"match": false,
					"packageHash": "1225c0a35a5856adf1614e30fbccfcb075a69b6c9d6e0189b791b89e84959caf",
					"size": 4470,
					"status": "missing-in-source"
				},
				"docs/indexes.md": {
					"match": false,
					"packageHash": "c1ed053f25e5f25b352d117dba0e5858e6df819db67d78318181ef00d4530378",
					"size": 3122,
					"status": "missing-in-source"
				},
				"docs/insert.md": {
					"match": false,
					"packageHash": "1184ac2e35a9978dff68985d61992357330f34990ca6c43883ea91cf096d1d64",
					"size": 5371,
					"status": "missing-in-source"
				},
				"docs/queries.md": {
					"match": false,
					"packageHash": "09285f8528f76e1fcbcfa67c63259c29b587dddf5a7abac8b983eb8230832812",
					"size": 8293,
					"status": "missing-in-source"
				},
				"docs/replicaset.md": {
					"match": false,
					"packageHash": "8f09555c992333a42a24816302b8b52e3139995161b1c340e38448ee0e154753",
					"size": 1952,
					"status": "missing-in-source"
				},
				"examples/admin.js": {
					"match": false,
					"packageHash": "2d440b977416005dea48cda521cdaf2478d443068ef008756dc48719b61203db",
					"size": 2193,
					"status": "missing-in-source"
				},
				"examples/blog.js": {
					"match": false,
					"packageHash": "137c4147a59743c8272baf0243e3f6ba2201ce20e97b3008d9afab263f6b916c",
					"size": 5940,
					"status": "missing-in-source"
				},
				"examples/capped.js": {
					"match": false,
					"packageHash": "53a9f9f12bb62e19ad8d4b9e5d3f79934f0bf1f57f582d3c02ab9a00f9efe532",
					"size": 1271,
					"status": "missing-in-source"
				},
				"examples/cursor.js": {
					"match": false,
					"packageHash": "efbda1522289effdb078a217545b364d84ea6a86abed04e1758d7f00e5362345",
					"size": 2560,
					"status": "missing-in-source"
				},
				"examples/gridfs.js": {
					"match": false,
					"packageHash": "3a0146ba12de6dc8bf07de5fd2dbbe5523821f2e67b61cbbf6658db6735369e4",
					"size": 5894,
					"status": "missing-in-source"
				},
				"examples/index.js": {
					"match": false,
					"packageHash": "c3ad958d2e3f17e9893474165393edb322f01afaebfa158244fed62a456baf3a",
					"size": 2368,
					"status": "missing-in-source"
				},
				"examples/info.js": {
					"match": false,
					"packageHash": "42fe6b013315fe972811cb4b5cd54f5f17820914d39de7b139c5034612c83209",
					"size": 1657,
					"status": "missing-in-source"
				},
				"examples/oplog.js": {
					"match": false,
					"packageHash": "cea2a2cd1a25e83eeb88823b5579725ef6ccaa8629c229ab9847c0fc426c8adc",
					"size": 3270,
					"status": "missing-in-source"
				},
				"examples/queries.js": {
					"match": false,
					"packageHash": "c39e274f9548374d0cf47baafd622d443b97429bd171ec20f22a17f77cfd4d36",
					"size": 5289,
					"status": "missing-in-source"
				},
				"examples/replSetServersQueries.js": {
					"match": false,
					"packageHash": "e4107de45e3049b4c9ad4d1cceb3c38d6c0bafc824621ef866678852b3318718",
					"size": 5741,
					"status": "missing-in-source"
				},
				"examples/replSetServersSimple.js": {
					"match": false,
					"packageHash": "12bac6387da463cdaeac2a101ea34ac7286bbccf01309230ae8df319e9177e45",
					"size": 2252,
					"status": "missing-in-source"
				},
				"examples/simple.js": {
					"match": false,
					"packageHash": "3194e95088f4cb9a12fd97b6ad375ab1874a1f52c33faea7eb9987d0095238b1",
					"size": 1686,
					"status": "missing-in-source"
				},
				"examples/strict.js": {
					"match": false,
					"packageHash": "73070e7cace51498c1dd0aedfadb13ac82cb9bfc5a13439c4c17890f021ba817",
					"size": 1445,
					"status": "missing-in-source"
				},
				"examples/types.js": {
					"match": false,
					"packageHash": "d3f0ee5438e7d555c9a085858d587f555f9d8be40dc808f1d64eac6c057e20d8",
					"size": 1678,
					"status": "missing-in-source"
				},
				"examples/url.js": {
					"match": false,
					"packageHash": "027764e2135279cec233b184fb6c5e8d432616d450e33303482994ecda85b89b",
					"size": 394,
					"status": "missing-in-source"
				},
				"external-libs/bson/Makefile": {
					"diff": "--- published/external-libs/bson/Makefile\n+++ rebuilt/external-libs/bson/Makefile\n@@ -6,43 +6,38 @@\n \trm -rf build .lock-wscript bson.node\n \tnode-waf configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t# @$(NODE) --expose-gc test/test_bson.js\n-\t# @$(NODE) --expose-gc test/test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n \t# @$(NODE) --expose-gc test/test_stackless_bson.js\n-\t@$(NODE) --expose-gc test/test_shared_objects.js\n \n all_debug:\n \trm -rf build .lock-wscript bson.node\n \tnode-waf --debug configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t# @$(NODE) --expose-gc test/test_bson.js\n-\t# @$(NODE) --expose-gc test/test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n \t# @$(NODE) --expose-gc test/test_stackless_bson.js\n-\t@$(NODE) --expose-gc test/test_shared_objects.js\n \n test:\n-\t# @$(NODE) --expose-gc test/test_bson.js\n-\t# @$(NODE) --expose-gc test/test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n \t# @$(NODE) --expose-gc test/test_stackless_bson.js\n-\t@$(NODE) --expose-gc test/test_shared_objects.js\n \n clang:\n \trm -rf build .lock-wscript bson.node\n \tCXX=clang node-waf configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t# @$(NODE) --expose-gc test/test_bson.js\n-\t# @$(NODE) --expose-gc test/test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n \t# @$(NODE) --expose-gc test/test_stackless_bson.js\n-\t@$(NODE) --expose-gc test/test_shared_objects.js\n \n clang_debug:\n \trm -rf build .lock-wscript bson.node\n \tCXX=clang node-waf --debug configure build\n \tcp -R ./build/Release/bson.node . || true\n-\t# @$(NODE) --expose-gc test/test_bson.js\n-\t# @$(NODE) --expose-gc test/test_full_bson.js\n+\t@$(NODE) --expose-gc test/test_bson.js\n+\t@$(NODE) --expose-gc test/test_full_bson.js\n \t# @$(NODE) --expose-gc test/test_stackless_bson.js\n-\t@$(NODE) --expose-gc test/test_shared_objects.js\n \n clean:\n \trm -rf build .lock-wscript bson.node\n",
					"match": false,
					"packageHash": "6a7a6f90fdb19d6818f35b1bd3bbf069fb0c7c39cd6fe6f679442ffec1b549f3",
					"size": 1556,
					"sourceHash": "739cf490555ff92e959059d59f3ab3116a6159879968a0b0b57c5ac838e62163",
					"status": "content"
				},
				"external-libs/bson/bson.cc": {
					"diff": "--- published/external-libs/bson/bson.cc\n+++ rebuilt/external-libs/bson/bson.cc\n@@ -1,7 +1,18 @@\n #include <assert.h>\n #include <string.h>\n #include <stdlib.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic push\n+#pragma clang diagnostic ignored \"-Wunused-parameter\"\n+#endif\n+\n #include <v8.h>\n+\n+#ifdef __clang__\n+#pragma clang diagnostic pop\n+#endif\n+\n #include <node.h>\n #include <node_version.h>\n #include <node_buffer.h>\n@@ -41,6 +52,12 @@\n const int32_t BSON_INT32_MAX = (int32_t)2147483647L;\n const int32_t BSON_INT32_MIN = (int32_t)(-1) * 2147483648L;\n \n+const int64_t BSON_INT64_MAX = ((int64_t)1 << 63) - 1;\n+const int64_t BSON_INT64_MIN = (int64_t)-1 << 63;\n+\n+const int64_t JS_INT_MAX = (int64_t)1 << 53;\n+const int64_t JS_INT_MIN = (int64_t)-1 << 53;\n+\n static Handle<Value> VException(const char *msg) {\n     HandleScope scope;\n     return ThrowException(Exception::Error(String::New(msg)));\n@@ -64,11 +81,12 @@\n   NODE_SET_PROTOTYPE_METHOD(constructor_template, \"deserialize\", BSONDeserialize);\n   NODE_SET_PROTOTYPE_METHOD(constructor_template, \"deserializeStream\", BSONDeserializeStream);\n \n-  // // Experimental\n-  //   NODE_SET_METHOD(constructor_template->GetFunction(), \"calculateObjectSize2\", CalculateObjectSize2);\n-  //   NODE_SET_METHOD(constructor_template->GetFunction(), \"serialize2\", BSONSerialize2);  \n+  // Experimental\n+  // NODE_SET_PROTOTYPE_METHOD(constructor_template, \"calculateObjectSize2\", CalculateObjectSize2);\n+  // NODE_SET_PROTOTYPE_METHOD(constructor_template, \"serialize2\", BSONSerialize2);\n+  // NODE_SET_METHOD(constructor_template->GetFunction(), \"serialize2\", BSONSerialize2);  \n \n-  target->Set(String::NewSymbol(\"BSON\"), constructor_template->GetFunction());\n+  target->ForceSet(String::NewSymbol(\"BSON\"), constructor_template->GetFunction());\n }\n \n // Create a new instance of BSON and assing it the existing context\n@@ -194,7 +212,7 @@\n   char *error_str = (char *)malloc(256 * sizeof(char));\n   // Decode the key\n   ssize_t len = DecodeBytes(key, BINARY);\n-  ssize_t written = DecodeWrite(key_str, len, key, BINARY);\n+  DecodeWrite(key_str, len, key, BINARY);\n   *(key_str + key->Utf8Length()) = '\\0';\n   // Check if we have a valid key\n   if(key->Utf8Length() > 0 && *(key_str) == '$') {\n@@ -334,468 +352,160 @@\n   return value;\n }\n \n-// //------------------------------------------------------------------------------------------------\n-// //\n-// // Experimental\n-// //\n-// //------------------------------------------------------------------------------------------------\n-// Handle<Value> BSON::CalculateObjectSize2(const Arguments &args) {\n-//   HandleScope scope;\n-//   // Ensure we have a valid object\n-//   if(args.Length() == 1 && !args[0]->IsObject()) return VException(\"One argument required - [object]\");\n-//   if(args.Length() > 1) return VException(\"One argument required - [object]\");\n-//   vector< Local<Value> > V;\n-//   V.push_back(args[0]);\n-//   \n-//   // Calculate size of the object\n-//   uint32_t object_size = BSON::calculate_object_size2(args[0]);\n-//   // Return the object size\n-//   return scope.Close(Uint32::New(object_size));\n-// }\n-// \n-// uint32_t BSON::calculate_object_size2(Handle<Value> value) {\n-//   // Final object size\n-//   uint32_t object_size = (4 + 1);\n-//   uint32_t stackIndex = 0;\n-//   // Controls the flow\n-//   bool done = false;\n-//   bool finished = false;\n-//   bool isObject = false;\n-// \n-//   // Define a local vector that keeps the stack\n-//   // vector<vector<Local<Value> > > stack;// = new vector<vector<Local<Value> > >(0);\n-//   \n-//   // My own stack max of 1024 objects deep\n-//   Local<Object> *stack[1024];\n-//   \n-//   // Current object we are processing\n-//   Local<Object> currentObject = value->ToObject();\n",
					"match": false,
					"packageHash": "9728b12dfcf421506d5bafe6dc5d304ddc32c93c72d7116d5b45dccdde727b31",
					"size": 100473,
					"sourceHash": "3bbf33024abd0bbd372cf8d602e8323759fdd165640ea5b8822dd862932a142e",
					"status": "content"
				},
				"external-libs/bson/bson.h": {
					"diff": "--- published/external-libs/bson/bson.h\n+++ rebuilt/external-libs/bson/bson.h\n@@ -25,15 +25,15 @@\n     static Handle<Value> SerializeWithBufferAndIndex(const Arguments &args);\n \n   \t// Experimental\n-    // static Handle<Value> CalculateObjectSize2(const Arguments &args);\n-    // static Handle<Value> BSONSerialize2(const Arguments &args);\n+    static Handle<Value> CalculateObjectSize2(const Arguments &args);\n+    static Handle<Value> BSONSerialize2(const Arguments &args);\n \n     // Constructor used for creating new BSON objects from C++\n     static Persistent<FunctionTemplate> constructor_template;\n \n   private:\n     static Handle<Value> New(const Arguments &args);\n-    static Handle<Value> deserialize(BSON *bson, char *data, uint32_t startIndex, bool is_array_item);\n+    static Handle<Value> deserialize(BSON *bson, char *data, uint32_t dataLength, uint32_t startIndex, bool is_array_item);\n     static uint32_t serialize(BSON *bson, char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, bool check_key, bool serializeFunctions);\n \n     static char* extract_string(char *data, uint32_t offset);\n@@ -98,8 +98,8 @@\n     static Handle<Value> decodeDBref(BSON *bson, Local<Value> ref, Local<Value> oid, Local<Value> db);    \n \n \t\t// Experimental\n-    // static uint32_t calculate_object_size2(Handle<Value> object);\n-    // static uint32_t serialize2(char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, uint32_t object_size, bool check_key);    \n+    static uint32_t calculate_object_size2(Handle<Value> object);    \n+    static uint32_t serialize2(char *serialized_object, uint32_t index, Handle<Value> name, Handle<Value> value, uint32_t object_size, bool check_key);    \n };\n \n #endif  // BSON_H_\n",
					"match": false,
					"packageHash": "62c422ea7cab3fe9fb1ed60019b6940e13c5c6bae9bd7cf1aa23685e267a95e4",
					"size": 4286,
					"sourceHash": "3ac7c29a803a949feb401a361cff6d5b953f45756933d3d0815dc4833beed2ae",
					"status": "content"
				},
				"external-libs/bson/index.js": {
					"diff": "--- published/external-libs/bson/index.js\n+++ rebuilt/external-libs/bson/index.js\n@@ -1,6 +1,6 @@\n var bson = require('./bson');\n exports.BSON = bson.BSON;\n-exports.Long = require('../../lib/mongodb/goog/math/long').Long;\n+exports.Long = require('../../lib/mongodb/bson/long').Long;\n exports.ObjectID = require('../../lib/mongodb/bson/objectid').ObjectID;\n exports.DBRef = require('../../lib/mongodb/bson/db_ref').DBRef;\n exports.Code = require('../../lib/mongodb/bson/code').Code;\n",
					"match": false,
					"packageHash": "19e85e1a0f1d24d40fe8c13675ef4c1794681eaa32c7fe8a8a4451b753e1ef05",
					"size": 1063,
					"sourceHash": "a2d8bb3379e3d4c0c8a825b410e0b924f98b62470fbdfbfb1a13b49e7fff5897",
					"status": "content"
				},
				"external-libs/bson/test/test_bson.js": {
					"match": false,
					"packageHash": "495acbd01276f25e16d84949afc4bcc0ba9b6946a286aabaf39e00e58a1904fc",
					"size": 18906,
					"status": "missing-in-source"
				},
				"external-libs/bson/test/test_full_bson.js": {
					"match": false,
					"packageHash": "a595dc05fa090fcf1720711a8b450cad8e796fe5d59920a6a1c1973c651692e4",
					"size": 7413,
					"status": "missing-in-source"
				},
				"external-libs/bson/test/test_shared_objects.js": {
					"match": false,
					"packageHash": "90b70369113bbcb43a9a82ea8eaf4723d6c906160da26cb90d84421a90ca9a07",
					"size": 11325,
					"status": "missing-in-source"
				},
				"external-libs/bson/test/test_stackless_bson.js": {
					"match": false,
					"packageHash": "13d367718d0aa2a1d03a87e811256107ad6fc3b9d7bfa93ea1edcacfff85089c",
					"size": 3490,
					"status": "missing-in-source"
				},
				"install.js": {
					"diff": "--- published/install.js\n+++ rebuilt/install.js\n@@ -10,7 +10,6 @@\n // Check if we want to build the native code\n var build_native = process.env['npm_package_config_native'] != null ? process.env['npm_package_config_native'] : 'false';\n build_native = build_native == 'true' ? true : false;\n-\n // If we are building the native bson extension ensure we use gmake if available\n if(build_native) {\n   // Check if we need to use gmake\n",
					"match": false,
					"packageHash": "955347cbab86146af5f817e5d6c514338c51e8cb1bc5c48de2b7df001ce98910",
					"size": 1565,
					"sourceHash": "773466260a06c5ce4dfabbe8530bccd1def293efcda3c09ad1dea99915efb931",
					"status": "content"
				},
				"lib/mongodb/admin.js": {
					"diff": "--- published/lib/mongodb/admin.js\n+++ rebuilt/lib/mongodb/admin.js\n@@ -1,13 +1,43 @@\n+/*!\n+ * Module dependencies.\n+ */\n var Collection = require('./collection').Collection,\n     Cursor = require('./cursor').Cursor,\n-    DbCommand = require('./commands/db_command').DbCommand,\n-    debug = require('util').debug, \n-    inspect = require('util').inspect;\n+    DbCommand = require('./commands/db_command').DbCommand;\n \n-var Admin = exports.Admin = function(db) {  \n+/**\n+ * Allows the user to access the admin functionality of MongoDB\n+ *\n+ * @class Represents the Admin methods of MongoDB.\n+ * @param {Object} db Current db instance we wish to perform Admin operations on.\n+ * @return {Function} Constructor for Admin type.\n+ */\n+function Admin(db) {  \n+  if(!(this instanceof Admin)) return new Admin(db);\n+  \n   this.db = db;\n };\n \n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n+Admin.prototype.buildInfo = function(callback) {\n+  this.serverInfo(callback);\n+}\n+\n+/**\n+ * Retrieve the server information for the current\n+ * instance of the db client\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api private\n+ */\n Admin.prototype.serverInfo = function(callback) {\n   var self = this;\n   var command = {buildinfo:1};\n@@ -17,14 +47,44 @@\n   });\n }\n \n+/**\n+ * Retrieve this db's server status.\n+ *\n+ * @param {Function} callback returns the server status.\n+ * @return {null}\n+ * @api public\n+ */\n+Admin.prototype.serverStatus = function(callback) {\n+  var self = this;\n+\n+  this.command({serverStatus: 1}, function(err, result) {\n+    if (err == null && result.documents[0].ok == 1) {\n+      callback(null, result.documents[0]);\n+    } else {\n+      if (err) {\n+        callback(err, false);\n+      } else {\n+        callback(self.wrap(result.documents[0]), false);\n+      }\n+    }\n+  });\n+};\n+\n+/**\n+ * Retrieve the current profiling Level for MongoDB\n+ * \n+ * @param {Function} callback Callback function of format `function(err, result) {}`.\n+ * @return {null} Returns no result\n+ * @api public\n+ */\n Admin.prototype.profilingLevel = function(callback) {\n   var self = this;\n   var command = {profile:-1};\n \n   this.command(command, function(err, doc) {\n     doc = doc.documents[0];\n-\n-    if(err == null && (doc.ok == 1 || doc.was.constructor == Numeric)) {\n+    \n+    if(err == null && (doc.ok == 1 || typeof doc.was === 'number')) {\n       var was = doc.was;\n       if(was == 0) {\n         callback(null, \"off\");\n@@ -41,6 +101,14 @@\n   });\n };\n",
					"match": false,
					"packageHash": "71db7c9061c78cf1febe59a7127f486c2050ba16fa9b2f13a8f48ec0b56ecf08",
					"size": 5273,
					"sourceHash": "a037f0b049bc29d04bfc3ee04e7483bbe7b8750c68b95e6c0b45b94b67897af1",
					"status": "content"
				},
				"lib/mongodb/bson/binary.js": {
					"match": false,
					"packageHash": "0d0f307d7a9384663d43b7e83b6823bdce801cec41b9d19dc1f72af355c854d8",
					"size": 3105,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/binary_parser.js": {
					"match": false,
					"packageHash": "ef9e3cac450d49f4d2f5879017e6bff297710d096dbeb8a47d5082db39f38b38",
					"size": 12297,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/binary_utils.js": {
					"match": false,
					"packageHash": "f9837d9b2dd787c6e452579657fb60e59d0831cf8a0b5d9cf03ab20c92d242d5",
					"size": 812,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/bson.js": {
					"match": false,
					"packageHash": "acf53fd1f5bd384b2ba89d520b4d1e81d908e70656528a2a008809c810096ea6",
					"size": 58920,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/code.js": {
					"match": false,
					"packageHash": "308b9def8d5dec0e88e7b9d8bb41151e4f2d2748b2df907a94bf051877e1c241",
					"size": 306,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/db_ref.js": {
					"match": false,
					"packageHash": "810bf5aa6705bf7dadbc681abdd88a8fe5a2cfdac6cd6427a28590a1c293cff0",
					"size": 403,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/double.js": {
					"match": false,
					"packageHash": "078a671bdde6e021ec8ee16423f843116b9162f10936fc7171707c09abc801b3",
					"size": 232,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/float_parser.js": {
					"match": false,
					"packageHash": "5b6b4382617418d5310f2a0565db42b33b957eb2c5c478fc428fdd82ee1d882d",
					"size": 3844,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/max_key.js": {
					"match": false,
					"packageHash": "0f58e237d93653be1056bbae33e574ed356b3b638600d4f547ff4ed965d52818",
					"size": 111,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/min_key.js": {
					"match": false,
					"packageHash": "42a28cb4f1e7d655aadc41fd2bfa3999dd5e664185ae8149fd558c63a2434a32",
					"size": 109,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/objectid.js": {
					"match": false,
					"packageHash": "0df7c0c1e207ea6f569ad8b964ca6320d1c797af1662d1602a1bb398cb942bc3",
					"size": 4904,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/symbol.js": {
					"match": false,
					"packageHash": "b6bd6b82dac9ebf14456f64ee1c9569da959caacd97e05c63920e20ffaa238fc",
					"size": 352,
					"status": "missing-in-source"
				},
				"lib/mongodb/bson/timestamp.js": {
					"match": false,
					"packageHash": "3a00c0c4753a0615ab2e65d314ddbbc5de3959bd58a248485c809281a29e4712",
					"size": 22822,
					"status": "missing-in-source"
				},
				"lib/mongodb/collection.js": {
					"diff": "--- published/lib/mongodb/collection.js\n+++ rebuilt/lib/mongodb/collection.js\n@@ -1,132 +1,100 @@\n /**\n  * Module dependencies.\n+ * @ignore\n  */\n var InsertCommand = require('./commands/insert_command').InsertCommand\n   , QueryCommand = require('./commands/query_command').QueryCommand\n   , DeleteCommand = require('./commands/delete_command').DeleteCommand\n   , UpdateCommand = require('./commands/update_command').UpdateCommand\n   , DbCommand = require('./commands/db_command').DbCommand\n-  , BinaryParser = require('./bson/binary_parser').BinaryParser\n-  , ObjectID = require('./bson/objectid').ObjectID\n-  , Code = require('./bson/code').Code\n+  , ObjectID = require('bson').ObjectID\n+  , Code = require('bson').Code\n   , Cursor = require('./cursor').Cursor\n-  , debug = require('util').debug\n-  , inspect = require('util').inspect;\n+  , utils = require('./utils');\n \n /**\n  * Precompiled regexes\n+ * @ignore\n **/\n const eErrorMessages = /No matching object found/;\n \n /**\n- * Sort functions, Normalize and prepare sort parameters\n- */\n-\n-function formatSortValue (sortDirection) {\n-  var value = (\"\" + sortDirection).toLowerCase();\n-\n-  switch (value) {\n-    case 'ascending':\n-    case 'asc':\n-    case '1':\n-      return 1;\n-    case 'descending':\n-    case 'desc':\n-    case '-1':\n-      return -1;\n-    default:\n-      throw new Error(\"Illegal sort clause, must be of the form \"\n-                    + \"[['field1', '(ascending|descending)'], \"\n-                    + \"['field2', '(ascending|descending)']]\");\n-  }\n-};\n-\n-function formattedOrderClause (sortValue) {\n-  var orderBy = {};\n-\n-  if (Array.isArray(sortValue)) {\n-    sortValue.forEach(function (sortElement) {\n-      if (sortElement.constructor == String) {\n-        orderBy[sortElement] = 1;\n-      } else {\n-        orderBy[sortElement[0]] = formatSortValue(sortElement[1]);\n-      }\n-    });\n-  } else if (sortValue.constructor == String) {\n-    orderBy[sortValue] = 1;\n-  } else {\n-    throw new Error(\"Illegal sort clause, must be of the form \" +\n-      \"[['field1', '(ascending|descending)'], ['field2', '(ascending|descending)']]\");\n-  }\n-\n-  return orderBy;\n-};\n-\n-/**\n  * toString helper.\n+ * @ignore\n  */\n-\n var toString = Object.prototype.toString;\n \n /**\n- * Collection constructor.\n+ * Create a new Collection instance\n  *\n- * @param {Database} db\n- * @param {String} collectionName\n- * @param {Function} pkFactory\n+ * Options\n+ *  - **slaveOk** {Boolean, default:false}, Allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions on the document.\n+ *  - **raw** {Boolean, default:false}, perform all operations using raw bson objects.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *\n+ * @class Represents a Collection\n+ * @param {Object} db db instance.\n+ * @param {String} collectionName collection name.\n+ * @param {Object} [pkFactory] alternative primary key factory.\n+ * @param {Object} [options] additional options for the collection.\n+ * @return {Object} a collection instance.\n  */\n-\n",
					"match": false,
					"packageHash": "3b4fa5a7a2b3e1fb9674252f3b1e34e3a7c15d86bbfb99369043ff6f0063f89b",
					"size": 36258,
					"sourceHash": "a3563efa6644f7f9f404b47e0386f1ca647574fd634b04b427ee5ef9c05c8b56",
					"status": "content"
				},
				"lib/mongodb/commands/base_command.js": {
					"diff": "--- published/lib/mongodb/commands/base_command.js\n+++ rebuilt/lib/mongodb/commands/base_command.js\n@@ -1,7 +1,3 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n-\n /**\n   Base object used for common functionality\n **/\n",
					"match": false,
					"packageHash": "4da764d9a6b2c9ef7ffb01aa2018388739e12237d0974f8f5b6a1632085caad1",
					"size": 774,
					"sourceHash": "3b9d5bf8d0033df72262cdd5c60feb91a4052597696bb1123bae0beeda3f9aa9",
					"status": "content"
				},
				"lib/mongodb/commands/db_command.js": {
					"diff": "--- published/lib/mongodb/commands/db_command.js\n+++ rebuilt/lib/mongodb/commands/db_command.js\n@@ -1,9 +1,7 @@\n var QueryCommand = require('./query_command').QueryCommand,\n   InsertCommand = require('./insert_command').InsertCommand,\n   inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  crypto = require('crypto'),\n-  inspect = require('util').inspect;\n+  crypto = require('crypto');\n \n /**\n   Db Command\n@@ -94,9 +92,11 @@\n };\n \n DbCommand.createGetLastErrorCommand = function(options, db) {\n-  var args = Array.prototype.slice.call(arguments, 0);\n-  db = args.pop();\n-  options = args.length ? args.shift() : {};\n+\n+  if (typeof db === 'undefined') {\n+    db =  options;\n+    options = {};\n+  }\n   // Final command \n   var command = {'getlasterror':1};\n   // If we have an options Object let's merge in the fields (fsync/wtimeout/w)\n@@ -121,23 +121,10 @@\n };\n \n DbCommand.createCreateIndexCommand = function(db, collectionName, fieldOrSpec, options) {\n-  var finalUnique = options == null || 'object' === typeof options ? false : options;\n   var fieldHash = {};\n   var indexes = [];\n   var keys;\n-  var sparse;\n-  var background;\n-  var geoMin, geoMax;\n   \n-  // If the options is a hash\n-  if(options != null && 'object' === typeof options) {\n-    finalUnique = options['unique'] != null ? options['unique'] : false;\n-    sparse = options['sparse'] != null ? options['sparse'] : false;\n-    background = options['background'] != null ? options['background'] : false;\n-    geoMin = options['min'] != null ? options['min'] : null;\n-    geoMax = options['max'] != null ? options['max'] : null;\n-  }\n-\n   // Get all the fields accordingly\n   if (fieldOrSpec.constructor === String) {             // 'type'\n     indexes.push(fieldOrSpec + '_' + 1);\n@@ -166,22 +153,27 @@\n       indexes.push(key + '_' + fieldOrSpec[key]);\n       fieldHash[key] = fieldOrSpec[key];\n     });\n-  } else {\n-    // undefined\n   }\n   \n   // Generate the index name\n   var indexName = indexes.join(\"_\");\n-\n   // Build the selector\n   var selector = {'ns':(db.databaseName + \".\" + collectionName), 'key':fieldHash, 'name':indexName};\n-  selector['unique'] = finalUnique;\n-  selector['sparse'] = sparse;\n-  selector['background'] = background;\n-\n-  if (geoMin !== null) selector['min'] = geoMin;\n-  if (geoMax !== null) selector['max'] = geoMax;\n \n+  // Ensure we have a correct finalUnique\n+  var finalUnique = options == null || 'object' === typeof options ? false : options;\n+  // Set up options\n+  options = options == null || typeof options == 'boolean' ? {} : options;\n+  \n+  // Add all the options\n+  var keys = Object.keys(options);\n+  // Add all the fields to the selector\n+  for(var i = 0; i < keys.length; i++) {\n+    selector[keys[i]] = options[keys[i]];\n+  }\n+  \n+  // If we don't have the unique property set on the selector\n+  if(selector['unique'] == null) selector['unique'] = finalUnique;\n   // Create the insert command for the index and return the document\n   return new InsertCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_INDEX_COLLECTION, false).add(selector);\n };\n@@ -194,6 +186,10 @@\n   return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'deleteIndexes':collectionName, 'index':indexName}, null);\n };\n \n+DbCommand.createReIndexCommand = function(db, collectionName) {\n+  return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'reIndex':collectionName}, null);\n+};\n+\n DbCommand.createDropDatabaseCommand = function(db) {\n   return new DbCommand(db, db.databaseName + \".\" + DbCommand.SYSTEM_COMMAND_COLLECTION, QueryCommand.OPTS_NO_CURSOR_TIMEOUT, 0, -1, {'dropDatabase':1}, null);\n };\n",
					"match": false,
					"packageHash": "e5e91952edbd77fa89da7f302b4e28e118e9132eb2a4310c935753f28d8df2e6",
					"size": 9020,
					"sourceHash": "6f41be547a863b0b0169eba3b2ecc8d56a3ca07357817dae0fd22c2063b2d8ed",
					"status": "content"
				},
				"lib/mongodb/commands/delete_command.js": {
					"diff": "--- published/lib/mongodb/commands/delete_command.js\n+++ rebuilt/lib/mongodb/commands/delete_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug, \n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -11,7 +9,7 @@\n \n   // Validate correctness off the selector\n   var object = selector;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;        \n     if(object_size != object.length)  {\n       var error = new Error(\"delete raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -40,7 +38,7 @@\n */\n DeleteCommand.prototype.toBinary = function() {\n   // Calculate total length of the document\n-  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.selector) + (4 * 4);\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.selector, false, true) + (4 * 4);\n   // Let's build the single pass buffer command\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n@@ -92,7 +90,7 @@\n \n   // Serialize the selector\n   // If we are passing a raw buffer, do minimal validation\n-  if(this.selector instanceof Buffer) {\n+  if(Buffer.isBuffer(this.selector)) {\n     documentLength = this.selector.length;\n     // Copy the data into the current buffer\n     this.selector.copy(_command, _index);\n",
					"match": false,
					"packageHash": "d734996eca9347a8f9564ec48c0acecbd117b655a3b67ff2a016e5d45cb46479",
					"size": 3962,
					"sourceHash": "4e4d0f12685765c7a6eb62947f5f73432a07453a99925a1a0e1a63d53d4d218a",
					"status": "content"
				},
				"lib/mongodb/commands/get_more_command.js": {
					"diff": "--- published/lib/mongodb/commands/get_more_command.js\n+++ rebuilt/lib/mongodb/commands/get_more_command.js\n@@ -1,8 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n   inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  binaryutils = require('../bson/binary_utils');\n+  binaryutils = require('../utils');\n \n /**\n   Get More Document Command\n@@ -27,31 +25,28 @@\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n   // Write the header information to the buffer\n-  _command[_index + 3] = (totalLengthOfCommand >> 24) & 0xff;     \n-  _command[_index + 2] = (totalLengthOfCommand >> 16) & 0xff;\n-  _command[_index + 1] = (totalLengthOfCommand >> 8) & 0xff;\n-  _command[_index] = totalLengthOfCommand & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = totalLengthOfCommand & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 8) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 16) & 0xff;\n+  _command[_index++] = (totalLengthOfCommand >> 24) & 0xff;     \n+\n   // Write the request ID\n-  _command[_index + 3] = (this.requestId >> 24) & 0xff;     \n-  _command[_index + 2] = (this.requestId >> 16) & 0xff;\n-  _command[_index + 1] = (this.requestId >> 8) & 0xff;\n-  _command[_index] = this.requestId & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = this.requestId & 0xff;\n+  _command[_index++] = (this.requestId >> 8) & 0xff;\n+  _command[_index++] = (this.requestId >> 16) & 0xff;\n+  _command[_index++] = (this.requestId >> 24) & 0xff;     \n+\n   // Write zero\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n   _command[_index++] = 0;\n+\n   // Write the op_code for the command\n-  _command[_index + 3] = (GetMoreCommand.OP_GET_MORE >> 24) & 0xff;     \n-  _command[_index + 2] = (GetMoreCommand.OP_GET_MORE >> 16) & 0xff;\n-  _command[_index + 1] = (GetMoreCommand.OP_GET_MORE >> 8) & 0xff;\n-  _command[_index] = GetMoreCommand.OP_GET_MORE & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = GetMoreCommand.OP_GET_MORE & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 8) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 16) & 0xff;\n+  _command[_index++] = (GetMoreCommand.OP_GET_MORE >> 24) & 0xff;     \n \n   // Write zero\n   _command[_index++] = 0;\n@@ -64,31 +59,25 @@\n   _command[_index - 1] = 0;    \n \n   // Number of documents to return\n-  _command[_index + 3] = (this.numberToReturn >> 24) & 0xff;     \n-  _command[_index + 2] = (this.numberToReturn >> 16) & 0xff;\n-  _command[_index + 1] = (this.numberToReturn >> 8) & 0xff;\n-  _command[_index] = this.numberToReturn & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = this.numberToReturn & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 8) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 16) & 0xff;\n+  _command[_index++] = (this.numberToReturn >> 24) & 0xff;     \n   \n   // Encode the cursor id\n   var low_bits = this.cursorId.getLowBits();\n   // Encode low bits\n-  _command[_index + 3] = (low_bits >> 24) & 0xff;     \n-  _command[_index + 2] = (low_bits >> 16) & 0xff;\n-  _command[_index + 1] = (low_bits >> 8) & 0xff;\n-  _command[_index] = low_bits & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n+  _command[_index++] = low_bits & 0xff;\n+  _command[_index++] = (low_bits >> 8) & 0xff;\n+  _command[_index++] = (low_bits >> 16) & 0xff;\n+  _command[_index++] = (low_bits >> 24) & 0xff;     \n   \n   var high_bits = this.cursorId.getHighBits();\n   // Encode high bits\n-  _command[_index + 3] = (high_bits >> 24) & 0xff;     \n-  _command[_index + 2] = (high_bits >> 16) & 0xff;\n-  _command[_index + 1] = (high_bits >> 8) & 0xff;\n-  _command[_index] = high_bits & 0xff;\n-  // Adjust index\n-  _index = _index + 4;\n-  \n+  _command[_index++] = high_bits & 0xff;\n+  _command[_index++] = (high_bits >> 8) & 0xff;\n+  _command[_index++] = (high_bits >> 16) & 0xff;\n",
					"match": false,
					"packageHash": "2bdf5588881e7d743730642f844a484ccb12669438bd34142ce80041d89690d5",
					"size": 3218,
					"sourceHash": "9c9ec06d4ad6306c6823afb357aa2f5a41fffc9ac401894f0ba240e13b72a592",
					"status": "content"
				},
				"lib/mongodb/commands/insert_command.js": {
					"diff": "--- published/lib/mongodb/commands/insert_command.js\n+++ rebuilt/lib/mongodb/commands/insert_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -37,7 +35,7 @@\n InsertCommand.OP_INSERT =\t2002;\n \n InsertCommand.prototype.add = function(document) {\n-  if(document instanceof Buffer) {\n+  if(Buffer.isBuffer(document)) {\n     var object_size = document[0] | document[1] << 8 | document[2] << 16 | document[3] << 24;    \n     if(object_size != document.length)  {\n       var error = new Error(\"insert raw message size does not match message header size [\" + document.length + \"] != [\" + object_size + \"]\");\n@@ -63,14 +61,14 @@\n   var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + (4 * 4);\n   // var docLength = 0\n   for(var i = 0; i < this.documents.length; i++) {\n-    if(this.documents[i] instanceof Buffer) {\n+    if(Buffer.isBuffer(this.documents[i])) {\n       totalLengthOfCommand += this.documents[i].length;\n     } else {\n       // Calculate size of document\n-      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.documents[i], this.serializeFunctions);      \n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.documents[i], this.serializeFunctions, true);\n     }\n   }\n-    \n+  \n   // Let's build the single pass buffer command\n   var _index = 0;\n   var _command = new Buffer(totalLengthOfCommand);\n@@ -119,7 +117,7 @@\n \n     // Serialize the selector\n     // If we are passing a raw buffer, do minimal validation\n-    if(object instanceof Buffer) {      \n+    if(Buffer.isBuffer(object)) {\n       documentLength = object.length;\n       // Copy the data into the current buffer\n       object.copy(_command, _index);\n",
					"match": false,
					"packageHash": "9e3ae657a998cec0267b72854841cfb0d284f8b5288f00b50e35e8177b529aa8",
					"size": 5161,
					"sourceHash": "90970dad921cc155b641d84092b4cee8d1b9f7486e4199685a856dc4fb4ed19d",
					"status": "content"
				},
				"lib/mongodb/commands/kill_cursor_command.js": {
					"diff": "--- published/lib/mongodb/commands/kill_cursor_command.js\n+++ rebuilt/lib/mongodb/commands/kill_cursor_command.js\n@@ -1,8 +1,6 @@\n var BaseCommand = require('./base_command').BaseCommand,\n   inherits = require('util').inherits,\n-  binaryutils = require('../bson/binary_utils'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  binaryutils = require('../utils');\n \n /**\n   Insert Document Command\n",
					"match": false,
					"packageHash": "9936ea981ab114f879b79fd0debb73194ffe4315f27e356a60cbe02c403e0ab8",
					"size": 3427,
					"sourceHash": "d01af72568b490f4e88bb07da854ed5b050494e55e90b60b8b3d11561f8ffa5e",
					"status": "content"
				},
				"lib/mongodb/commands/query_command.js": {
					"diff": "--- published/lib/mongodb/commands/query_command.js\n+++ rebuilt/lib/mongodb/commands/query_command.js\n@@ -1,8 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Insert Document Command\n@@ -11,20 +8,21 @@\n   BaseCommand.call(this);\n \n   // Validate correctness off the selector\n-  var object = query;\n-  if(object instanceof Buffer) {\n-    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n-    if(object_size != object.length)  {\n+  var object = query,\n+    object_size;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n       var error = new Error(\"query selector raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n       error.name = 'MongoError';\n       throw error;\n     }\n   }\n \n-  var object = returnFieldSelector;\n-  if(object instanceof Buffer) {\n-    var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n-    if(object_size != object.length)  {\n+  object = returnFieldSelector;\n+  if(Buffer.isBuffer(object)) {\n+    object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n+    if(object_size != object.length) {\n       var error = new Error(\"query fields raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n       error.name = 'MongoError';\n       throw error;\n@@ -45,7 +43,7 @@\n   // Let us defined on a command basis if we want functions to be serialized or not\n   if(options['serializeFunctions'] != null && options['serializeFunctions']) {\n     this.serializeFunctions = true;\n-  }  \n+  }\n };\n \n inherits(QueryCommand, BaseCommand);\n@@ -66,18 +64,18 @@\n QueryCommand.prototype.toBinary = function() {\n   var totalLengthOfCommand = 0;\n   // Calculate total length of the document\n-  if(this.query instanceof Buffer) {\n+  if(Buffer.isBuffer(this.query)) {\n     totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.query.length + (4 * 4);    \n   } else {\n-    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.db.bson.calculateObjectSize(this.query, this.serializeFunctions) + (4 * 4);    \n+    totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + 4 + this.db.bson.calculateObjectSize(this.query, this.serializeFunctions, true) + (4 * 4);    \n   }\n   \n   // Calculate extra fields size\n-  if(this.returnFieldSelector != null && !(this.returnFieldSelector instanceof Buffer))  {\n+  if(this.returnFieldSelector != null && !(Buffer.isBuffer(this.returnFieldSelector)))  {\n     if(Object.keys(this.returnFieldSelector).length > 0) {\n-      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.returnFieldSelector, this.serializeFunctions);\n+      totalLengthOfCommand += this.db.bson.calculateObjectSize(this.returnFieldSelector, this.serializeFunctions, true);\n     }\n-  } else if(this.returnFieldSelector instanceof Buffer) {\n+  } else if(Buffer.isBuffer(this.returnFieldSelector)) {\n     totalLengthOfCommand += this.returnFieldSelector.length;\n   }\n \n@@ -144,7 +142,7 @@\n   var object = this.query;\n \n   // Serialize the selector\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     documentLength = object.length;\n     // Copy the data into the current buffer\n     object.copy(_command, _index);\n@@ -164,7 +162,7 @@\n   _command[_index - 1] = 0;    \n \n   // Push field selector if available\n-  if(this.returnFieldSelector != null && !(this.returnFieldSelector instanceof Buffer))  {\n+  if(this.returnFieldSelector != null && !(Buffer.isBuffer(this.returnFieldSelector)))  {\n     if(Object.keys(this.returnFieldSelector).length > 0) {\n       var documentLength = this.db.bson.serializeWithBufferAndIndex(this.returnFieldSelector, this.checkKeys, _command, _index, this.serializeFunctions) - _index + 1;\n       // Write the length to the document\n@@ -177,7 +175,7 @@\n       // Add terminating 0 for the object\n       _command[_index - 1] = 0;    \n     }\n-  } if(this.returnFieldSelector != null && this.returnFieldSelector instanceof Buffer)  {\n+  } if(this.returnFieldSelector != null && Buffer.isBuffer(this.returnFieldSelector))  {\n     // Document binary length\n",
					"match": false,
					"packageHash": "bfad2d55959c16c44eb8abd889e94b20967016be777de3a858fdad393d4772a3",
					"size": 8396,
					"sourceHash": "42e8d9b5c0118fe3e93e96f00a9998faafbc79b058168904ec3341353c71240b",
					"status": "content"
				},
				"lib/mongodb/commands/update_command.js": {
					"diff": "--- published/lib/mongodb/commands/update_command.js\n+++ rebuilt/lib/mongodb/commands/update_command.js\n@@ -1,7 +1,5 @@\n var BaseCommand = require('./base_command').BaseCommand,\n-  inherits = require('util').inherits,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  inherits = require('util').inherits;\n \n /**\n   Update Document Command\n@@ -10,7 +8,7 @@\n   BaseCommand.call(this);\n \n   var object = spec;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n     if(object_size != object.length)  {\n       var error = new Error(\"update spec raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -20,7 +18,7 @@\n   }\n \n   var object = document;\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;    \n     if(object_size != object.length)  {\n       var error = new Error(\"update document raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n@@ -65,8 +63,8 @@\n */\n UpdateCommand.prototype.toBinary = function() {\n   // Calculate total length of the document\n-  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.spec, false) +\n-      this.db.bson.calculateObjectSize(this.document, this.serializeFunctions) + (4 * 4);\n+  var totalLengthOfCommand = 4 + Buffer.byteLength(this.collectionName) + 1 + 4 + this.db.bson.calculateObjectSize(this.spec, false, true) +\n+      this.db.bson.calculateObjectSize(this.document, this.serializeFunctions, true) + (4 * 4);\n \n   // Let's build the single pass buffer command\n   var _index = 0;\n@@ -122,7 +120,7 @@\n \n   // Serialize the selector\n   // If we are passing a raw buffer, do minimal validation\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n     if(object_size != object.length) throw new Error(\"raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n     documentLength = object.length;\n@@ -148,7 +146,7 @@\n \n   // Serialize the document\n   // If we are passing a raw buffer, do minimal validation\n-  if(object instanceof Buffer) {\n+  if(Buffer.isBuffer(object)) {\n     var object_size = object[0] | object[1] << 8 | object[2] << 16 | object[3] << 24;\n     if(object_size != object.length) throw new Error(\"raw message size does not match message header size [\" + object.length + \"] != [\" + object_size + \"]\");\n     documentLength = object.length;\n",
					"match": false,
					"packageHash": "89212ccae8e11b6660e27fc21c12158aee8108ff880f7b81128b344fea747977",
					"size": 6701,
					"sourceHash": "1e9efcade1290911d79395a36a74e0f4bdfc7608c739482547738ac544d537d3",
					"status": "content"
				},
				"lib/mongodb/connection/connection.js": {
					"diff": "--- published/lib/mongodb/connection/connection.js\n+++ rebuilt/lib/mongodb/connection/connection.js\n@@ -1,11 +1,9 @@\n var utils = require('./connection_utils'),\n   inherits = require('util').inherits,\n   net = require('net'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n   EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits,\n-  binaryutils = require('../bson/binary_utils'),\n+  binaryutils = require('../utils'),\n   tls = require('tls');\n \n var Connection = exports.Connection = function(id, socketOptions) {\n@@ -39,29 +37,31 @@\n }\n \n // Set max bson size\n-Connection.DEFAULT_MAX_BSON_SIZE = 4 * 1024 * 1024 * 4 * 3;\n+Connection.DEFAULT_MAX_BSON_SIZE = 1024 * 1024 * 4;\n \n // Inherit event emitter so we can emit stuff wohoo\n-// inherits(Connection, SimpleEmitter);\n inherits(Connection, EventEmitter);\n \n Connection.prototype.start = function() {\n   // If we have a normal connection\n   if(this.socketOptions.ssl) {\n     // Create a new stream\n-    this.connection = new net.Stream();\n+    this.connection = new net.Socket();    \n     // Set options on the socket\n     this.connection.setTimeout(this.socketOptions.timeout);\n-    this.connection.setNoDelay(this.socketOptions.noDelay);\n+    // Work around for 0.4.X\n+    if(process.version.indexOf(\"v0.4\") == -1) this.connection.setNoDelay(this.socketOptions.noDelay);\n     // Set keep alive if defined\n-    if(this.socketOptions.keepAlive > 0) {\n-      this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n-    } else {\n-      this.connection.setKeepAlive(false);\n-    } \n-\n+    if(process.version.indexOf(\"v0.4\") == -1) {\n+      if(this.socketOptions.keepAlive > 0) {\n+        this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n+      } else {\n+        this.connection.setKeepAlive(false);\n+      }         \n+    }\n+    \n     // Set up pair for tls with server, accept self-signed certificates as well\n-    this.pair = tls.createSecurePair(false);\n+    var pair = this.pair = tls.createSecurePair(false);\n     // Set up encrypted streams\n     this.pair.encrypted.pipe(this.connection);\n     this.connection.pipe(this.pair.encrypted);\n@@ -73,7 +73,7 @@\n     this.pair.cleartext.on(\"data\", createDataHandler(this));\n     // Add handlers\n     this.connection.on(\"error\", errorHandler(this));\n-    // Add all handlers to the socket to manage it\n+    // this.connection.on(\"connect\", connectHandler(this));\n     this.connection.on(\"end\", endHandler(this));\n     this.connection.on(\"timeout\", timeoutHandler(this));\n     this.connection.on(\"drain\", drainHandler(this));\n@@ -81,19 +81,20 @@\n     // Start socket\n     this.connection.connect(this.socketOptions.port, this.socketOptions.host);\n   } else {\n-    // // Create a new stream\n-    // this.connection = new net.Stream();\n-    // // Create new connection instance\n-    this.connection = new net.Socket();\n+    // Create new connection instance\n+    this.connection = net.createConnection(this.socketOptions.port, this.socketOptions.host);\n     // Set options on the socket\n     this.connection.setTimeout(this.socketOptions.timeout);\n-    this.connection.setNoDelay(this.socketOptions.noDelay);\n+    // Work around for 0.4.X\n+    if(process.version.indexOf(\"v0.4\") == -1) this.connection.setNoDelay(this.socketOptions.noDelay);\n     // Set keep alive if defined\n-    if(this.socketOptions.keepAlive > 0) {\n-      this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n-    } else {\n-      this.connection.setKeepAlive(false);\n-    }   \n+    if(process.version.indexOf(\"v0.4\") == -1) {\n+      if(this.socketOptions.keepAlive > 0) {\n+        this.connection.setKeepAlive(true, this.socketOptions.keepAlive);\n+      } else {\n+        this.connection.setKeepAlive(false);\n+      }         \n+    }\n \n     // Set up write stream\n     this.writeSteam = this.connection;\n@@ -106,8 +107,6 @@\n     this.connection.on(\"timeout\", timeoutHandler(this));\n",
					"match": false,
					"packageHash": "dc722e3c2df7849f0953ed1febd16418c58d9c1374a738c9e05f13e36363d088",
					"size": 16486,
					"sourceHash": "a05c2fb8cfdcc1099cb7e68c0b153a5789d48a77dbfe1a1d3b78f940f008ee46",
					"status": "content"
				},
				"lib/mongodb/connection/connection_pool.js": {
					"diff": "--- published/lib/mongodb/connection/connection_pool.js\n+++ rebuilt/lib/mongodb/connection/connection_pool.js\n@@ -18,6 +18,7 @@\n   this.bson = bson;\n   // PoolSize is always + 1 for special reserved \"measurment\" socket (like ping, stats etc)\n   this.poolSize = poolSize;\n+  this.minPoolSize = Math.floor(this.poolSize / 2) + 1;\n   \n   // Set default settings for the socket options\n   utils.setIntegerParameter(this.socketOptions, 'timeout', 0);\n@@ -31,13 +32,7 @@\n   utils.setIntegerParameter(this.socketOptions, 'bufferSize', 0);  \n   \n   // Internal structures\n-  // this.waitingToOpen = {};\n-  // this.connectionsWithErrors = {};\n-  // this.openConnections = {};\n-  // this.connectionsWithErrors = [];\n-  this.openConnections = [];\n-  this.connections = [];\n-  \n+  this.openConnections = [];  \n   // Assign connection id's\n   this.connectionId = 0;\n   \n@@ -73,15 +68,13 @@\n     connection.on(\"connect\", function(err, connection) {\n       // Add connection to list of open connections\n       _self.openConnections.push(connection);\n-      _self.connections.push(connection)\n-\n       // If the number of open connections is equal to the poolSize signal ready pool\n-      if(_self.connections.length === _self.poolSize && _self._poolState !== 'disconnected') {\n+      if(_self.openConnections.length === _self.poolSize && _self._poolState !== 'disconnected') {\n         // Set connected\n         _self._poolState = 'connected';\n         // Emit pool ready\n         _self.emit(\"poolReady\");\n-      } else if(_self.connections.length < _self.poolSize) {\n+      } else if(_self.openConnections.length < _self.poolSize) {\n         // We need to open another connection, make sure it's in the next\n         // tick so we don't get a cascade of errors\n         process.nextTick(function() {\n@@ -96,7 +89,7 @@\n     connection.on(\"error\", function(err, connection) {\n       numberOfErrors++;\n       // If we are already disconnected ignore the event\n-      if(connectionStatus !== 'disconnected') {\n+      if(connectionStatus != 'disconnected' && _self.listeners(\"error\").length > 0) {\n         _self.emit(\"error\", err);        \n       }\n \n@@ -104,15 +97,14 @@\n       connectionStatus = 'disconnected';\n       // Set disconnected\n       _self._poolState = 'disconnected'; \n-      // Clean up\n-      _self.openConnections = [];    \n-      _self.connections = [];\n+      // Stop\n+      _self.stop();\n     });\n \n     // Close handler\n     connection.on(\"close\", function() {\n       // If we are already disconnected ignore the event\n-      if(connectionStatus !== 'disconnected') {\n+      if(connectionStatus !== 'disconnected' && _self.listeners(\"close\").length > 0) {\n         _self.emit(\"close\");        \n       }\n \n@@ -120,29 +112,32 @@\n       connectionStatus = 'disconnected';\n       // Set disconnected\n       _self._poolState = 'disconnected'; \n-      // Clean up\n-      _self.openConnections = [];    \n-      _self.connections = [];\n+      // Stop\n+      _self.stop();\n     });\n \n     // Timeout handler\n     connection.on(\"timeout\", function(err, connection) {\n       // If we are already disconnected ignore the event\n-      if(connectionStatus !== 'disconnected') {\n-        _self.emit(\"error\", err);        \n+      if(connectionStatus !== 'disconnected' && _self.listeners(\"timeout\").length > 0) {\n+        _self.emit(\"timeout\", err);        \n       }\n \n       // Set disconnected\n       connectionStatus = 'disconnected';\n       // Set disconnected\n       _self._poolState = 'disconnected'; \n-      // Clean up\n-      _self.openConnections = [];    \n-      _self.connections = [];\n+      // Stop\n+      _self.stop();\n",
					"match": false,
					"packageHash": "228ffcd25c7794092fcd7131da77c65f81e22e1e73edc5537e85e72511ace914",
					"size": 7639,
					"sourceHash": "ccd192af58ec6c8b8d2fe094bf77ce695fca8f47f7e69c869489fadb0e9f00ab",
					"status": "content"
				},
				"lib/mongodb/connection/repl_set_servers.js": {
					"match": false,
					"packageHash": "3badec5d9cf20a05595ea0ef18c227cb80acdb92622443876cb253734ececf8b",
					"size": 33929,
					"status": "missing-in-source"
				},
				"lib/mongodb/connection/server.js": {
					"diff": "--- published/lib/mongodb/connection/server.js\n+++ rebuilt/lib/mongodb/connection/server.js\n@@ -2,11 +2,32 @@\n   DbCommand = require('../commands/db_command').DbCommand,\n   MongoReply = require('../responses/mongo_reply').MongoReply,\n   ConnectionPool = require('./connection_pool').ConnectionPool,\n-  SimpleEmitter = require('./simple_emitter').SimpleEmitter,\n-  MongoReply = require(\"../responses/mongo_reply\").MongoReply,\n+  EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits;\n \n-var Server = exports.Server = function(host, port, options) {\n+/**\n+ * Class representing a single MongoDB Server connection\n+ *\n+ * Options\n+ *  - **readPreference** {String, default:null}, set's the read preference (Server.READ_PRIMAR, Server.READ_SECONDARY_ONLY, Server.READ_SECONDARY)\n+ *  - **ssl** {Boolean, default:false}, use ssl connection (needs to have a mongod server with ssl support)\n+ *  - **slaveOk** {Boolean, default:false}, legacy option allowing reads from secondary, use **readPrefrence** instead.\n+ *  - **poolSize** {Number, default:1}, number of connections in the connection pool, set to 1 as default for legacy reasons.\n+ *  - **socketOptions** {Object, default:null}, an object containing socket options to use (noDelay:(boolean), keepAlive:(number), timeout:(number))\n+ *  - **logger** {Object, default:null}, an object representing a logger that you want to use, needs to support functions debug, log, error **({error:function(message, object) {}, log:function(message, object) {}, debug:function(message, object) {}})**.\n+ *  - **auto_reconnect** {Boolean, default:false}, reconnect on error.\n+ *\n+ * @class Represents a Server connection.\n+ * @param {String} host the server host\n+ * @param {Number} port the server port\n+ * @param {Object} [options] optional options for insert command\n+ */\n+function Server(host, port, options) {\n+  // Set up event emitter\n+  EventEmitter.call(this);  \n+  // Set up Server instance\n+  if(!(this instanceof Server)) return new Server(host, port, options);\n+  \n   var self = this;\n   this.host = host;\n   this.port = port;\n@@ -17,14 +38,25 @@\n   this.poolSize = this.options.poolSize == null ? 1 : this.options.poolSize;\n   this.ssl = this.options.ssl == null ? false : this.options.ssl;\n   this.slaveOk = this.options[\"slave_ok\"];\n-  // Setters and getters\n-  this.__defineGetter__(\"autoReconnect\", function() { return self.options['auto_reconnect'] == null ? false : this.options['auto_reconnect']; });\n-  this.__defineGetter__(\"connection\", function() { return self.internalConnection; });\n-  this.__defineSetter__(\"connection\", function(connection) { self.internalConnection = connection; });\n-  this.__defineGetter__(\"master\", function() { return self.internalMaster; });\n-  this.__defineSetter__(\"master\", function(value) { self.internalMaster = value; });\n-  this.__defineGetter__(\"primary\", function() { return self; });\n-  this.__defineGetter__(\"readPreference\", function() { return Server.READ_PRIMARY; });\n+  this._used = false;\n+  \n+  // Get the readPreference\n+  var readPreference = this.options['readPreference'];  \n+  // Read preference setting\n+  if(readPreference != null) {\n+    if(readPreference != Server.READ_PRIMARY && readPreference != Server.READ_SECONDARY_ONLY\n+      && readPreference != Server.READ_SECONDARY) {\n+        throw new Error(\"Illegal readPreference mode specified, \" + readPreference);\n+    }\n+    \n+    // Set read Preference\n+    this._readPreference = readPreference;\n+  } else {\n+    this._readPreference = null;        \n+  }\n+  \n+  // Contains the isMaster information returned from the server\n+  this.isMasterDoc;\n \n   // Set default connection pool options\n   this.socketOptions = this.options.socketOptions != null ? this.options.socketOptions : {};\n@@ -37,7 +69,7 @@\n   this.logger = this.options.logger != null \n     && (typeof this.options.logger.debug == 'function') \n     && (typeof this.options.logger.error == 'function') \n-    && (typeof this.options.logger.debug == 'function') \n+    && (typeof this.options.logger.log == 'function') \n       ? this.options.logger : {error:function(message, object) {}, log:function(message, object) {}, debug:function(message, object) {}};\n \n   // Just keeps list of events we allow\n@@ -49,21 +81,87 @@\n   this._state = {'runtimeStats': {'queryStats':new RunningStats()}};  \n   // Do we record server stats or not\n   this.recordQueryStats = false;\n+  \n+  // Setters and getters\n+  Object.defineProperty(this, \"autoReconnect\", { enumerable: true\n+    , get: function () {\n+        return this.options['auto_reconnect'] == null ? false : this.options['auto_reconnect'];\n+      }\n+  });  \n+\n+  Object.defineProperty(this, \"connection\", { enumerable: true\n+    , get: function () {\n+        return this.internalConnection;\n+      }\n+    , set: function(connection) {\n+        this.internalConnection = connection;\n+      }\n",
					"match": false,
					"packageHash": "04a893aa5fb49812cefddf90cd61e5521ce05e4b843579b971fc4568132e038d",
					"size": 18062,
					"sourceHash": "2509622ad0acf9deda2227f721c9bc9a58e2ac9d418b1c75fe584cc42bcac5b3",
					"status": "content"
				},
				"lib/mongodb/connection/simple_emitter.js": {
					"match": false,
					"packageHash": "06f23ea1be542085b58435016ba2d0bd7ead99822d13837161357e0110f777cc",
					"size": 1760,
					"status": "missing-in-source"
				},
				"lib/mongodb/cursor.js": {
					"diff": "--- published/lib/mongodb/cursor.js\n+++ rebuilt/lib/mongodb/cursor.js\n@@ -1,49 +1,41 @@\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   GetMoreCommand = require('./commands/get_more_command').GetMoreCommand,\n   KillCursorCommand = require('./commands/kill_cursor_command').KillCursorCommand,\n-  Long = require('./goog/math/long').Long,\n+  Long = require('bson').Long,\n   CursorStream = require('./cursorstream'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  utils = require('./utils');\n \n /**\n  * Constructor for a cursor object that handles all the operations on query result\n- * using find. This cursor object is unidirectional and cannot traverse backwards.\n- * As an alternative, {@link Cursor#toArray} can be used to obtain all the results.\n- * Clients should not be creating a cursor directly, but use {@link Collection#find}\n- * to acquire a cursor.\n- *\n- * @constructor\n- *\n- * @param db {Db} The database object to work with\n- * @param collection {Colleciton} The collection to query\n- * @param selector\n- * @param fields\n- * @param skip {number}\n- * @param limit {number} The number of results to return. -1 has a special meaning and\n- *     is used by {@link Db#eval}. A value of 1 will also be treated as if it were -1.\n- * @param sort {string|Array<Array<string|object> >} Please refer to {@link Cursor#sort}\n- * @param hint\n- * @param explain\n- * @param snapshot\n- * @param timeout\n- * @param tailable {?boolean}\n- * @param batchSize {?number} The number of the subset of results to request the database\n- *     to return for every request. This should initially be greater than 1 otherwise\n- *     the database will automatically close the cursor. The batch size can be set to 1\n- *     with {@link Cursor#batchSize} after performing the initial query to the database.\n- * @param raw {?boolean} Return all query documents as raw buffers (default false)\n- * @param read {?boolean} Specify override of read from source (primary/secondary)\n- *\n- * @see Cursor#toArray\n- * @see Cursor#skip\n- * @see Cursor#sort\n- * @see Cursor#limit\n- * @see Cursor#batchSize\n- * @see Collection#find\n- * @see Db#eval\n- */\n-var Cursor = exports.Cursor = function(db, collection, selector, fields, skip, limit, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk, raw, read) {\n+ * using find. This cursor object is unidirectional and cannot traverse backwards. Clients should not be creating a cursor directly, \n+ * but use find to acquire a cursor.\n+ *\n+ * @class Represents a Cursor.\n+ * @param {Db} db the database object to work with.\n+ * @param {Collection} collection the collection to query.\n+ * @param {Object} selector the query selector.\n+ * @param {Object} fields an object containing what fields to include or exclude from objects returned.\n+ * @param {Number} skip number of documents to skip.\n+ * @param {Number} limit the number of results to return. -1 has a special meaning and is used by Db.eval. A value of 1 will also be treated as if it were -1.\n+ * @param {String|Array|Object} sort the required sorting for the query.\n+ * @param {Object} hint force the query to use a specific index.\n+ * @param {Boolean} explain return the explaination of the query.\n+ * @param {Boolean} snapshot Snapshot mode assures no duplicates are returned.\n+ * @param {Boolean} timeout allow the query to timeout.\n+ * @param {Boolean} tailable allow the cursor to be tailable.\n+ * @param {Number} batchSize the number of the subset of results to request the database to return for every request. This should initially be greater than 1 otherwise the database will automatically close the cursor. The batch size can be set to 1 with cursorInstance.batchSize after performing the initial query to the database.\n+ * @param {Boolean} raw return all query documents as raw buffers (default false).\n+ * @param {Boolean} read specify override of read from source (primary/secondary).\n+ * @param {Boolean} returnKey only return the index key.\n+ * @param {Number} maxScan limit the number of items to scan.\n+ * @param {Number} min set index bounds.\n+ * @param {Number} max set index bounds.\n+ * @param {Boolean} showDiskLoc show disk location of results.\n+ * @param {String} comment you can put a $comment field on a query to make looking in the profiler logs simpler.\n+ */\n+function Cursor(db, collection, selector, fields, skip, limit\n+\t, sort, hint, explain, snapshot, timeout, tailable, batchSize, slaveOk, raw, read\n+\t, returnKey, maxScan, min, max, showDiskLoc, comment) {\n   this.db = db;\n   this.collection = collection;\n   this.selector = selector;\n@@ -60,6 +52,12 @@\n   this.slaveOk = slaveOk == null ? collection.slaveOk : slaveOk;\n   this.raw = raw == null ? false : raw;\n   this.read = read == null ? true : read;\n+  this.returnKey = returnKey;\n+  this.maxScan = maxScan;\n+  this.min = min;\n+  this.max = max;\n+  this.showDiskLoc = showDiskLoc;\n+  this.comment = comment;\n   \n   this.totalNumberOfRecords = 0;\n   this.items = [];\n@@ -76,6 +74,9 @@\n /**\n  * Resets this cursor to its initial state. All settings like the query string,\n  * tailable, batchSizeValue, skipValue and limits are preserved.\n",
					"match": false,
					"packageHash": "cf205ebcf1d4ba56db3861dfbebc920ec24bf3b0032e8c417eba60e7ee0daa73",
					"size": 23843,
					"sourceHash": "c4a6fb5b596bb88a2e03a3d00859e3cb8ae2ac573e12fa196743abfbf01737ea",
					"status": "content"
				},
				"lib/mongodb/cursorstream.js": {
					"diff": "--- published/lib/mongodb/cursorstream.js\n+++ rebuilt/lib/mongodb/cursorstream.js\n@@ -1,20 +1,25 @@\n-\n /**\n  * Module dependecies.\n  */\n-\n var Stream = require('stream').Stream;\n \n /**\n  * CursorStream\n  *\n- * Returns a stream interface for the `cursor`.\n+ * Returns a stream interface for the **cursor**.\n  *\n- * @param {Cursor} cursor\n+ * Events\n+ *  - **data** {function(item) {}} the data event triggers when a document is ready.\n+ *  - **error** {function(err) {}} the error event triggers if an error happens.\n+ *  - **end** {function() {}} the end event triggers when there is no more documents available.\n+ *\n+ * @class Represents a CursorStream.\n+ * @param {Cursor} cursor a cursor object that the stream wraps.\n  * @return {Stream}\n  */\n-\n-function CursorStream (cursor) {\n+function CursorStream(cursor) {\n+  if(!(this instanceof CursorStream)) return new CursorStream(cursor);\n+  \n   Stream.call(this);\n \n   this.readable = true;\n@@ -31,28 +36,26 @@\n \n /**\n  * Inherit from Stream\n- * @private\n+ * @ignore\n+ * @api private\n  */\n-\n CursorStream.prototype.__proto__ = Stream.prototype;\n \n /**\n  * Flag stating whether or not this stream is readable.\n  */\n-\n CursorStream.prototype.readable;\n \n /**\n  * Flag stating whether or not this stream is paused.\n  */\n-\n CursorStream.prototype.paused;\n \n /**\n  * Initialize the cursor.\n- * @private\n+ * @ignore\n+ * @api private\n  */\n-\n CursorStream.prototype._init = function () {\n   if (this._destroyed) return;\n   this._next();\n@@ -60,9 +63,9 @@\n \n /**\n  * Pull the next document from the cursor.\n- * @private\n+ * @ignore\n+ * @api private\n  */\n-\n CursorStream.prototype._next = function () {\n   if (this.paused || this._destroyed) return;\n \n@@ -79,9 +82,9 @@\n \n /**\n  * Handle each document as its returned from the cursor.\n- * @private\n+ * @ignore\n+ * @api private\n  */\n-\n CursorStream.prototype._onNextObject = function (err, doc) {\n   if (err) return this.destroy(err);\n \n@@ -93,17 +96,19 @@\n }\n \n /**\n- * Pauses this stream.\n+ * Pauses the stream.\n+ *\n+ * @api public\n  */\n",
					"match": false,
					"packageHash": "6597ae92a7bc784562dd1c7e8c6ce7a3c7a4d27baf8ce8d339c42715f9568f97",
					"size": 2321,
					"sourceHash": "0e80e052fb49e079d8b9a5c5f957370f0780cd5e677ec1b8638aafa7e7ea4b2e",
					"status": "content"
				},
				"lib/mongodb/db.js": {
					"diff": "--- published/lib/mongodb/db.js\n+++ rebuilt/lib/mongodb/db.js\n@@ -1,20 +1,23 @@\n+/**\n+ * Module dependencies.\n+ * @ignore\n+ */\n var QueryCommand = require('./commands/query_command').QueryCommand,\n   DbCommand = require('./commands/db_command').DbCommand,\n-  BinaryParser = require('./bson/binary_parser').BinaryParser,\n   MongoReply = require('./responses/mongo_reply').MongoReply,\n   Admin = require('./admin').Admin,\n   Collection = require('./collection').Collection,\n   Server = require('./connection/server').Server,\n-  ReplSetServers = require('./connection/repl_set_servers').ReplSetServers,\n+  ReplSet = require('./connection/repl_set').ReplSet,\n   Cursor = require('./cursor').Cursor,\n   EventEmitter = require('events').EventEmitter,\n   inherits = require('util').inherits,\n-  crypto = require('crypto'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  b = require('./bson/bson');\n+  crypto = require('crypto');\n \n-// Internal class for callback storage\n+/**\n+ * Internal class for callback storage \n+ * @ignore\n+ */\n var CallbackStore = function() {\n   // Make class an event emitter\n   EventEmitter.call(this);\n@@ -22,13 +25,53 @@\n   this._notReplied = {};\n }\n \n+/**\n+ * @ignore\n+ */\n inherits(CallbackStore, EventEmitter);\n \n-var Db = exports.Db = function(databaseName, serverConfig, options) {\n+/**\n+ * Create a new Db instance.\n+ *\n+ * Options\n+ *  - **strict** {true | {w:n, wtimeout:n} | {fsync:true}, default:false}, execute insert with a getLastError command returning the result of the insert command.\n+ *  - **native_parser** {Boolean, default:false}, use c++ bson parser.\n+ *  - **forceServerObjectId** {Boolean, default:false}, force server to create _id fields instead of client.\n+ *  - **pkFactory** {Object}, object overriding the basic ObjectID primary key generation.\n+ *  - **slaveOk** {Boolean, default:false}, allow reads from secondaries.\n+ *  - **serializeFunctions** {Boolean, default:false}, serialize functions.\n+ *  - **raw** {Boolean, default:false}, peform operations using raw bson buffers.\n+ *  - **recordQueryStats** {Boolean, default:false}, record query statistics during execution.\n+ *  - **reaper** {Boolean, default:false}, enables the reaper, timing out calls that never return.\n+ *  - **reaperInterval** {Number, default:10000}, number of miliseconds between reaper wakups.\n+ *  - **reaperTimeout** {Number, default:30000}, the amount of time before a callback times out.\n+ *  - **retryMiliSeconds** {Number, default:5000}, number of miliseconds between retries.\n+ *  - **numberOfRetries** {Number, default:5}, number of retries off connection.\n+ *\n+ * @class Represents a Collection\n+ * @param {String} databaseName name of the database.\n+ * @param {Object} serverConfig server config object.\n+ * @param {Object} [options] additional options for the collection.\n+ */\n+function Db(databaseName, serverConfig, options) {\n+\n+  if(!(this instanceof Db)) return new Db(databaseName, serverConfig, options);\n+  \n   EventEmitter.call(this);\n   this.databaseName = databaseName;\n-  this.serverConfig = serverConfig;\n+  this.serverConfig = serverConfig;  \n   this.options = options == null ? {} : options;  \n+  // State to check against if the user force closed db\n+  this._applicationClosed = false;\n+  // Fetch the override flag if any\n+  var overrideUsedFlag = this.options['override_used_flag'] == null ? false : this.options['override_used_flag'];  \n+  // Verify that nobody is using this config\n+  if(!overrideUsedFlag && typeof this.serverConfig == 'object' && this.serverConfig._isUsed()) {\n+    throw new Error(\"A Server or ReplSet instance cannot be shared across multiple Db instances\");\n+  } else if(!overrideUsedFlag && typeof this.serverConfig == 'object'){\n+    // Set being used\n+    this.serverConfig._used = true;    \n+  }\n   \n   // Ensure we have a valid db name\n   validateDatabaseName(databaseName);\n@@ -36,26 +79,33 @@\n   // Contains all the connections for the db\n   try {\n     this.native_parser = this.options.native_parser;\n+    // The bson lib\n+    var bsonLib = this.bsonLib = this.options.native_parser ? require('bson').BSONNative : new require('bson').BSONPure;\n     // Fetch the serializer object\n-    var BSON = this.options.native_parser ? require('../../external-libs/bson').BSON : new require('./bson/bson').BSON;\n+    var BSON = bsonLib.BSON;\n     // Create a new instance\n-    this.bson = new BSON([b.Long, b.ObjectID, b.Binary, b.Code, b.DBRef, b.Symbol, b.Double, b.Timestamp, b.MaxKey, b.MinKey]);\n",
					"match": false,
					"packageHash": "3d2a079b637ddd3d66f91d7ffca379800a611601f8de85a552e93f2b54be7156",
					"size": 43502,
					"sourceHash": "8975ea6eee925ca6f0b33ba4cb074d8de4c712ad9540ccd346aeb97e626a7e93",
					"status": "content"
				},
				"lib/mongodb/goog/math/long.js": {
					"match": false,
					"packageHash": "fc0c7c9faa936fd72e56a3fdb80b22ae55831fdad1efcb4e694c29bc0fae603c",
					"size": 21763,
					"status": "missing-in-source"
				},
				"lib/mongodb/gridfs/chunk.js": {
					"diff": "--- published/lib/mongodb/gridfs/chunk.js\n+++ rebuilt/lib/mongodb/gridfs/chunk.js\n@@ -1,12 +1,8 @@\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  Binary = require('../bson/binary').Binary,\n-  ObjectID = require('../bson/objectid').ObjectID,\n-  sys = require('util'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+var Binary = require('bson').Binary,\n+  ObjectID = require('bson').ObjectID;\n \n /**\n- * Class for representing a signle chunk in GridFS.\n+ * Class for representing a single chunk in GridFS.\n  *\n  * @class\n  *\n@@ -21,7 +17,10 @@\n  * @see Chunk#buildMongoObject\n  */\n var Chunk = exports.Chunk = function(file, mongoObject) {\n+  if(!(this instanceof Chunk)) return new Chunk(file, mongoObject);\n+  \n   this.file = file;\n+  var self = this;\n   var mongoObjectFinal = mongoObject == null ? {} : mongoObject;\n \n   this.objectId = mongoObjectFinal._id == null ? new ObjectID() : mongoObjectFinal._id;\n@@ -29,30 +28,37 @@\n   this.data = new Binary();\n \n   if(mongoObjectFinal.data == null) {\n-  } else if(mongoObjectFinal.data.constructor == String) {\n+  } else if(typeof mongoObjectFinal.data == \"string\") {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data, 'binary', 0);\n     this.data = new Binary(buffer);\n-  } else if(mongoObjectFinal.data.constructor == Array) {\n+  } else if(Array.isArray(mongoObjectFinal.data)) {\n     var buffer = new Buffer(mongoObjectFinal.data.length);\n     buffer.write(mongoObjectFinal.data.join(''), 'binary', 0);\n     this.data = new Binary(buffer);\n   } else if(mongoObjectFinal.data instanceof Binary || Object.prototype.toString.call(mongoObjectFinal.data) == \"[object Binary]\") {    \n     this.data = mongoObjectFinal.data;\n-  } else if(mongoObjectFinal.data instanceof Buffer) {\n+  } else if(Buffer.isBuffer(mongoObjectFinal.data)) {\n   } else {\n     throw Error(\"Illegal chunk format\");\n   }\n   // Update position\n   this.internalPosition = 0;\n-\t/**\n-\t * The position of the read/write head\n-\t * @name position\n-\t * @lends Chunk#\n-\t * @field\n-\t */\n-  this.__defineGetter__(\"position\", function() { return this.internalPosition; });\n-  this.__defineSetter__(\"position\", function(value) { this.internalPosition = value; });\n+\n+  /**\n+   * The position of the read/write head\n+   * @name position\n+   * @lends Chunk#\n+   * @field\n+   */\n+  Object.defineProperty(this, \"position\", { enumerable: true\n+    , get: function () {\n+        return this.internalPosition;\n+      }\n+    , set: function(value) {\n+        this.internalPosition = value;\n+      }\n+  });  \n };\n \n /**\n@@ -66,7 +72,8 @@\n Chunk.prototype.write = function(data, callback) {\n   this.data.write(data, this.internalPosition);\n   this.internalPosition = this.data.length();\n-  callback(null, this);\n+  if(callback != null) return callback(null, this);\n+  return this;\n };\n \n /**\n@@ -97,7 +104,6 @@\n       data = this.data.buffer.slice(this.internalPosition, this.internalPosition + length);\n     } else { //Native BSON\n       data = new Buffer(length);\n-      //length = data.write(this.data.read(this.internalPosition, length), 'binary', 0);\n       length = this.data.readInto(data, this.internalPosition);\n     }\n     this.internalPosition = this.internalPosition + length;\n",
					"match": false,
					"packageHash": "dd263d67501d052350ac7831d097bf24a9cfd8d0ce4f0f2b070f39d594daa856",
					"size": 6680,
					"sourceHash": "ebb1d349b3e79a84bc1a2f0a2d8bb5679a6f63325d9d48d0665c1296119ff3a5",
					"status": "content"
				},
				"lib/mongodb/gridfs/grid.js": {
					"diff": "--- published/lib/mongodb/gridfs/grid.js\n+++ rebuilt/lib/mongodb/gridfs/grid.js\n@@ -1,13 +1,18 @@\n var GridStore = require('./gridstore').GridStore,\n-  ObjectID = require('../bson/objectid').ObjectID,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+  ObjectID = require('bson').ObjectID;\n \n /**\n- * Simple Grid interface\n+ * A class representation of a simple Grid interface.\n  *\n+ * @class Represents the Grid.\n+ * @param {Db} db A database instance to interact with.\n+ * @param {String} [fsName] optional different root collection for GridFS.\n+ * @return {Grid}\n  */\n-var Grid = exports.Grid = function(db, fsName) {\n+function Grid(db, fsName) {\n+\n+  if(!(this instanceof Grid)) return new Grid(db, fsName);\n+  \n   this.db = db;\n   this.fsName = fsName == null ? GridStore.DEFAULT_ROOT_COLLECTION : fsName;\n } \n@@ -15,13 +20,11 @@\n /**\n  * Puts binary data to the grid\n  *\n- * @param data Buffer with Binary Data\n- * @param options {object=} opt_argument The options for the files.\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n- *     to this object.\n- *\n+ * @param {Buffer} data buffer with Binary Data.\n+ * @param {Object} [options] the options for the files.\n+ * @callback {Function} this will be called after this method is executed. The first parameter will contain an Error object if an error occured or null otherwise. The second parameter will contain a reference to this object.\n+ * @return {null}\n+ * @api public\n  */\n Grid.prototype.put = function(data, options, callback) {\n   var self = this;\n@@ -32,7 +35,7 @@\n   options['root'] = options['root'] == null ? this.fsName : options['root'];\n     \n   // Return if we don't have a buffer object as data\n-  if(!(data instanceof Buffer)) return callback(new Error(\"Data object must be a buffer object\"), null);    \n+  if(!(Buffer.isBuffer(data))) return callback(new Error(\"Data object must be a buffer object\"), null);    \n   // Get filename if we are using it\n   var filename = options['filename'];\n   // Create gridstore\n@@ -40,7 +43,7 @@\n   gridStore.open(function(err, gridStore) {\n     if(err) return callback(err, null);\n \n-    gridStore.writeBuffer(data, function(err, result) {\n+    gridStore.write(data, function(err, result) {\n       if(err) return callback(err, null);\n \n       gridStore.close(function(err, result) {\n@@ -54,12 +57,10 @@\n /**\n  * Get binary data to the grid\n  *\n- * @param id ObjectID for file\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n- *     to this object.\n- *\n+ * @param {ObjectID} id ObjectID for file.\n+ * @callback {Function} this will be called after this method is executed. The first parameter will contain an Error object if an error occured or null otherwise. The second parameter will contain a reference to this object.\n+ * @return {null}\n+ * @api public\n  */\n Grid.prototype.get = function(id, callback) {\n   // Validate that we have a valid ObjectId\n@@ -70,7 +71,7 @@\n     if(err) return callback(err, null);\n     \n     // Return the data\n-    gridStore.readBuffer(function(err, data) {\n+    gridStore.read(function(err, data) {\n       return callback(err, data)\n     });  \n   })\n@@ -79,12 +80,10 @@\n /**\n  * Delete file from grid\n  *\n- * @param id ObjectID for file\n- * @callback {function(?Error, GridStore)} This will be called after this method\n- *     is executed. The first parameter will contain an Error object if an error\n- *     occured or null otherwise. The second parameter will contain a reference\n- *     to this object.\n- *\n+ * @param {ObjectID} id ObjectID for file.\n",
					"match": false,
					"packageHash": "b00694d0f6b7e8c95f9d15758950f5f6699a57e4f45dca67dfc52a2ada43ff8e",
					"size": 3296,
					"sourceHash": "2b8f34d347f7cff63ec859c8eb5e7f43212b384cd72adfcfb7c997e2caf6eae8",
					"status": "content"
				},
				"lib/mongodb/gridfs/gridstore.js": {
					"diff": "--- published/lib/mongodb/gridfs/gridstore.js\n+++ rebuilt/lib/mongodb/gridfs/gridstore.js\n@@ -6,17 +6,13 @@\n  * chunks of split files behind the scenes. More information about GridFS can be\n  * found <a href=\"http://www.mongodb.org/display/DOCS/GridFS\">here</a>.\n  */\n-\n-var BinaryParser = require('../bson/binary_parser').BinaryParser,\n-  Chunk = require('./chunk').Chunk,\n+var Chunk = require('./chunk').Chunk,\n   DbCommand = require('../commands/db_command').DbCommand,\n-  ObjectID = require('../bson/objectid').ObjectID,\n+  ObjectID = require('bson').ObjectID,\n   Buffer = require('buffer').Buffer,\n   fs = require('fs'),\n   util = require('util'),\n-  debug = require('util').debug,\n-  inspect = require('util').inspect,\n-  Stream = require('stream').Stream;\n+  ReadStream = require('./readstream').ReadStream;\n \n var REFERENCE_BY_FILENAME = 0,\n   REFERENCE_BY_ID = 1;\n@@ -24,37 +20,48 @@\n /**\n  * A class representation of a file stored in GridFS.\n  *\n- * @class\n- *\n- * @param db {Db} A database instance to interact with.\n- * @param filename {string} The name for the file.\n- * @param mode {?string} Set the mode for this file. Available modes:\n- *     <ul>\n- *       <li>\"r\" - read only. This is the default mode.</li>\n- *       <li>\"w\" - write in truncate mode. Existing data will be overwriten</li>\n- *       <li>\"w+\" - write in edit mode.</li>\n- *     </ul>\n-\n- * @param options {?object} Optional properties to specify. Recognized keys:\n- *\n- *     <pre><code>\n- *     {\n- *       'root' : , // {string} root collection to use. Defaults to GridStore#DEFAULT_ROOT_COLLECTION\n- *       'chunk_type' : , // {string} mime type of the file. Defaults to GridStore#DEFAULT_CONTENT_TYPE\n- *       'chunk_size' : , // {number} size for the chunk. Defaults to Chunk#DEFAULT_CHUNK_SIZE.\n- *       'metadata' : , // {object} arbitrary data the user wants to store\n- *     }\n- *     </code></pre>\n- *\n- * @see <a href=\"http://www.mongodb.org/display/DOCS/GridFS+Specification\">MongoDB GridFS Specification</a>\n+ * Modes\n+ *  - **\"r\"** - read only. This is the default mode.\n+ *  - **\"w\"** - write in truncate mode. Existing data will be overwriten.\n+ *  - **w+\"** - write in edit mode.\n+ *\n+ * Options\n+ *  - **root** {String}, root collection to use. Defaults to **{GridStore.DEFAULT_ROOT_COLLECTION}**.\n+ *  - **chunk_type** {String}, mime type of the file. Defaults to **{GridStore.DEFAULT_CONTENT_TYPE}**.\n+ *  - **chunk_size** {Number}, size for the chunk. Defaults to **{Chunk.DEFAULT_CHUNK_SIZE}**.\n+ *  - **metadata** {Object}, arbitrary data the user wants to store.\n+ *\n+ * @class Represents the GridStore.\n+ * @param {Db} db A database instance to interact with.\n+ * @param {ObjectID} id an unique ObjectID for this file\n+ * @param {String} [filename] optional a filename for this file, no unique constrain on the field\n+ * @param {String} mode set the mode for this file.\n+ * @param {Object} options optional properties to specify. Recognized keys:\n+ * @return {GridStore}\n  */\n-var GridStore = exports.GridStore = function(db, fileIdObject, mode, options) {\n+function GridStore(db, id, filename, mode, options) {\n+  if(!(this instanceof GridStore)) return new GridStore(db, id, filename, mode, options);\n+\n+  var self = this;\n   this.db = db;  \n+  var _filename = filename;\n+\n+  if(typeof filename == 'string' && typeof mode == 'string') {\n+    _filename = filename;  \n+  } else if(typeof filename == 'string' && typeof mode == 'object' && mode != null) {\n+    var _mode = mode;\n+    mode = filename;\n+    options = _mode;    \n+    _filename = id;\n+  } else if(typeof filename == 'string' && mode == null) {\n+    mode = filename;\n+    _filename = id;\n+  }\n   \n   // set grid referencetype\n-  this.referenceBy = typeof fileIdObject == 'string' ? 0 : 1;\n-  this.filename = fileIdObject;\n-  this.fileId = fileIdObject;\n+  this.referenceBy = typeof id == 'string' ? 0 : 1;\n+  this.filename = _filename;\n+  this.fileId = typeof id == 'string' ? new ObjectID() : id;\n   \n   // Set up the rest\n   this.mode = mode == null ? \"r\" : mode;\n@@ -62,45 +69,55 @@\n",
					"match": false,
					"packageHash": "024aa85e55569a18b161cd099f74b46acdb83350288301dacef7418cc90c7847",
					"size": 42725,
					"sourceHash": "3431d98442b60a3a8125b38249d63dfe6a81338d3bf41d9b86b2e8e71c0c3aac",
					"status": "content"
				},
				"lib/mongodb/index.js": {
					"diff": "--- published/lib/mongodb/index.js\n+++ rebuilt/lib/mongodb/index.js\n@@ -1,13 +1,11 @@\n-\n try {\n-  exports.BSONPure = require('./bson/bson');\n-  exports.BSONNative = require('../../external-libs/bson');\n+  exports.BSONPure = require('bson').BSONPure;\n+  exports.BSONNative = require('bson').BSONNative;\n } catch(err) {\n   // do nothing\n }\n \n-[ 'bson/binary_parser'\n-  , 'commands/base_command'\n+[ 'commands/base_command'\n   , 'commands/db_command'\n   , 'commands/delete_command'\n   , 'commands/get_more_command'\n@@ -20,10 +18,9 @@\n   , 'collection'\n   , 'connection/connection'\n   , 'connection/server'\n-  , 'connection/repl_set_servers'\n+  , 'connection/repl_set'\n   , 'cursor'\n   , 'db'\n-  , 'goog/math/long'\n   , 'gridfs/grid'\n   ,\t'gridfs/chunk'\n   , 'gridfs/gridstore'].forEach(function (path) {\n@@ -31,15 +28,31 @@\n   \tfor (var i in module) {\n   \t\texports[i] = module[i];\n     }\n+\n+    // backwards compat\n+    exports.ReplSetServers = exports.ReplSet;\n+    \n+    // Add BSON Classes\n+    exports.Binary = require('bson').Binary;\n+    exports.Code = require('bson').Code;\n+    exports.DBRef = require('bson').DBRef;\n+    exports.Double = require('bson').Double;\n+    exports.Long = require('bson').Long;\n+    exports.MinKey = require('bson').MinKey;\n+    exports.MaxKey = require('bson').MaxKey;\n+    exports.ObjectID = require('bson').ObjectID;\n+    exports.Symbol = require('bson').Symbol;\n+    exports.Timestamp = require('bson').Timestamp;  \n+    \n+    // Add BSON Parser\n+    exports.BSON = require('bson').BSONPure.BSON;\n });\n \n-// Exports all the classes for the NATIVE JS BSON Parser\n-exports.native = function() {\n+// Exports all the classes for the PURE JS BSON Parser\n+exports.pure = function() {\n   var classes = {};\n   // Map all the classes\n-  [ 'bson/binary_parser'\n-    , '../../external-libs/bson/bson'\n-    , 'commands/base_command'\n+  [ 'commands/base_command'\n     , 'commands/db_command'\n     , 'commands/delete_command'\n     , 'commands/get_more_command'\n@@ -52,7 +65,7 @@\n     , 'collection'\n     , 'connection/connection'\n     , 'connection/server'\n-    , 'connection/repl_set_servers'\n+    , 'connection/repl_set'\n     , 'cursor'\n     , 'db'\n     , 'gridfs/grid'\n@@ -63,17 +76,34 @@\n     \t\tclasses[i] = module[i];\n       }\n   });\n+\n+  // backwards compat\n+  classes.ReplSetServers = exports.ReplSet;\n+\n+  // Add BSON Classes\n+  classes.Binary = require('bson').Binary;\n+  classes.Code = require('bson').Code;\n+  classes.DBRef = require('bson').DBRef;\n+  classes.Double = require('bson').Double;\n+  classes.Long = require('bson').Long;\n+  classes.MinKey = require('bson').MinKey;\n+  classes.MaxKey = require('bson').MaxKey;\n+  classes.ObjectID = require('bson').ObjectID;\n+  classes.Symbol = require('bson').Symbol;\n+  classes.Timestamp = require('bson').Timestamp;\n+\n+  // Add BSON Parser\n+  classes.BSON = require('bson').BSONPure.BSON;\n+\n",
					"match": false,
					"packageHash": "27237bb38f9c69a17e08e052e3493a19a137069f6417e24c6ac19192bbd93eb6",
					"size": 2531,
					"sourceHash": "f177a8277bbee6a3ec1c3e002bda2026d47cad3da7c164edb7f615a0b9c7b360",
					"status": "content"
				},
				"lib/mongodb/responses/mongo_reply.js": {
					"diff": "--- published/lib/mongodb/responses/mongo_reply.js\n+++ rebuilt/lib/mongodb/responses/mongo_reply.js\n@@ -1,6 +1,4 @@\n-var Long = require('../goog/math/long').Long,\n-  debug = require('util').debug,\n-  inspect = require('util').inspect;\n+var Long = require('bson').Long;\n \n /**\n   Reply message from mongo db\n@@ -58,8 +56,7 @@\n \n     // Actual main creator of the processFunction setting internal state to control the flow\n     var parseFunction = function(_self, _binary_reply, _batchSize, _numberReturned) {\n-      var object_index = 0;\n-      \n+      var object_index = 0;      \n       // Internal loop process that will use nextTick to ensure we yield some time\n       var processFunction = function() {\n         // Adjust batchSize if we have less results left than batchsize\n",
					"match": false,
					"packageHash": "3163a57c2b2e516221b7d446959b7c73e7d831b16c19c72fb8ca2851cb1a373b",
					"size": 6206,
					"sourceHash": "2a7cc4f286e5792d1ac34e4e92fc1586b483f6f974fb93afbf63d6d80d64e842",
					"status": "content"
				},
				"package.json": {
					"diff": "--- published/package.json\n+++ rebuilt/package.json\n@@ -1,7 +1,7 @@\n { \"name\" :            \"mongodb\"\n , \"description\" :     \"A node.js driver for MongoDB\"\n , \"keywords\" :        [\"mongodb\", \"mongo\", \"driver\", \"db\"]\n-, \"version\" :         \"0.9.7-3\"\n+, \"version\" :         \"0.9.9-8\"\n , \"author\" :          \"Christian Amor Kvalheim <christkv@gmail.com>\"\n , \"contributors\" :  [ \"Aaron Heckmann\",\n                       \"Christoph Pojer\",\n@@ -57,14 +57,24 @@\n                     , \"url\" :   \"http://github.com/christkv/node-mongodb-native.git\" }\n , \"bugs\" :          { \"mail\" :  \"node-mongodb-native@googlegroups.com\"\n                     , \"url\" :   \"http://github.com/christkv/node-mongodb-native/issues\" }\n-, \"os\" :            [ \"linux\"\n-                    , \"darwin\"\n-                    , \"freebsd\" ]\n+, \"dependencies\" : {\n+  \"bson\": \"0.0.4\"\n+}                    \n+, \"devDependencies\": {\n+      \"dox\": \"0.2.0\"\n+    , \"uglify-js\": \"1.2.5\"\n+    , \"ejs\": \"0.6.1\"\n+    , \"nodeunit\": \"0.7.3\"\n+    , \"github3\": \">=0.3.0\"\n+\t  , \"markdown\": \"0.3.1\"\n+\t  , \"gleak\": \"0.2.3\"\n+\t  , \"step\": \"0.0.5\"\n+  }\n , \"config\":         { \"native\" : false }                    \n , \"main\":             \"./lib/mongodb/index\"\n , \"directories\" :   { \"lib\" : \"./lib/mongodb\" }\n , \"engines\" :       { \"node\" : \">=0.4.0\" }\n-, \"scripts\": { \"install\" : \"node install.js\" }\n+, \"scripts\": { \"test\" : \"make test_pure\" }\n , \"licenses\" :    [ { \"type\" :  \"Apache License, Version 2.0\"\n                     , \"url\" :   \"http://www.apache.org/licenses/LICENSE-2.0\" } ]\n }\n",
					"match": false,
					"packageHash": "471061b4eb2e7e4cdd69fc087a39be8f0ea59bfa87960eb1c44b2beefa28cd18",
					"size": 2895,
					"sourceHash": "3e819dd4d5c4f71977047355855ffb75eb246f9cfed7b07d12c2abb32fdb8bc5",
					"status": "content"
				},
				"test/admin_test.js": {
					"match": false,
					"packageHash": "3eeb88612380ece5d85b23a9acf1dd9b5ad0e26117c85787f6ba2df3c9191e62",
					"size": 7497,
					"status": "missing-in-source"
				},
				"test/authentication_test.js": {
					"match": false,
					"packageHash": "cbecafc67651b202be044ed51a1a0b5f9fc6e0f2b2dff4abc0660f4e065f1546",
					"size": 5139,
					"status": "missing-in-source"
				},
				"test/auxilliary/authentication_test.js": {
					"match": false,
					"packageHash": "fdf5125456a610b766ab4de57f15480b34508fa4f4141512218fa73abb2523e3",
					"size": 6967,
					"status": "missing-in-source"
				},
				"test/auxilliary/repl_set_ssl_test.js": {
					"match": false,
					"packageHash": "15d3122c26b9160a9903750b34f536eb161f0f462febf0a487306887b0a7b741",
					"size": 2097,
					"status": "missing-in-source"
				},
				"test/auxilliary/replicaset_auth_test.js": {
					"match": false,
					"packageHash": "f444a6462c5d47a540af846b4f2ebeda55ce65347bb82c0b42957dcedea7e20f",
					"size": 7121,
					"status": "missing-in-source"
				},
				"test/auxilliary/single_server_kill_reconnect.js": {
					"match": false,
					"packageHash": "dc91ee99e91fd8b6aa9b7d3332bd7778abfd4551fd305f91b23cb60d0a7e0c4f",
					"size": 5743,
					"status": "missing-in-source"
				},
				"test/auxilliary/ssl_test.js": {
					"match": false,
					"packageHash": "6ad88ea94a28f4e97474023193b35a9d3edc812b65a4f63853b69a71f42d0c5a",
					"size": 2460,
					"status": "missing-in-source"
				},
				"test/bson/bson_test.js": {
					"match": false,
					"packageHash": "9a3a38e24519a9ef4fe8bddd7064c14818da077820870098d1ebf8dfeaf6f118",
					"size": 72340,
					"status": "missing-in-source"
				},
				"test/bson/commands_test.js": {
					"match": false,
					"packageHash": "20d7c9a17678dee1d46f05ce9987681d62d9614b0f30421a07982c449af10a6d",
					"size": 4899,
					"status": "missing-in-source"
				},
				"test/certificates/mycert.pem": {
					"match": false,
					"packageHash": "b58108b5dd45b1d0779b402004309b7c7f2cbd1a525f9611770079754a4f9697",
					"size": 2021,
					"status": "missing-in-source"
				},
				"test/collection_test.js": {
					"match": false,
					"packageHash": "2692c40ce81ff424fba71e3acf85f1ae8c3e0ec239ba20d3abd025853c2457bc",
					"size": 23859,
					"status": "missing-in-source"
				},
				"test/connect_test.js": {
					"match": false,
					"packageHash": "40397edea1be8a667a79b8813d2cd2ad778e4014ee4a91bd5dab6b46d8fb4fc6",
					"size": 3193,
					"status": "missing-in-source"
				},
				"test/connection/connection_pool_test.js": {
					"match": false,
					"packageHash": "c602bd0cdd9640ba1f38c30ea127a0b19ca84010b115c3e35440e2b52c90f51e",
					"size": 2916,
					"status": "missing-in-source"
				},
				"test/connection/message_parser_test.js": {
					"match": false,
					"packageHash": "fb7aa246d527e4c951a5530ff37fe99f17cfedaa93298e66f984790d8925c583",
					"size": 14647,
					"status": "missing-in-source"
				},
				"test/connection_test.js": {
					"match": false,
					"packageHash": "92d2ca4b6a9c6448a4275c4a1c53e9364acc5c3c345c34c0fce65d81d72643c1",
					"size": 5490,
					"status": "missing-in-source"
				},
				"test/cursor_test.js": {
					"match": false,
					"packageHash": "22a667397024d2a0ea7bd3b70055790042340a8b8b8f4af6aa737ac85ee097f8",
					"size": 42108,
					"status": "missing-in-source"
				},
				"test/custom_pk_test.js": {
					"match": false,
					"packageHash": "6a4681636770d9c833824de1cbecf21f67dceb6f5827ef954c66152713d28634",
					"size": 3205,
					"status": "missing-in-source"
				},
				"test/db_test.js": {
					"match": false,
					"packageHash": "f9074ec559d676751ad19549269f2963e9d20356260a90c28c0eddd04112b461",
					"size": 13077,
					"status": "missing-in-source"
				},
				"test/error_test.js": {
					"match": false,
					"packageHash": "efa1e4f376aa9bb874f8072fbd30cd3d6cddd7e4c6f85a52211fa216f9fcd267",
					"size": 12449,
					"status": "missing-in-source"
				},
				"test/exception_handling_test.js": {
					"match": false,
					"packageHash": "ee89953331f01eb62dbc9005cf5585a6e364050550ff3a4ec6da5ad9bea2dbf2",
					"size": 2928,
					"status": "missing-in-source"
				},
				"test/find_test.js": {
					"match": false,
					"packageHash": "3d7ca239e90579704a0d6af6018b0b10e600f9b27744278ad5e65463f77157a7",
					"size": 45570,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_file_test.js": {
					"match": false,
					"packageHash": "bfb81d7cd164aadfa43901282ed1da9f6f74b4eda46663d80f66c1f1e11b1764",
					"size": 26672,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_stream_test.js": {
					"match": false,
					"packageHash": "229c062b3dd0ea3a0f51e978db2e4485edc2c6039e6653058271a5d0f7a44a35",
					"size": 8157,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_store_test.js": {
					"match": false,
					"packageHash": "3f215f348a17c95e169231be2b8397adda83a27f8ea8adb1bf902ab08db0c279",
					"size": 24661,
					"status": "missing-in-source"
				},
				"test/gridstore/grid_test.js": {
					"match": false,
					"packageHash": "2470912bbebb677fdfb8dde81ef636c7a85d69d9fec7d10b6f8c4bafb012fd91",
					"size": 3410,
					"status": "missing-in-source"
				},
				"test/gridstore/iya_logo_final_bw.jpg": {
					"match": false,
					"packageHash": "fee907eb6666fc164dd837943ef9317b17ef74d8fc7580ef9d8d450a4998a4ce",
					"size": 74008,
					"status": "missing-in-source"
				},
				"test/gridstore/test_gs_weird_bug.png": {
					"match": false,
					"packageHash": "357ae8923961baeaf8eb1d434f5a3f8d9ac51ae0dbb2ed412e2687615f757ebc",
					"size": 52184,
					"status": "missing-in-source"
				},
				"test/gridstore/test_gs_working_field_read.pdf": {
					"match": false,
					"packageHash": "c866f44c43c7554e4b25394fa52901ee8c8c176649d4cd7f4dd4e92e8d4bd4f8",
					"size": 130253,
					"status": "missing-in-source"
				},
				"test/index_test.js": {
					"match": false,
					"packageHash": "21aa41b271f96d1a90f8c6a18b087e95e9accb202647a838844024aacb647662",
					"size": 13117,
					"status": "missing-in-source"
				},
				"test/insert_test.js": {
					"match": false,
					"packageHash": "d046c0e2714d9f11ae0a6d257a018aacf483670978612a26d5805392177dbcfa",
					"size": 32254,
					"status": "missing-in-source"
				},
				"test/logging_test.js": {
					"match": false,
					"packageHash": "8b4952136f09e5d8a3e999c4102889992e307566ab0b0ec630bd1eb7be10bdb9",
					"size": 2626,
					"status": "missing-in-source"
				},
				"test/manual_tests/replicaset_test.js": {
					"match": false,
					"packageHash": "75aede839390a0e5ba582fa25b8e645f14d8ca04227366e7396ce4f36f82a4b2",
					"size": 1918,
					"status": "missing-in-source"
				},
				"test/manual_tests/server_load.js": {
					"match": false,
					"packageHash": "605fc5914779f8e5974f5f2443f9a04dace10eeaf7a9718f8c81de8b4859b843",
					"size": 3759,
					"status": "missing-in-source"
				},
				"test/manual_tests/single_test.js": {
					"match": false,
					"packageHash": "7cd2977bf2a07d47114b203a72da79bdfcfc9d21a557441059a319b13a42c547",
					"size": 1517,
					"status": "missing-in-source"
				},
				"test/manual_tests/test.js": {
					"match": false,
					"packageHash": "31bf1ee11ead9bf0b80c78a48d13780a13052511764e06f952237355237b0268",
					"size": 2338,
					"status": "missing-in-source"
				},
				"test/map_reduce_test.js": {
					"match": false,
					"packageHash": "89c73406f102104de90c5cac263317df7294fcefe0e6b7406d494c20a3eb94c0",
					"size": 14466,
					"status": "missing-in-source"
				},
				"test/multiple_dbs_on_connection_pool_test.js": {
					"match": false,
					"packageHash": "0f49e29f6943b92e3f356a94880d9773d0c060fb8b2c96b1c12529f7bbb6507e",
					"size": 7317,
					"status": "missing-in-source"
				},
				"test/objectid_test.js": {
					"match": false,
					"packageHash": "07692c6fc0a5ff604c81771403aa78060c06f2c4b51e10e34ea2b05232b86d56",
					"size": 7025,
					"status": "missing-in-source"
				},
				"test/raw_test.js": {
					"match": false,
					"packageHash": "3216c7ca45de51e0f5d4068407c608aefe3ae68e25b4350fe932acfaa17995e9",
					"size": 15392,
					"status": "missing-in-source"
				},
				"test/regexp_test.js": {
					"match": false,
					"packageHash": "fa917602cd4f8071cd17b595da6e3405acc6728b1f6ca778a666927065f2a653",
					"size": 3958,
					"status": "missing-in-source"
				},
				"test/remove_test.js": {
					"match": false,
					"packageHash": "0de1fd452e0c9cb024b08280026c68737eb68a9f04e387927ad15018de4193fc",
					"size": 2761,
					"status": "missing-in-source"
				},
				"test/replicaset/connect_test.js": {
					"match": false,
					"packageHash": "0ee6b9a58c9a7ee75f7c02c78626a09bff796392fad63f00ca9413dc99ba3a40",
					"size": 12328,
					"status": "missing-in-source"
				},
				"test/replicaset/count_test.js": {
					"match": false,
					"packageHash": "b96251de74b3178d90ca2d7d82fd0b8512ee2dd350ab3a4ffa813ed3e83f571e",
					"size": 6007,
					"status": "missing-in-source"
				},
				"test/replicaset/insert_test.js": {
					"match": false,
					"packageHash": "208150fe11a763c8edf545a0dd2381eea27910e1aab285e8a1d3dad2864e1f4b",
					"size": 20042,
					"status": "missing-in-source"
				},
				"test/replicaset/query_secondaries_test.js": {
					"match": false,
					"packageHash": "ae18b077ff976a5133f2021822f4b6421c850b1be39da61a4e254eba8c1faa64",
					"size": 8413,
					"status": "missing-in-source"
				},
				"test/replicaset/tags_test.js": {
					"match": false,
					"packageHash": "df48047a5d799c7fe9236c809802b8e160c964b066c33fbe043ec17f9d3b142b",
					"size": 13136,
					"status": "missing-in-source"
				},
				"test/replicaset/two_server_tests.js": {
					"match": false,
					"packageHash": "7d7d976f7448080a5d2a60dcfbe541ee2c1c5c7b7f9f5935cb33d49e85cf2f4e",
					"size": 4107,
					"status": "missing-in-source"
				},
				"test/streaming_test.js": {
					"match": false,
					"packageHash": "4e785f189c541bb7c6387bfd51d3ef787285bede62a6a772c8b5e45088ebff7b",
					"size": 4330,
					"status": "missing-in-source"
				},
				"test/tools/keyfile.txt": {
					"match": false,
					"packageHash": "58800aef534e5a1004302db4a5caa14a1e2310829f363d4d210a93dabe2e214c",
					"size": 53,
					"status": "missing-in-source"
				},
				"test/tools/replica_set_manager.js": {
					"match": false,
					"packageHash": "9ad8d3b167c11aca45a846a22b69e4c0938eca69e8b9659a8569ed6233519560",
					"size": 20199,
					"status": "missing-in-source"
				},
				"test/tools/server_manager.js": {
					"match": false,
					"packageHash": "764ad772ef25961a614c7addf68dedd541589a1135f407187129303a813313c6",
					"size": 5083,
					"status": "missing-in-source"
				},
				"test/tools/sharding_manager.js": {
					"match": false,
					"packageHash": "09babdb7eb46bbbca96bd44ad83eed0572ff11b23c47c9b87bd59fb3ba4ab454",
					"size": 5544,
					"status": "missing-in-source"
				},
				"test/unicode_test.js": {
					"match": false,
					"packageHash": "e67692189a9cdc9429dffa0d8603da17747461fc1ecaea9a03010f71584be0bf",
					"size": 6233,
					"status": "missing-in-source"
				},
				"tools/gleak.js": {
					"match": false,
					"packageHash": "987c7d623f4141cee43bf344c46bdcd6f69cc4610228b4f5b520c95cabea0d4e",
					"size": 98,
					"status": "missing-in-source"
				},
				"tools/test_all.js": {
					"match": false,
					"packageHash": "bbbf0cab776a4e184d0c593916a1507da52bde03ee5a7471fb869c9c91b425e2",
					"size": 5306,
					"status": "missing-in-source"
				},
				"tools/test_set_runner.js": {
					"match": false,
					"packageHash": "e6b06af0a6d4e44fb00087c144e447da6291fb67c424735e1f7d6229b741a6e3",
					"size": 907,
					"status": "missing-in-source"
				},
				".travis.yml": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/connection/repl_set.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/gridfs/readstream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/mongodb/utils.js": {
					"match": false,
					"status": "missing-in-package"
				}
			},
			"summary": {
				"differentFiles": 29,
				"matchingFiles": 5,
				"missingInPackage": 4,
				"missingInSource": 174,
				"score": 0.02358490566037736,
				"totalFiles": 212
			}
		}
	}
]
