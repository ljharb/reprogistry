[
	{
		"reproduceVersion": "0.0.0-local",
		"timestamp": "2026-01-22T07:58:46.259Z",
		"os": "linux",
		"arch": "x64",
		"strategy": "npm:11.8.0",
		"reproduced": false,
		"attested": false,
		"package": {
			"spec": "kafka-node@0.2.9",
			"name": "kafka-node",
			"version": "0.2.9",
			"location": "https://registry.npmjs.org/kafka-node/-/kafka-node-0.2.9.tgz",
			"integrity": "sha512-VQSQFP2zkEPH0364sZpNN+faq9ox9v0qWLnlNOwb+2AXhvsyqty18j10h9cwSRVznwS63LAIgYgarm5pwaMb+Q==",
			"publishedAt": "2014-09-13T03:26:29.775Z",
			"publishedWith": {
				"node": null,
				"npm": "1.3.24"
			},
			"dependencies": {
				"buffermaker": "1.2.0",
				"binary": "~0.3.0",
				"buffer-crc32": "~0.2.1",
				"node-zookeeper-client": "0.2.0",
				"async": "0.7.0",
				"node-uuid": "1.4.1",
				"lodash": "~2.2.1"
			}
		},
		"source": {
			"integrity": null,
			"location": "git@github.com:SOHU-Co/kafka-node.git",
			"spec": "github:SOHU-Co/kafka-node#HEAD"
		},
		"comparisonHash": "9ef46a5bd31291e57cbab41e2f17c57bd6da0b5b",
		"diff": {
			"files": {
				".npmignore": {
					"match": false,
					"packageHash": "6bc6ea99584ece733dbc7c6f423dc63b281402ef037e604fbad72852dd867530",
					"size": 50,
					"status": "missing-in-source"
				},
				"DOCS.md": {
					"match": false,
					"packageHash": "5fe07ffb43a5c269dde0132b1f660e93000485fcbb47da37175e1224c1a94570",
					"size": 614,
					"status": "missing-in-source"
				},
				"LICENSE": {
					"diff": "--- published/LICENSE\n+++ rebuilt/LICENSE\n@@ -1,4 +1,4 @@\n-Copyright (c) 2013 sohu.com\n+Copyright (c) 2015 sohu.com\n \n Permission is hereby granted, free of charge, to any person obtaining a copy of\n this software and associated documentation files (the \"Software\"), to deal in\n",
					"match": false,
					"packageHash": "71613e0841659903a38fcbf52cc2e7da47de5319a1a5322d669590fef244f2fe",
					"size": 1053,
					"sourceHash": "000ed7f0afc107b5ae976477cfd2e6e66f114da9f168bd455df65e74c0693256",
					"status": "content"
				},
				"Makefile": {
					"match": false,
					"packageHash": "c183e6591c8ab6b6548c9a78dfb19179d4e20613fd303870c70bce6db99b06f8",
					"size": 293,
					"status": "missing-in-source"
				},
				"README.md": {
					"diff": "--- published/README.md\n+++ rebuilt/README.md\n@@ -1,113 +1,250 @@\n Kafka-node\n ==========\n \n-Kafka-node is a nodejs client with Zookeeper integration for apache Kafka. It only supports the latest version of Kafka 0.8 which is still under development, so this module\n-is _not production ready_ so far.\n+[![Build Status](https://travis-ci.org/SOHU-Co/kafka-node.svg?branch=master)](https://travis-ci.org/SOHU-Co/kafka-node)\n+[![Coverage Status](https://coveralls.io/repos/github/SOHU-Co/kafka-node/badge.svg?branch=master)](https://coveralls.io/github/SOHU-Co/kafka-node?branch=master)\n \n-The Zookeeper integration does the following jobs:\n+[![NPM](https://nodei.co/npm/kafka-node.png)](https://nodei.co/npm/kafka-node/)\n+<!--[![NPM](https://nodei.co/npm-dl/kafka-node.png?height=3)](https://nodei.co/npm/kafka-node/)-->\n \n-* Loads broker metadata from Zookeeper before we can communicate with the Kafka server\n-* Watches broker state, if broker changes, the client will refresh broker and topic metadata stored in the client\n+\n+Kafka-node is a Node.js client for Apache Kafka 0.9 and later.\n+\n+# Table of Contents\n+<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n+<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n+\n+\n+- [Features](#features)\n+- [Install Kafka](#install-kafka)\n+- [API](#api)\n+  - [KafkaClient](#kafkaclient)\n+  - [Producer](#producer)\n+  - [HighLevelProducer](#highlevelproducer)\n+  - [ProducerStream](#producerstream)\n+  - [Consumer](#consumer)\n+  - [ConsumerStream](#consumerstream)\n+  - [ConsumerGroup](#consumergroup)\n+  - [ConsumerGroupStream](#consumergroupstream)\n+  - [Offset](#offset)\n+  - [Admin](#admin)\n+- [Troubleshooting / FAQ](#troubleshooting--faq)\n+  - [HighLevelProducer with KeyedPartitioner errors on first send](#highlevelproducer-with-keyedpartitioner-errors-on-first-send)\n+  - [How do I debug an issue?](#how-do-i-debug-an-issue)\n+  - [For a new consumer how do I start consuming from the latest message in a partition?](#for-a-new-consumer-how-do-i-start-consuming-from-the-latest-message-in-a-partition)\n+  - [ConsumerGroup does not consume on all partitions](#consumergroup-does-not-consume-on-all-partitions)\n+  - [How to throttle messages / control the concurrency of processing messages](#how-to-throttle-messages--control-the-concurrency-of-processing-messages)\n+  - [How do I produce and consume binary data?](#how-do-i-produce-and-consume-binary-data)\n+  - [What are these node-gyp and snappy errors?](#what-are-these-node-gyp-and-snappy-errors)\n+  - [How do I configure the log output?](#how-do-i-configure-the-log-output)\n+  - [Error: Not a message set. Magic byte is 2](#error-not-a-message-set-magic-byte-is-2)\n+- [Running Tests](#running-tests)\n+- [LICENSE - \"MIT\"](#license---mit)\n+\n+<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n+\n+# Features\n+* Consumer\n+* Producer and High Level Producer\n+* Node Stream Producer (Kafka 0.9+)\n+* Node Stream Consumers (ConsumerGroupStream Kafka 0.9+)\n+* Manage topic Offsets\n+* SSL connections to brokers (Kafka 0.9+)\n+* SASL/PLAIN Authentication (Kafka 0.10+)\n+* Consumer Groups managed by Kafka coordinator (Kafka 0.9+)\n+* Connect directly to brokers (Kafka 0.9+)\n+* Administrative APIs\n+\t* List Groups\n+\t* Describe Groups\n+\t* Create Topics\n \n # Install Kafka\n-Follow the [instructions](https://cwiki.apache.org/KAFKA/kafka-08-quick-start.html) on the Kafka wiki to build Kafka 0.8 and get a test broker up and running.\n+Follow the [instructions](http://kafka.apache.org/documentation.html#quickstart) on the Kafka wiki to build Kafka and get a test broker up and running.\n \n # API\n-## Client\n-### Client(connectionString, clientId, [zkOptions])\n-* `connectionString`: Zookeeper connection string, default `localhost:2181/kafka0.8`\n-* `clientId`: This is a user supplied identifier for the client application, default `kafka-node-client`\n-* `zkOptions`: **Object**, Zookeeper options, see [node-zookeeper-client](https://github.com/alexguan/node-zookeeper-client#client-createclientconnectionstring-options)\n \n-### close(cb)\n-Closes the connection to Zookeeper and the brokers so that the node process can exit gracefully.\n+## KafkaClient\n \n-* `cb`: **Function**, the callback\n+New KafkaClient connects directly to Kafka brokers.\n+\n+### Options\n+* `kafkaHost` : A string of kafka broker/host combination delimited by comma for example: `kafka-1.us-east-1.myapp.com:9093,kafka-2.us-east-1.myapp.com:9093,kafka-3.us-east-1.myapp.com:9093` default: `localhost:9092`.\n+* `connectTimeout` : in ms it takes to wait for a successful connection before moving to the next host default: `10000`\n+* `requestTimeout` : in ms for a kafka request to timeout default: `30000`\n+* `autoConnect` : automatically connect when KafkaClient is instantiated otherwise you need to manually call `connect` default: `true`\n+* `connectRetryOptions` : object hash that applies to the initial connection. see [retry](https://www.npmjs.com/package/retry) module for these options.\n+* `idleConnection` : allows the broker to disconnect an idle connection from a client (otherwise the clients continues to O after being disconnected). The value is elapsed time in ms without any data written to the TCP socket. default: 5 minutes\n+* `reconnectOnIdle` : when the connection is closed due to client idling, client will attempt to auto-reconnect. default: true\n+* `maxAsyncRequests` : maximum async operations at a time toward the kafka cluster. default: 10\n+* `sslOptions`: **Object**, options to be passed to the tls broker sockets, ex. `{ rejectUnauthorized: false }` (Kafka 0.9+)\n+* `sasl`: **Object**, SASL authentication configuration (only SASL/PLAIN is currently supported), ex. `{ mechanism: 'plain', username: 'foo', password: 'bar' }` (Kafka 0.10+)\n+\n+### Example\n+\n",
					"match": false,
					"packageHash": "4d4e0e2719e24b4170f5e5ea1f6c5ddd3bff4bc2b6c136d17aedd0bd85030203",
					"size": 14507,
					"sourceHash": "88b40c708719286db4beb3724f5cc971d25de427a84c6d5f36e22ffa09313947",
					"status": "content"
				},
				"example/consumer.js": {
					"match": false,
					"packageHash": "144f2c68e598ef072ac86f9a0d26626d3d7205f1b414db19881dc2ce5db1afdc",
					"size": 1112,
					"status": "missing-in-source"
				},
				"example/high-level-consumer.js": {
					"match": false,
					"packageHash": "b06682f8ac25009b16d90c53514289fb18e356bf918a65a10f547c87a4589af3",
					"size": 923,
					"status": "missing-in-source"
				},
				"example/high-level-producer.js": {
					"match": false,
					"packageHash": "33ef645738b9e789d7fdd91068f821be4e1d71a731236d4f25562f19d7977705",
					"size": 645,
					"status": "missing-in-source"
				},
				"example/metadata.js": {
					"match": false,
					"packageHash": "6e63ab737740def96fee51d0056a93e77903e8072e9c1d6307a950d12a43b47c",
					"size": 611,
					"status": "missing-in-source"
				},
				"example/offset.js": {
					"match": false,
					"packageHash": "1b557c65f998f4084bfc80f43078c6d80ca6d3ed52ea4f68f29dbbdcd98df0cc",
					"size": 801,
					"status": "missing-in-source"
				},
				"example/producer.js": {
					"match": false,
					"packageHash": "990950c979f4c6a424c5fe3b662c569e14300d47f169852c30c96aba29cb0ecd",
					"size": 647,
					"status": "missing-in-source"
				},
				"example/topics.js": {
					"match": false,
					"packageHash": "6620d11437525c571074655198230c0c341cf3cdc448892a3c0c03f37ab826fa",
					"size": 1116,
					"status": "missing-in-source"
				},
				"kafka.js": {
					"diff": "--- published/kafka.js\n+++ rebuilt/kafka.js\n@@ -1,6 +1,18 @@\n-exports.HighLevelConsumer = require('./lib/highLevelConsumer');\n+'use strict';\n+\n exports.HighLevelProducer = require('./lib/highLevelProducer');\n+exports.ProducerStream = require('./lib/producerStream');\n+exports.ConsumerGroup = require('./lib/consumerGroup');\n+exports.ConsumerGroupStream = require('./lib/consumerGroupStream');\n exports.Consumer = require('./lib/consumer');\n+exports.ConsumerStream = require('./lib/consumerStream');\n exports.Producer = require('./lib/producer');\n-exports.Client = require('./lib/client')\n-exports.Offset = require('./lib/offset')\n+exports.KafkaClient = require('./lib/kafkaClient');\n+exports.Offset = require('./lib/offset');\n+exports.Admin = require('./lib/admin');\n+exports.KeyedMessage = require('./lib/protocol').KeyedMessage;\n+exports.DefaultPartitioner = require('./lib/partitioner').DefaultPartitioner;\n+exports.CyclicPartitioner = require('./lib/partitioner').CyclicPartitioner;\n+exports.RandomPartitioner = require('./lib/partitioner').RandomPartitioner;\n+exports.KeyedPartitioner = require('./lib/partitioner').KeyedPartitioner;\n+exports.CustomPartitioner = require('./lib/partitioner').CustomPartitioner;\n",
					"match": false,
					"packageHash": "2601f8a1dee44afaed4f6c9986cea455feef1957e231c2161ad0ec64ad6bd7a2",
					"size": 302,
					"sourceHash": "8b58de328b077ed1cc6f01e2ef8fee7f6ac41e28e26054e64a0b7fa1c39bafe8",
					"status": "content"
				},
				"lib/cache.js": {
					"match": false,
					"packageHash": "849f9c1c4f1e40391f707f8862dff6cc03444fe44bd499f71136be0bff2a3c3c",
					"size": 1128,
					"status": "missing-in-source"
				},
				"lib/client.js": {
					"match": false,
					"packageHash": "d55e56d7206a5fe42bf3ea033003be9b3958874ee9da4dc4b8e4a83636720824",
					"size": 16747,
					"status": "missing-in-source"
				},
				"lib/consumer.js": {
					"diff": "--- published/lib/consumer.js\n+++ rebuilt/lib/consumer.js\n@@ -1,122 +1,134 @@\n 'use strict';\n \n-var util = require('util'),\n-    _ = require('lodash'),\n-    events = require('events'),\n-    Client = require('./client'),\n-    protocol = require('./protocol'),\n-    Offset = require('./offset'),\n-    errors = require('./errors')\n+var util = require('util');\n+var _ = require('lodash');\n+var EventEmitter = require('events');\n+var logger = require('./logging')('kafka-node:Consumer');\n+var utils = require('./utils');\n \n var DEFAULTS = {\n-    groupId: 'kafka-node-group',\n-    // Auto commit config\n-    autoCommit: true,\n-    autoCommitMsgCount: 100,\n-    autoCommitIntervalMs: 5000,\n-    // Fetch message config\n-    fetchMaxWaitMs: 100,\n-    fetchMinBytes: 1,\n-    fetchMaxBytes: 1024 * 1024,\n-    fromOffset: false\n+  groupId: 'kafka-node-group',\n+  // Auto commit config\n+  autoCommit: true,\n+  autoCommitIntervalMs: 5000,\n+  // Fetch message config\n+  fetchMaxWaitMs: 100,\n+  fetchMinBytes: 1,\n+  fetchMaxBytes: 1024 * 1024,\n+  fromOffset: false,\n+  encoding: 'utf8'\n };\n \n var nextId = (function () {\n-    var id = 0;\n-    return function () {\n-        return id++;\n-    }\n+  var id = 0;\n+  return function () {\n+    return id++;\n+  };\n })();\n \n-var Consumer = function (client, topics, options) {\n-    if (!topics) {\n-        throw new Error('Must have payloads');\n-    }\n-\n-    this.fetchCount = 0;\n-    this.client = client;\n-    this.options = _.defaults( (options||{}), DEFAULTS );\n-    this.ready = false;\n-    this.id = nextId();\n-    this.payloads = this.buildPayloads(topics);\n-    this.connect();\n+function Consumer (client, topics, options) {\n+  EventEmitter.call(this);\n+  if (!topics) {\n+    throw new Error('Must have payloads');\n+  }\n+\n+  utils.validateTopics(topics);\n+\n+  this.fetchCount = 0;\n+  this.client = client;\n+  this.options = _.defaults((options || {}), DEFAULTS);\n+  this.ready = false;\n+  this.paused = this.options.paused;\n+  this.id = nextId();\n+  this.payloads = this.buildPayloads(topics);\n+  this.connect();\n+  this.encoding = this.options.encoding;\n+\n+  if (this.options.groupId) {\n+    utils.validateConfig('options.groupId', this.options.groupId);\n+  }\n }\n-util.inherits(Consumer, events.EventEmitter);\n+util.inherits(Consumer, EventEmitter);\n \n Consumer.prototype.buildPayloads = function (payloads) {\n-    var self = this;\n-    return payloads.map(function (p) {\n-        if (typeof p !== 'object') p = { topic: p };\n-        p.partition = p.partition || 0;\n-        p.offset = p.offset || 0;\n-        p.maxBytes = self.options.fetchMaxBytes;\n-        p.metadata = 'm'; // metadata can be arbitrary\n-        return p;\n-    });\n-}\n",
					"match": false,
					"packageHash": "4acb8124d9b46339341b60b961550f73376faffbf58204d0329855f45492b1a1",
					"size": 6936,
					"sourceHash": "2757735c81b294e403f90041381e1208614094df9ac1bff3532241c4206e8f6b",
					"status": "content"
				},
				"lib/errors/BrokerNotAvailableError.js": {
					"diff": "--- published/lib/errors/BrokerNotAvailableError.js\n+++ rebuilt/lib/errors/BrokerNotAvailableError.js\n@@ -1,4 +1,4 @@\n-var util = require('util')\n+var util = require('util');\n \n /**\n  * A broker/leader was not available or discoverable for the action requested\n@@ -8,11 +8,11 @@\n  * @constructor\n  */\n var BrokerNotAvailableError = function (message) {\n-    Error.captureStackTrace(this, this)\n-    this.message = message\n-}\n+  Error.captureStackTrace(this, this);\n+  this.message = message;\n+};\n \n-util.inherits(BrokerNotAvailableError, Error)\n-BrokerNotAvailableError.prototype.name = 'BrokerNotAvailableError'\n+util.inherits(BrokerNotAvailableError, Error);\n+BrokerNotAvailableError.prototype.name = 'BrokerNotAvailableError';\n \n-module.exports = BrokerNotAvailableError\n\\ No newline at end of file\n+module.exports = BrokerNotAvailableError;\n",
					"match": false,
					"packageHash": "dcdc331e41219cb019ea2c14215a1dedaa4045afc10c18823aee18924e2eb198",
					"size": 485,
					"sourceHash": "ae69522299bfce80bae04eb74cde7f3e78e3259c9141413aab5015f9ff2a1d16",
					"status": "content"
				},
				"lib/errors/FailedToRebalanceConsumerError.js": {
					"diff": "--- published/lib/errors/FailedToRebalanceConsumerError.js\n+++ rebuilt/lib/errors/FailedToRebalanceConsumerError.js\n@@ -1,4 +1,4 @@\n-var util = require('util')\n+var util = require('util');\n \n /**\n  * Failed to rebalance the consumer\n@@ -8,11 +8,11 @@\n  * @constructor\n  */\n var FailedToRebalanceConsumerError = function (message) {\n-    Error.captureStackTrace(this, this)\n-    this.message = message\n-}\n+  Error.captureStackTrace(this, this);\n+  this.message = message;\n+};\n \n-util.inherits(FailedToRebalanceConsumerError, Error)\n-FailedToRebalanceConsumerError.prototype.name = 'FailedToRebalanceConsumerError'\n+util.inherits(FailedToRebalanceConsumerError, Error);\n+FailedToRebalanceConsumerError.prototype.name = 'FailedToRebalanceConsumerError';\n \n-module.exports = FailedToRebalanceConsumerError\n\\ No newline at end of file\n+module.exports = FailedToRebalanceConsumerError;\n",
					"match": false,
					"packageHash": "52e717b6729444e3ef9a7ae5e19af92b1206342cbeb50570b3c347a536c95d28",
					"size": 497,
					"sourceHash": "76ba2b45b0ce57e6396ee3cc6dafba202bd23ea4785cbdb5b0f92098d6d3b516",
					"status": "content"
				},
				"lib/errors/FailedToRegisterConsumerError.js": {
					"diff": "--- published/lib/errors/FailedToRegisterConsumerError.js\n+++ rebuilt/lib/errors/FailedToRegisterConsumerError.js\n@@ -1,18 +1,20 @@\n-var util = require('util')\n+var util = require('util');\n+var NestedError = require('nested-error-stacks');\n \n /**\n  * Failed to register the consumer\n  *\n  * @param {String} message A message describing the problem with the registration of the consumer\n+ * @param {Error} error An error related to the registration of the consumer\n  *\n  * @constructor\n  */\n-var FailedToRegisterConsumerError = function (message) {\n-    Error.captureStackTrace(this, this)\n-    this.message = message\n-}\n+var FailedToRegisterConsumerError = function (message, nested) {\n+  NestedError.call(this, message, nested);\n+  this.message = message;\n+};\n \n-util.inherits(FailedToRegisterConsumerError, Error)\n-FailedToRegisterConsumerError.prototype.name = 'FailedToRegisterConsumerError'\n+util.inherits(FailedToRegisterConsumerError, NestedError);\n+FailedToRegisterConsumerError.prototype.name = 'FailedToRegisterConsumerError';\n \n-module.exports = FailedToRegisterConsumerError\n\\ No newline at end of file\n+module.exports = FailedToRegisterConsumerError;\n",
					"match": false,
					"packageHash": "9369dfafe452fecdf1bf40ef6210c3acd5df56bf3d0dcb7c6e1da26e2ae8c157",
					"size": 496,
					"sourceHash": "e638c719a9796776be91a6a3cc7f4fb95898daf403c18ce327bd409ca3da9309",
					"status": "content"
				},
				"lib/errors/InvalidConsumerOffsetError.js": {
					"diff": "--- published/lib/errors/InvalidConsumerOffsetError.js\n+++ rebuilt/lib/errors/InvalidConsumerOffsetError.js\n@@ -1,4 +1,7 @@\n-var util = require('util')\n+'use strict';\n+\n+const util = require('util');\n+const NestedError = require('nested-error-stacks');\n \n /**\n  * The offset for the comsumer is invalid\n@@ -7,12 +10,11 @@\n  *\n  * @constructor\n  */\n-var InvalidConsumerOffsetError = function (message) {\n-    Error.captureStackTrace(this, this)\n-    this.message = message\n-}\n+const InvalidConsumerOffsetError = function (message, nested) {\n+  NestedError.apply(this, arguments);\n+};\n \n-util.inherits(InvalidConsumerOffsetError, Error)\n-InvalidConsumerOffsetError.prototype.name = 'InvalidConsumerOffsetError'\n+util.inherits(InvalidConsumerOffsetError, NestedError);\n+InvalidConsumerOffsetError.prototype.name = 'InvalidConsumerOffsetError';\n \n-module.exports = InvalidConsumerOffsetError\n\\ No newline at end of file\n+module.exports = InvalidConsumerOffsetError;\n",
					"match": false,
					"packageHash": "77c69b86a6aa2d4a26e2ce52e059b5b12d4372c14cce760ce1f9bd842dd319fc",
					"size": 496,
					"sourceHash": "b6c8d6cc5cfe9ab21276c7853cac4946fb766f6559bad83550931ffa3dcfe3a9",
					"status": "content"
				},
				"lib/errors/TopicsNotExistError.js": {
					"diff": "--- published/lib/errors/TopicsNotExistError.js\n+++ rebuilt/lib/errors/TopicsNotExistError.js\n@@ -1,4 +1,4 @@\n-var util = require('util')\n+var util = require('util');\n \n /**\n  * One or more topics did not exist for the requested action\n@@ -8,12 +8,12 @@\n  * @constructor\n  */\n var TopicsNotExistError = function (topics) {\n-    Error.captureStackTrace(this, this)\n-    this.topics = topics\n-    this.message = 'The topic(s) ' + topics.toString() + ' do not exist'\n-}\n+  Error.captureStackTrace(this, this);\n+  this.topics = topics;\n+  this.message = 'The topic(s) ' + topics.toString() + ' do not exist';\n+};\n \n-util.inherits(TopicsNotExistError, Error)\n-TopicsNotExistError.prototype.name = 'TopicsNotExistError'\n+util.inherits(TopicsNotExistError, Error);\n+TopicsNotExistError.prototype.name = 'TopicsNotExistError';\n \n-module.exports = TopicsNotExistError\n\\ No newline at end of file\n+module.exports = TopicsNotExistError;\n",
					"match": false,
					"packageHash": "cd0792e4b309e278a5bc50e2992e6e9d8b22379b20370ef6608f04837391c7b6",
					"size": 516,
					"sourceHash": "dfd2273a868ae14abb7010b79c13cac38e734eb611e9aadcf13ccd6e2a461701",
					"status": "content"
				},
				"lib/errors/index.js": {
					"diff": "--- published/lib/errors/index.js\n+++ rebuilt/lib/errors/index.js\n@@ -1,7 +1,20 @@\n module.exports = {\n-    BrokerNotAvailableError: require('./BrokerNotAvailableError'),\n-    TopicsNotExistError: require('./TopicsNotExistError'),\n-    FailedToRegisterConsumerError: require('./FailedToRegisterConsumerError'),\n-    InvalidConsumerOffsetError: require('./InvalidConsumerOffsetError'),\n-    FailedToRebalanceConsumerError: require('./FailedToRebalanceConsumerError')\n-}\n\\ No newline at end of file\n+  ApiNotSupportedError: require('./ApiNotSupportedError'),\n+  BrokerNotAvailableError: require('./BrokerNotAvailableError'),\n+  TopicsNotExistError: require('./TopicsNotExistError'),\n+  FailedToRegisterConsumerError: require('./FailedToRegisterConsumerError'),\n+  InvalidConsumerOffsetError: require('./InvalidConsumerOffsetError'),\n+  FailedToRebalanceConsumerError: require('./FailedToRebalanceConsumerError'),\n+  InvalidConfigError: require('./InvalidConfigError'),\n+  SaslAuthenticationError: require('./SaslAuthenticationError'),\n+  InvalidRequestError: require('./InvalidRequestError'),\n+  ConsumerGroupErrors: [\n+    require('./GroupCoordinatorNotAvailableError'),\n+    require('./GroupLoadInProgressError'),\n+    require('./HeartbeatTimeoutError'),\n+    require('./IllegalGenerationError'),\n+    require('./NotCoordinatorForGroupError'),\n+    require('./RebalanceInProgressError'),\n+    require('./UnknownMemberIdError')\n+  ]\n+};\n",
					"match": false,
					"packageHash": "00f073b9daceea4e0dbdbf4c0b32595e0e584ae80cfe67b34fef8bd3504badda",
					"size": 378,
					"sourceHash": "bf5c4f9787033ce9eecc916169e04ac93060d57326b5af0143b86ff64600a7aa",
					"status": "content"
				},
				"lib/highLevelConsumer.js": {
					"match": false,
					"packageHash": "25498c8ea77f312c1328989f12b233ec3941c99878e82cf0450d87364d050767",
					"size": 18308,
					"status": "missing-in-source"
				},
				"lib/highLevelProducer.js": {
					"diff": "--- published/lib/highLevelProducer.js\n+++ rebuilt/lib/highLevelProducer.js\n@@ -1,148 +1,15 @@\n 'use strict';\n \n-var util = require('util'),\n-    events = require('events'),\n-    _ = require('lodash'),\n-    Client = require('./client'),\n-    protocol = require('./protocol'),\n-    Message = protocol.Message,\n-    ProduceRequest = protocol.ProduceRequest,\n-    DEFAULTS = {\n-        requireAcks: 1,\n-        ackTimeoutMs: 100\n-    };\n+var util = require('util');\n+var BaseProducer = require('./baseProducer');\n \n-/**\n- * Provides common functionality for a kafka producer\n- *\n- * @param {Client} client A kafka client object to use for the producer\n- * @param {Object} [options] An object containing configuration options\n- * @param {Number} [options.requireAcks=1] Configuration for when to consider a message as acknowledged.\n- *      <li>0 = No ack required</li>\n- *      <li>1 = Leader ack required</li>\n- *      <li>-1 = All in sync replicas ack required</li>\n- *\n- * @param {Number} [options.ackTimeoutMs=100] The amount of time in milliseconds to wait for all acks before considered\n- *      the message as errored\n- *\n- * @constructor\n- */\n-var HighLevelProducer = function (client, options) {\n-    var useOptions = options || {};\n-\n-    this.ready = false;\n-    this.client = client;\n-\n-    this.requireAcks = useOptions.requireAcks || DEFAULTS.requireAcks\n-    this.ackTimeoutMs = useOptions.ackTimeoutMs || DEFAULTS.ackTimeoutMs\n-\n-    this.connect();\n-}\n-\n-util.inherits(HighLevelProducer, events.EventEmitter);\n-\n-HighLevelProducer.prototype.connect = function () {\n-    // emiter...\n-    var self = this;\n-    this.ready = this.client.ready;\n-    if (this.ready) self.emit('ready');\n-    this.client.on('ready', function () {\n-        if (!self.ready) self.emit('ready');\n-        self.ready = true;\n-    });\n-    this.client.on('error', function (err) {\n-    });\n-    this.client.on('close', function () {\n-    });\n-}\n-\n-/**\n- * Sends a new message or array of messages to a topic/partition\n- * This will use the\n- *\n- * @see Client#sendProduceRequest for a more low level way to send messages to kafka\n- *\n- * @param {Array.<HighLevelProducer~sendPayload>} payloads An array of topic payloads\n- * @param {HighLevelProducer~sendCallback} cb A function to call once the send has completed\n- */\n-HighLevelProducer.prototype.send = function (payloads, cb) {\n-    this.client.sendProduceRequest(this.buildPayloads(payloads), this.requireAcks, this.ackTimeoutMs, cb);\n+/** @inheritdoc */\n+function HighLevelProducer (client, options, customPartitioner) {\n+  BaseProducer.call(this, client, options, BaseProducer.PARTITIONER_TYPES.cyclic, customPartitioner);\n }\n \n-HighLevelProducer.prototype.buildPayloads = function (payloads) {\n-    var that = this;\n-    return payloads.map(function (p) {\n-        if (p.partition) p.partition = p.partition || 0;\n-        else p.partition = that.client.nextPartition(p.topic)\n-        var messages = _.isArray(p.messages) ? p.messages : [p.messages];\n-        messages = messages.map(function (message) {\n-            return new Message(0, 0, '', message);\n-        });\n-        return new ProduceRequest(p.topic, p.partition, messages);\n-    });\n-}\n-\n-HighLevelProducer.prototype.createTopics = function (topics, async, cb) {\n-    var self = this;\n-    if (!this.ready) {\n-        setTimeout(function () {\n-            self.createTopics(topics, async, cb);\n-        }, 100);\n-        return;\n-    }\n",
					"match": false,
					"packageHash": "d9dc79eec475351df67a23f4a6eb64bc1a8548c985a3728f980e8d1eeaee79a0",
					"size": 4795,
					"sourceHash": "fbb4c98daa74f111354670ac4c31473a930ee0c33aeed612381d334772f41150",
					"status": "content"
				},
				"lib/offset.js": {
					"diff": "--- published/lib/offset.js\n+++ rebuilt/lib/offset.js\n@@ -1,50 +1,125 @@\n 'use strict';\n \n-var Offset = function (client) {\n-    this.client = client;\n-    this.ready = this.client.ready;\n-    this.client.once('connect', function () {\n-        this.ready = true;\n-    }.bind(this));\n+var util = require('util');\n+var async = require('async');\n+var EventEmitter = require('events');\n+\n+function Offset (client) {\n+  EventEmitter.call(this);\n+  var self = this;\n+  this.client = client;\n+  this.ready = this.client.ready;\n+  this.client.on('ready', function () {\n+    self.ready = true;\n+    self.emit('ready');\n+  });\n+  this.client.once('connect', function () {\n+    self.emit('connect');\n+  });\n+  this.client.on('error', function (err) {\n+    self.emit('error', err);\n+  });\n }\n+util.inherits(Offset, EventEmitter);\n \n Offset.prototype.fetch = function (payloads, cb) {\n-    if (!this.ready) {\n-        setTimeout(function () {\n-            this.fetch(payloads, cb);\n-        }.bind(this), 100);\n-        return;\n-    }\n-    this.client.sendOffsetRequest(this.buildPayloads(payloads),cb);\n-}\n+  if (!this.ready) {\n+    this.once('ready', () => this.fetch(payloads, cb));\n+    return;\n+  }\n+  this.client.sendOffsetRequest(this.buildPayloads(payloads), cb);\n+};\n \n-Offset.prototype.buildPayloads = function (paylaods) {\n-    return paylaods.map(function (p) {\n-        p.partition = p.partition || 0;\n-        p.time = p.time || Date.now();\n-        p.maxNum = p.maxNum || 1;\n-        p.metadata = 'm'; // metadata can be arbitrary\n-        return p;\n-    });\n-}\n+Offset.prototype.buildPayloads = function (payloads) {\n+  return payloads.map(function (p) {\n+    p.partition = p.partition || 0;\n+    p.time = p.time || Date.now();\n+    p.maxNum = p.maxNum || 1;\n+    p.metadata = 'm'; // metadata can be arbitrary\n+    return p;\n+  });\n+};\n+\n+Offset.prototype.buildOffsetFetchV1Payloads = function (payloads) {\n+  return payloads.reduce(function (out, p) {\n+    out[p.topic] = out[p.topic] || [];\n+    out[p.topic].push(p.partition || 0);\n+    return out;\n+  }, {});\n+};\n \n Offset.prototype.commit = function (groupId, payloads, cb) {\n-    if (!this.ready) {\n-        setTimeout(function () {\n-            this.commit(groupId, payloads, cb);\n-        }.bind(this), 100);\n-        return;\n-    }\n-    this.client.sendOffsetCommitRequest(groupId, this.buildPayloads(payloads), cb);\n-}\n+  if (!this.ready) {\n+    this.once('ready', () => this.commit(groupId, payloads, cb));\n+    return;\n+  }\n+  this.client.sendOffsetCommitRequest(groupId, this.buildPayloads(payloads), cb);\n+};\n \n-Offset.prototype.fetchCommits = function (groupId, payloads, cb) {\n-    if (!this.ready) {\n-        setTimeout(function () {\n-            this.fetchCommits(groupId, payloads, cb);\n-        }.bind(this), 100);\n-        return;\n+Offset.prototype.fetchCommits = Offset.prototype.fetchCommitsV1 = function (groupId, payloads, cb) {\n+  if (!this.ready) {\n",
					"match": false,
					"packageHash": "e57fbdc7be901721264ecb7f4046f3deda8de663df42a921d38a9d6c19a07aac",
					"size": 1400,
					"sourceHash": "552ebd04d10c253281cdff7902bb9458862a9293bf24e571de2bea19f225769e",
					"status": "content"
				},
				"lib/producer.js": {
					"diff": "--- published/lib/producer.js\n+++ rebuilt/lib/producer.js\n@@ -1,143 +1,15 @@\n 'use strict';\n \n-var util = require('util'),\n-    events = require('events'),\n-    _ = require('lodash'),\n-    Client = require('./client'),\n-    protocol = require('./protocol'),\n-    Message = protocol.Message,\n-    ProduceRequest = protocol.ProduceRequest,\n-    DEFAULTS = {\n-        requireAcks: 1,\n-        ackTimeoutMs: 100\n-    };\n+var util = require('util');\n+var BaseProducer = require('./baseProducer');\n \n-/**\n- * Provides common functionality for a kafka producer\n- *\n- * @param {Client} client A kafka client object to use for the producer\n- * @param {Object} [options] An object containing configuration options\n- * @param {Number} [options.requireAcks=1] Configuration for when to consider a message as acknowledged.\n- *      <li>0 = No ack required</li>\n- *      <li>1 = Leader ack required</li>\n- *      <li>-1 = All in sync replicas ack required</li>\n- *\n- * @param {Number} [options.ackTimeoutMs=100] The amount of time in milliseconds to wait for all acks before considered\n- *      the message as errored\n- *\n- * @constructor\n- */\n-var Producer = function (client, options) {\n-    var useOptions = options || {};\n-\n-    this.ready = false;\n-    this.client = client;\n-\n-    this.requireAcks = useOptions.requireAcks || DEFAULTS.requireAcks\n-    this.ackTimeoutMs = useOptions.ackTimeoutMs || DEFAULTS.ackTimeoutMs\n-\n-    this.connect();\n-}\n-\n-util.inherits(Producer, events.EventEmitter);\n-\n-Producer.prototype.connect = function () {\n-    // emiter...\n-    var self = this;\n-    this.ready = this.client.ready;\n-    if (this.ready) self.emit('ready');\n-    this.client.on('ready', function () {\n-        if (!self.ready) self.emit('ready');\n-        self.ready = true;\n-    });\n-    this.client.on('error', function (err) {\n-    });\n-    this.client.on('close', function () {\n-    });\n+/** @inheritdoc */\n+function Producer (client, options, customPartitioner) {\n+  BaseProducer.call(this, client, options, BaseProducer.PARTITIONER_TYPES.default, customPartitioner);\n }\n \n-/**\n- * Sends a new message or array of messages to a topic/partition\n- * This will use the\n- *\n- * @see Client#sendProduceRequest for a more low level way to send messages to kafka\n- *\n- * @param {Array.<Producer~sendPayload>} payloads An array of topic payloads\n- * @param {Producer~sendCallback} cb A function to call once the send has completed\n- */\n-Producer.prototype.send = function (payloads, cb) {\n-    this.client.sendProduceRequest(this.buildPayloads(payloads), this.requireAcks, this.ackTimeoutMs, cb);\n-}\n+util.inherits(Producer, BaseProducer);\n \n-Producer.prototype.buildPayloads = function (payloads) {\n-    return payloads.map(function (p) {\n-        p.partition = p.partition || 0;\n-        var messages = _.isArray(p.messages) ? p.messages : [p.messages];\n-        messages = messages.map(function (message) {\n-            return new Message(0,0,'',message);\n-        });\n-        return new ProduceRequest(p.topic, p.partition, messages);\n-    });\n-}\n+Producer.PARTITIONER_TYPES = BaseProducer.PARTITIONER_TYPES;\n \n-Producer.prototype.createTopics = function (topics, async, cb) {\n-    var self = this;\n-    if (!this.ready) {\n-        setTimeout(function () {\n-            self.createTopics(topics, async, cb);\n-        }, 100);\n-        return;\n-    }\n",
					"match": false,
					"packageHash": "616e6c29431df0d989997107105275fd2069923132fe73d49cdf628904b99c9e",
					"size": 4652,
					"sourceHash": "df9bcfef0965a1901061252e2e3fabaad126c47d082d9748889792b9a58111f8",
					"status": "content"
				},
				"lib/protocol/index.js": {
					"diff": "--- published/lib/protocol/index.js\n+++ rebuilt/lib/protocol/index.js\n@@ -1,7 +1,7 @@\n 'use strict';\n \n-var _ = require('lodash'),\n-    struct = require('./protocol_struct'),\n-    protocol = require('./protocol');\n+var _ = require('lodash');\n+var struct = require('./protocol_struct');\n+var protocol = require('./protocol');\n \n exports = _.extend(exports, struct, protocol);\n",
					"match": false,
					"packageHash": "13282d155e1f330d07a11bca89cd626f1672f08ff35e3800a456f50f6e335ed0",
					"size": 171,
					"sourceHash": "3bbe67ae0be0177b2b98145f4f32a4aac6b60459813a7524ffbd7262bfb06f20",
					"status": "content"
				},
				"lib/protocol/protocol.js": {
					"diff": "--- published/lib/protocol/protocol.js\n+++ rebuilt/lib/protocol/protocol.js\n@@ -1,507 +1,1883 @@\n 'use strict';\n \n-var Binary = require('binary'),\n-    Buffermaker = require('buffermaker'),\n-    _  = require('lodash'),\n-    crc32 = require('buffer-crc32'),\n-    protocol = require('./protocol_struct'),\n-    KEYS = protocol.KEYS,\n-    REQUEST_TYPE = protocol.REQUEST_TYPE,\n-    ERROR_CODE = protocol.ERROR_CODE,\n-    FetchResponse = protocol.FetchResponse,\n-    PartitionMetadata = protocol.PartitionMetadata;\n-\n-var API_VERSION = 0,\n-    REPLICA_ID = -1,\n-    CODEC_NONE = 0,\n-    CODEC_GZIP = 1,\n-    CODEC_SNAPPY = 2;\n-\n-function groupByTopic(payloads) {\n-    return payloads.reduce(function (out, p) {\n-        out[p.topic] = out[p.topic] || {};\n-        out[p.topic][p.partition] = p;\n-        return out;\n-    }, {});\n-}\n-\n-function encodeRequestWithLength(request) {\n-    return new Buffermaker()\n-        .Int32BE(request.length)\n-        .string(request)\n+var Binary = require('binary');\n+var Buffermaker = require('buffermaker');\n+var _ = require('lodash');\n+var crc32 = require('buffer-crc32');\n+var protocol = require('./protocol_struct');\n+var getCodec = require('../codec');\n+var REQUEST_TYPE = protocol.REQUEST_TYPE;\n+var ERROR_CODE = protocol.ERROR_CODE;\n+var GROUP_ERROR = protocol.GROUP_ERROR;\n+var PartitionMetadata = protocol.PartitionMetadata;\n+const API_KEY_TO_NAME = _.invert(REQUEST_TYPE);\n+const MessageSizeTooLarge = require('../errors/MessageSizeTooLargeError');\n+const SaslAuthenticationError = require('../errors/SaslAuthenticationError');\n+const InvalidRequestError = require('../errors/InvalidRequestError');\n+const async = require('async');\n+\n+var API_VERSION = 0;\n+var REPLICA_ID = -1;\n+var GROUPS_PROTOCOL_TYPE = 'consumer';\n+\n+function groupByTopic (payloads) {\n+  return payloads.reduce(function (out, p) {\n+    out[p.topic] = out[p.topic] || {};\n+    out[p.topic][p.partition] = p;\n+    return out;\n+  }, {});\n+}\n+\n+function encodeRequestWithLength (request) {\n+  return new Buffermaker().Int32BE(request.length).string(request).make();\n+}\n+\n+function encodeRequestHeader (clientId, correlationId, apiKey, apiVersion) {\n+  return new Buffermaker()\n+    .Int16BE(apiKey)\n+    .Int16BE(apiVersion || API_VERSION)\n+    .Int32BE(correlationId)\n+    .Int16BE(clientId.length)\n+    .string(clientId);\n+}\n+\n+function encodeSaslHandshakeRequest (clientId, correlationId, apiVersion, mechanism) {\n+  var request = encodeRequestHeader(clientId, correlationId, REQUEST_TYPE.saslHandshake, apiVersion);\n+  request.Int16BE(mechanism.length).string(mechanism.toUpperCase());\n+  return encodeRequestWithLength(request.make());\n+}\n+\n+function decodeSaslHandshakeResponse (resp) {\n+  var mechanisms = [];\n+  var errorCode = null;\n+\n+  Binary.parse(resp)\n+    .word32bs('size')\n+    .word32bs('correlationId')\n+    .word16bs('errorCode')\n+    .tap(function (vars) {\n+      errorCode = vars.errorCode;\n+    })\n+    .word32bs('numMechanisms')\n+    .loop(_decodeMechanisms);\n+\n+  function _decodeMechanisms (end, vars) {\n+    if (vars.numMechanisms-- === 0) {\n+      return end();\n+    }\n+    this\n",
					"match": false,
					"packageHash": "82f6e0afa9d9e66e1ffc2ebd473d420e50b01a4fbdf07eee7dc47bf365ec7980",
					"size": 16818,
					"sourceHash": "87ce000940bde5e144500372bc1ea4f0e93662f263123a3ed4fb76d876a345b6",
					"status": "content"
				},
				"lib/protocol/protocol_struct.js": {
					"diff": "--- published/lib/protocol/protocol_struct.js\n+++ rebuilt/lib/protocol/protocol_struct.js\n@@ -1,56 +1,105 @@\n 'use strict';\n \n-function createStruct() {\n-    var args = arguments[0];\n-    return function () {\n-        for (var i=0; i<args.length; i++) {\n-            this[args[i]] = arguments[i];\n-        }\n+function createStruct () {\n+  var args = arguments[0];\n+  return function () {\n+    for (var i = 0; i < args.length; i++) {\n+      this[args[i]] = arguments[i];\n     }\n+  };\n }\n \n var KEYS = {\n-    FetchRequest: ['topic', 'partition', 'offset', 'maxBytes'],\n-    FetchResponse: ['topic', 'fetchPartitions'],\n-    OffsetCommitRequest: ['topic', 'partition', 'offset', 'metadata', 'committing', 'autoCommitIntervalMs'],\n-    OffsetCommitResponse: [],\n-    TopicAndPartition: ['topic', 'partition'],\n-    PartitionMetadata: ['topic', 'partition', 'leader', 'replicas', 'isr'],\n-    Message: ['magic', 'attributes', 'key', 'value'],\n-    ProduceRequest: ['topic', 'partition', 'messages'],\n-    Request: ['payloads', 'encoder', 'decoder', 'callback']\n-}\n+  FetchRequest: ['topic', 'partition', 'offset', 'maxBytes'],\n+  FetchResponse: ['topic', 'fetchPartitions'],\n+  OffsetCommitRequest: ['topic', 'partition', 'offset', 'metadata', 'committing', 'autoCommitIntervalMs'],\n+  OffsetCommitResponse: [],\n+  TopicAndPartition: ['topic', 'partition'],\n+  PartitionMetadata: ['topic', 'partition', 'leader', 'replicas', 'isr'],\n+  Message: ['magic', 'attributes', 'key', 'value', 'timestamp'],\n+  ProduceRequest: ['topic', 'partition', 'messages', 'attributes'],\n+  Request: ['payloads', 'encoder', 'decoder', 'callback']\n+};\n \n var ERROR_CODE = {\n-    '0': 'NoError',\n-    '-1': 'Unknown',\n-    '1': 'OffsetOutOfRange',\n-    '2': 'InvalidMessage',\n-    '3': 'UnknownTopicOrPartition',\n-    '4': 'InvalidMessageSize',\n-    '5': 'LeaderNotAvailable',\n-    '6': 'NotLeaderForPartition',\n-    '7': 'RequestTimedOut',\n-    '8': 'BrokerNotAvailable',\n-    '9': 'ReplicaNotAvailable',\n-    '10': 'MessageSizeTooLarge',\n-    '11': 'StaleControllerEpochCode',\n-    '12': 'OffsetMetadataTooLargeCode'\n-}\n+  '0': 'NoError',\n+  '-1': 'Unknown',\n+  '1': 'OffsetOutOfRange',\n+  '2': 'InvalidMessage',\n+  '3': 'UnknownTopicOrPartition',\n+  '4': 'InvalidMessageSize',\n+  '5': 'LeaderNotAvailable',\n+  '6': 'NotLeaderForPartition',\n+  '7': 'RequestTimedOut',\n+  '8': 'BrokerNotAvailable',\n+  '9': 'ReplicaNotAvailable',\n+  '10': 'MessageSizeTooLarge',\n+  '11': 'StaleControllerEpochCode',\n+  '12': 'OffsetMetadataTooLargeCode',\n+  '14': 'GroupLoadInProgress',\n+  '15': 'GroupCoordinatorNotAvailable',\n+  '16': 'NotCoordinatorForGroup',\n+  '17': 'InvalidTopic',\n+  '18': 'RecordListTooLarge',\n+  '19': 'NotEnoughReplicas',\n+  '20': 'NotEnoughReplicasAfterAppend',\n+  '21': 'InvalidRequiredAcks',\n+  '22': 'IllegalGeneration',\n+  '23': 'InconsistentGroupProtocol',\n+  '25': 'UnknownMemberId',\n+  '26': 'InvalidSessionTimeout',\n+  '27': 'RebalanceInProgress',\n+  '28': 'InvalidCommitOffsetSize',\n+  '29': 'TopicAuthorizationFailed',\n+  '30': 'GroupAuthorizationFailed',\n+  '31': 'ClusterAuthorizationFailed',\n+  '41': 'NotController',\n+  '42': 'InvalidRequest'\n+};\n \n-var REQUEST_TYPE = {\n-    produce: 0,\n-    fetch: 1,\n-    offset: 2,\n-    metadata: 3,\n-    leader: 4,\n-    stopReplilca: 5,\n",
					"match": false,
					"packageHash": "a1f970288243fbfa7bf0051805e65f48f6295f50253c50dc1eda07f34645f7bf",
					"size": 1548,
					"sourceHash": "2cffd57192db1689e2af20b86d7a5645877ad7d8b00ba12b727e1c4830ab253c",
					"status": "content"
				},
				"lib/zookeeper.js": {
					"match": false,
					"packageHash": "f76da5b41f4b64f732c9bee91c1e31cee131f040112ba8d65928e24cfca5f2ca",
					"size": 13640,
					"status": "missing-in-source"
				},
				"package.json": {
					"diff": "--- published/package.json\n+++ rebuilt/package.json\n@@ -1,28 +1,77 @@\n {\n-    \"name\": \"kafka-node\",\n-    \"description\": \"node client for Apache kafka, only support kafka 0.8 and above\",\n-    \"version\": \"0.2.9\",\n-    \"main\": \"kafka.js\",\n-    \"dependencies\": {\n-        \"buffermaker\": \"1.2.0\",\n-        \"binary\": \"~0.3.0\",\n-        \"buffer-crc32\": \"~0.2.1\",\n-        \"node-zookeeper-client\": \"0.2.0\",\n-        \"async\": \"0.7.0\",\n-        \"node-uuid\": \"1.4.1\",\n-        \"lodash\": \"~2.2.1\"\n-    },\n-    \"devDependencies\": {\n-        \"mocha\": \"^1.18.2\",\n-        \"should\": \"~1.2.2\",\n-        \"line-by-line\": \"~0.1.1\",\n-        \"optimist\": \"~0.6.0\"\n-    },\n-    \"repository\": {\n-        \"type\": \"git\",\n-        \"url\": \"git@github.com:SOHU-Co/kafka-node.git\"\n-    },\n-    \"scripts\": {\n-        \"test\": \"make test\"\n-    }\n+  \"name\": \"kafka-node\",\n+  \"description\": \"Client for Apache Kafka v0.9.x, v0.10.x and v0.11.x\",\n+  \"keywords\": [\n+    \"kafka\",\n+    \"consumer\",\n+    \"producer\",\n+    \"broker\"\n+  ],\n+  \"files\": [\n+    \"kafka.js\",\n+    \"logging.js\",\n+    \"lib\",\n+    \"types\"\n+  ],\n+  \"bugs\": \"https://github.com/SOHU-co/kafka-node/issues\",\n+  \"version\": \"5.0.0\",\n+  \"main\": \"kafka.js\",\n+  \"types\": \"types/index.d.ts\",\n+  \"license\": \"MIT\",\n+  \"dependencies\": {\n+    \"async\": \"^2.6.2\",\n+    \"binary\": \"~0.3.0\",\n+    \"bl\": \"^2.2.0\",\n+    \"buffer-crc32\": \"~0.2.5\",\n+    \"buffermaker\": \"~1.2.0\",\n+    \"debug\": \"^2.1.3\",\n+    \"denque\": \"^1.3.0\",\n+    \"lodash\": \"^4.17.4\",\n+    \"minimatch\": \"^3.0.2\",\n+    \"nested-error-stacks\": \"^2.0.0\",\n+    \"optional\": \"^0.1.3\",\n+    \"retry\": \"^0.10.1\",\n+    \"uuid\": \"^3.0.0\"\n+  },\n+  \"engines\": {\n+    \"node\": \">=8.5.1\"\n+  },\n+  \"optionalDependencies\": {\n+    \"snappy\": \"^6.0.1\"\n+  },\n+  \"devDependencies\": {\n+    \"@types/node\": \"^10.12.27\",\n+    \"coveralls\": \"^2.11.12\",\n+    \"doctoc\": \"^1.2.0\",\n+    \"eslint\": \"^5.14.1\",\n+    \"eslint-config-semistandard\": \"^13.0.0\",\n+    \"eslint-config-standard\": \"^12.0.0\",\n+    \"eslint-plugin-dependencies\": \"^2.2.0\",\n+    \"eslint-plugin-import\": \"^2.16.0\",\n+    \"eslint-plugin-node\": \"^8.0.1\",\n+    \"eslint-plugin-promise\": \"^4.0.1\",\n+    \"eslint-plugin-standard\": \"^4.0.0\",\n+    \"execa\": \"^0.6.1\",\n+    \"istanbul\": \"^0.4.4\",\n+    \"mocha\": \"^3.1.0\",\n+    \"optimist\": \"^0.6.1\",\n+    \"proxyquire\": \"^1.7.10\",\n+    \"should\": \"^6.0.0\",\n+    \"sinon\": \"^2.0.0\",\n+    \"through2\": \"^2.0.3\",\n+    \"tslint\": \"^5.13.0\",\n+    \"tslint-config-semistandard\": \"^7.0.0\",\n+    \"typescript\": \"^2.8.3\"\n+  },\n+  \"repository\": {\n+    \"type\": \"git\",\n+    \"url\": \"https://github.com/SOHU-Co/kafka-node.git\"\n+  },\n+  \"scripts\": {\n+    \"test:ts\": \"tslint --project ./types/tsconfig.json --config ./types/tslint.json && tsc --project types\",\n",
					"match": false,
					"packageHash": "85a55c0f6bceab78e7be61ca9faea1738389ddf341e3dff2db975f47e1528288",
					"size": 718,
					"sourceHash": "3773a076c260f4f1f751161abf0510b5918c14ee924c5ec9faa3ce1b3d95ddeb",
					"status": "content"
				},
				"test/manual.gracefulexit.js": {
					"match": false,
					"packageHash": "ef53ed07258151759247eb6327e4ed034d6faedef9195baa780a0c592a418eae",
					"size": 397,
					"status": "missing-in-source"
				},
				"test/mocha.opts": {
					"match": false,
					"packageHash": "34708d07f421110b81aab21ae833486f14b34918a4b23d1c714b173c28c09b64",
					"size": 65,
					"status": "missing-in-source"
				},
				"test/test.consumer.js": {
					"match": false,
					"packageHash": "5a37033bb5752b77e3ccca072dbd997a143a520d3af24035ccd76b48639ef4e5",
					"size": 11426,
					"status": "missing-in-source"
				},
				"test/test.offset.js": {
					"match": false,
					"packageHash": "6cce1cb12d2c0d152ebd2531c67a5debf2ee60c30b351e4c5905534b97978d28",
					"size": 2901,
					"status": "missing-in-source"
				},
				"test/test.producer.js": {
					"match": false,
					"packageHash": "a2db28e51025898b21414f0013a1d2108742a4a509769a8d19787d449624b597",
					"size": 2374,
					"status": "missing-in-source"
				},
				"test/test.zookeeper.js": {
					"match": false,
					"packageHash": "3aac325c8be2c91030c87a1114920c490a4cd6a477ff123a97e63d6008d07639",
					"size": 2151,
					"status": "missing-in-source"
				},
				"lib/admin.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/assignment/index.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/assignment/range.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/assignment/roundrobin.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/baseClient.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/baseProducer.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/batch/KafkaBuffer.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/codec/index.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/codec/snappy.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/commitStream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/consumerGroup.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/consumerGroupHeartbeat.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/consumerGroupRecovery.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/consumerGroupStream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/consumerStream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/ApiNotSupportedError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/GroupCoordinatorNotAvailableError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/GroupLoadInProgressError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/HeartbeatTimeoutError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/IllegalGenerationError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/InvalidConfigError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/InvalidRequestError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/MessageSizeTooLargeError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/NotControllerError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/NotCoordinatorForGroupError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/RebalanceInProgressError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/SaslAuthenticationError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/TimeoutError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/errors/UnknownMemberIdError.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/kafkaClient.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/logging.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/partitioner.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/producerStream.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/protocol/protocolVersions.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/resources/index.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/utils.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/wrapper/BrokerReadable.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/wrapper/BrokerTransform.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"lib/wrapper/BrokerWrapper.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"logging.js": {
					"match": false,
					"status": "missing-in-package"
				},
				"types/index.d.ts": {
					"match": false,
					"status": "missing-in-package"
				},
				"types/kafka-node-tests.ts": {
					"match": false,
					"status": "missing-in-package"
				},
				"types/tsconfig.json": {
					"match": false,
					"status": "missing-in-package"
				},
				"types/tslint.json": {
					"match": false,
					"status": "missing-in-package"
				}
			},
			"summary": {
				"differentFiles": 17,
				"matchingFiles": 0,
				"missingInPackage": 44,
				"missingInSource": 20,
				"score": 0,
				"totalFiles": 81
			}
		},
		"prodDependencies": [
			{
				"name": "ansi-regex",
				"version": "3.0.1"
			},
			{
				"name": "aproba",
				"version": "1.2.0"
			},
			{
				"name": "are-we-there-yet",
				"version": "1.1.7"
			},
			{
				"name": "async",
				"version": "2.6.4"
			},
			{
				"name": "available-typed-arrays",
				"version": "1.0.7"
			},
			{
				"name": "balanced-match",
				"version": "1.0.2"
			},
			{
				"name": "binary",
				"version": "0.3.0"
			},
			{
				"name": "bindings",
				"version": "1.5.0"
			},
			{
				"name": "bl",
				"version": "2.2.1"
			},
			{
				"name": "brace-expansion",
				"version": "1.1.12"
			},
			{
				"name": "buffer-alloc",
				"version": "1.2.0"
			},
			{
				"name": "buffer-alloc-unsafe",
				"version": "1.1.0"
			},
			{
				"name": "buffer-crc32",
				"version": "0.2.13"
			},
			{
				"name": "buffer-fill",
				"version": "1.0.0"
			},
			{
				"name": "buffermaker",
				"version": "1.2.1"
			},
			{
				"name": "buffers",
				"version": "0.1.1"
			},
			{
				"name": "call-bind",
				"version": "1.0.8"
			},
			{
				"name": "call-bind-apply-helpers",
				"version": "1.0.2"
			},
			{
				"name": "call-bound",
				"version": "1.0.4"
			},
			{
				"name": "chainsaw",
				"version": "0.1.0"
			},
			{
				"name": "chownr",
				"version": "1.1.4"
			},
			{
				"name": "code-point-at",
				"version": "1.1.0"
			},
			{
				"name": "concat-map",
				"version": "0.0.1"
			},
			{
				"name": "console-control-strings",
				"version": "1.1.0"
			},
			{
				"name": "core-util-is",
				"version": "1.0.3"
			},
			{
				"name": "debug",
				"version": "2.6.9"
			},
			{
				"name": "decompress-response",
				"version": "3.3.0"
			},
			{
				"name": "deep-extend",
				"version": "0.6.0"
			},
			{
				"name": "define-data-property",
				"version": "1.1.4"
			},
			{
				"name": "delegates",
				"version": "1.0.0"
			},
			{
				"name": "denque",
				"version": "1.5.1"
			},
			{
				"name": "detect-libc",
				"version": "1.0.3"
			},
			{
				"name": "dunder-proto",
				"version": "1.0.1"
			},
			{
				"name": "end-of-stream",
				"version": "1.4.5"
			},
			{
				"name": "es-define-property",
				"version": "1.0.1"
			},
			{
				"name": "es-errors",
				"version": "1.3.0"
			},
			{
				"name": "es-object-atoms",
				"version": "1.1.1"
			},
			{
				"name": "expand-template",
				"version": "2.0.3"
			},
			{
				"name": "file-uri-to-path",
				"version": "1.0.0"
			},
			{
				"name": "for-each",
				"version": "0.3.5"
			},
			{
				"name": "fs-constants",
				"version": "1.0.0"
			},
			{
				"name": "function-bind",
				"version": "1.1.2"
			},
			{
				"name": "gauge",
				"version": "2.7.4"
			},
			{
				"name": "ansi-regex",
				"version": "2.1.1"
			},
			{
				"name": "is-fullwidth-code-point",
				"version": "1.0.0"
			},
			{
				"name": "string-width",
				"version": "1.0.2"
			},
			{
				"name": "strip-ansi",
				"version": "3.0.1"
			},
			{
				"name": "get-intrinsic",
				"version": "1.3.0"
			},
			{
				"name": "get-proto",
				"version": "1.0.1"
			},
			{
				"name": "github-from-package",
				"version": "0.0.0"
			},
			{
				"name": "gopd",
				"version": "1.2.0"
			},
			{
				"name": "has-property-descriptors",
				"version": "1.0.2"
			},
			{
				"name": "has-symbols",
				"version": "1.1.0"
			},
			{
				"name": "has-tostringtag",
				"version": "1.0.2"
			},
			{
				"name": "has-unicode",
				"version": "2.0.1"
			},
			{
				"name": "hasown",
				"version": "2.0.2"
			},
			{
				"name": "inherits",
				"version": "2.0.4"
			},
			{
				"name": "ini",
				"version": "1.3.8"
			},
			{
				"name": "is-callable",
				"version": "1.2.7"
			},
			{
				"name": "is-fullwidth-code-point",
				"version": "2.0.0"
			},
			{
				"name": "is-typed-array",
				"version": "1.1.15"
			},
			{
				"name": "isarray",
				"version": "1.0.0"
			},
			{
				"name": "lodash",
				"version": "4.17.23"
			},
			{
				"name": "long",
				"version": "1.1.2"
			},
			{
				"name": "math-intrinsics",
				"version": "1.1.0"
			},
			{
				"name": "mimic-response",
				"version": "1.0.1"
			},
			{
				"name": "minimatch",
				"version": "3.1.2"
			},
			{
				"name": "minimist",
				"version": "1.2.0"
			},
			{
				"name": "mkdirp",
				"version": "0.5.6"
			},
			{
				"name": "minimist",
				"version": "1.2.8"
			},
			{
				"name": "ms",
				"version": "2.0.0"
			},
			{
				"name": "nan",
				"version": "2.24.0"
			},
			{
				"name": "napi-build-utils",
				"version": "1.0.2"
			},
			{
				"name": "nested-error-stacks",
				"version": "2.1.1"
			},
			{
				"name": "node-abi",
				"version": "2.30.1"
			},
			{
				"name": "noop-logger",
				"version": "0.1.1"
			},
			{
				"name": "npmlog",
				"version": "4.1.2"
			},
			{
				"name": "number-is-nan",
				"version": "1.0.1"
			},
			{
				"name": "object-assign",
				"version": "4.1.1"
			},
			{
				"name": "once",
				"version": "1.4.0"
			},
			{
				"name": "optional",
				"version": "0.1.4"
			},
			{
				"name": "os-homedir",
				"version": "1.0.2"
			},
			{
				"name": "possible-typed-array-names",
				"version": "1.1.0"
			},
			{
				"name": "prebuild-install",
				"version": "5.3.0"
			},
			{
				"name": "tunnel-agent",
				"version": "0.6.0"
			},
			{
				"name": "process-nextick-args",
				"version": "2.0.1"
			},
			{
				"name": "pump",
				"version": "2.0.1"
			},
			{
				"name": "rc",
				"version": "1.2.8"
			},
			{
				"name": "readable-stream",
				"version": "2.3.8"
			},
			{
				"name": "safe-buffer",
				"version": "5.1.2"
			},
			{
				"name": "retry",
				"version": "0.10.1"
			},
			{
				"name": "safe-buffer",
				"version": "5.2.1"
			},
			{
				"name": "semver",
				"version": "5.7.2"
			},
			{
				"name": "set-blocking",
				"version": "2.0.0"
			},
			{
				"name": "set-function-length",
				"version": "1.2.2"
			},
			{
				"name": "signal-exit",
				"version": "3.0.7"
			},
			{
				"name": "simple-concat",
				"version": "1.0.1"
			},
			{
				"name": "simple-get",
				"version": "2.8.2"
			},
			{
				"name": "snappy",
				"version": "6.3.5"
			},
			{
				"name": "string_decoder",
				"version": "1.1.1"
			},
			{
				"name": "string-width",
				"version": "2.1.1"
			},
			{
				"name": "strip-ansi",
				"version": "4.0.0"
			},
			{
				"name": "strip-json-comments",
				"version": "2.0.1"
			},
			{
				"name": "tar-fs",
				"version": "1.16.6"
			},
			{
				"name": "pump",
				"version": "1.0.3"
			},
			{
				"name": "tar-stream",
				"version": "1.6.2"
			},
			{
				"name": "bl",
				"version": "1.2.3"
			},
			{
				"name": "to-buffer",
				"version": "1.2.2"
			},
			{
				"name": "isarray",
				"version": "2.0.5"
			},
			{
				"name": "traverse",
				"version": "0.3.9"
			},
			{
				"name": "typed-array-buffer",
				"version": "1.0.3"
			},
			{
				"name": "util-deprecate",
				"version": "1.0.2"
			},
			{
				"name": "uuid",
				"version": "3.4.0"
			},
			{
				"name": "which-pm-runs",
				"version": "1.1.0"
			},
			{
				"name": "which-typed-array",
				"version": "1.1.20"
			},
			{
				"name": "wide-align",
				"version": "1.1.5"
			},
			{
				"name": "wrappy",
				"version": "1.0.2"
			},
			{
				"name": "xtend",
				"version": "4.0.2"
			}
		]
	}
]
